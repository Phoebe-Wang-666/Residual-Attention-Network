{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1765856531946,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "cH8Bv8ULWdo1",
    "outputId": "94fe7b5a-c2ba-46b0-c9a1-d78b31c29933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4040-fall2025-project-swye  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1765856532828,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "3beoh2-KWd6e",
    "outputId": "bbc7e605-3ba6-4672-812e-88711a761d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/e4040-fall2025-project-swye\n"
     ]
    }
   ],
   "source": [
    "%cd e4040-fall2025-project-swye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1765856534357,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "DoN1sczjWjYx",
    "outputId": "ea438acb-1d4d-4005-c6fc-ed19ea02f8b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n",
      " checkpoints\t        README.md\t       train_cifar10_robustness.py\n",
      " compare_attention.py   README-proj.md\t      'train_cifar_new 12.13.py'\n",
      " demo.py\t        README_TF.md\t      'train_cifar_new 12.14.py'\n",
      " demo_ten.py\t        robustness_runs_fast   train_cifar_new.py\n",
      " models\t\t        run_all_models.py      train_cifar_tf.py\n",
      " parameter.py\t        select_best.py\t       train_imagenet.py\n",
      " plot.py\t        train_best.py\t       untitled.txt\n",
      " plot_results.py        train_cifar_100.py\n",
      "\n",
      "./checkpoints:\n",
      "attention164_best_model.weights.h5  attention92_simple_best_model.weights.h5\n",
      "\n",
      "./models:\n",
      "attention128_tf.py  attention56_tf.py\t   __init__tf.py  resnet128_tf.py\n",
      "attention164_tf.py  attention92_simple.py  layers_tf.py   resnet164_tf.py\n",
      "attention56.py\t    attention92_tf.py\t   __pycache__\t  resnet92_tf.py\n",
      "\n",
      "./models/__pycache__:\n",
      "attention128_tf.cpython-312.pyc  attention92_simple.cpython-312.pyc\n",
      "attention164_tf.cpython-312.pyc  attention92_tf.cpython-312.pyc\n",
      "attention56.cpython-312.pyc\t __init__.cpython-312.pyc\n",
      "attention56_tf.cpython-312.pyc\t layers_tf.cpython-312.pyc\n",
      "attention92.cpython-312.pyc\t resnet92_tf.cpython-312.pyc\n",
      "\n",
      "./robustness_runs_fast:\n",
      "attention92_noise0.10_seed0.weights.h5\tattention92_noise0.30_seed0.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1765856537181,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "xlOW66HRAlHC",
    "outputId": "acdafc7e-42b7-4b10-ee94-4293072098d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/e4040-fall2025-project-swye/models\n"
     ]
    }
   ],
   "source": [
    "%cd models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6521,
     "status": "ok",
     "timestamp": 1765735128081,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "j0PibJrRWlMN",
    "outputId": "94cbb1b3-928b-45b9-9bac-458df0afc6f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/e4040-fall2025-project-swye\n",
      "2025-12-14 17:58:41.840825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765735121.862985    4509 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765735121.869601    4509 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765735121.887078    4509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765735121.887116    4509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765735121.887121    4509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765735121.887131    4509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-14 17:58:41.891703: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/e4040-fall2025-project-swye/models/cifar_14.py\", line 8, in <module>\n",
      "    from models.attention56 import ResidualAttentionModel56\n",
      "ModuleNotFoundError: No module named 'models'\n"
     ]
    }
   ],
   "source": [
    "%cd /content/e4040-fall2025-project-swye\n",
    "!python models/cifar_14.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1729981,
     "status": "ok",
     "timestamp": 1765736932962,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "Ko9IbaNJBNpA",
    "outputId": "9d58881b-10cc-4848-a17d-119be3e86727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:00:03.266762: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765735203.285237    4874 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765735203.290755    4874 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765735203.304772    4874 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765735203.304794    4874 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765735203.304797    4874 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765735203.304799    4874 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-14 18:00:03.309119: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention92\n",
      "Epochs: 40\n",
      "Batch size: 64\n",
      "Learning rate: 0.001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n",
      "Training samples: 50000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "2025-12-14 18:00:27.861765: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765735227.863534    4874 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "\n",
      "Building attention92 model...\n",
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'residual_attention_model92', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "\u001b[1mModel: \"residual_attention_model92\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv1 (\u001b[94mConv1\u001b[0m)                   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ stage1 (\u001b[94mStage1\u001b[0m)                 │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ stage2 (\u001b[94mStage2\u001b[0m)                 │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ stage2__keep (\u001b[94mStage2_Keep\u001b[0m)      │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ last_stage (\u001b[94mLastStage\u001b[0m)          │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\n",
      "Total parameters: 0\n",
      "Epoch 1/40\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765735276.766947    5022 service.cc:152] XLA service 0x7fc02c0522d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765735276.766985    5022 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-14 18:01:17.996898: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765735284.368624    5022 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "2025-12-14 18:01:27.295395: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.315 = (f32[64,32,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,3,32,32]{3,2,1,0} %bitcast.62039, f32[32,3,3,3]{3,2,1,0} %bitcast.62046, f32[32]{0} %bitcast.75962), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/conv1_1/conv2d_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:01:27.450945: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.316 = (f32[64,32,16,16]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,32,16,16]{3,2,1,0} %bitcast.76126, f32[32,32,3,3]{3,2,1,0} %bitcast.62120, f32[32]{0} %bitcast.76186), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage1_1/conv1_1_1/conv2d_1_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:01:27.592658: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.318 = (f32[64,32,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,32,8,8]{3,2,1,0} %bitcast.76679, f32[32,32,3,3]{3,2,1,0} %bitcast.62330, f32[32]{0} %bitcast.76739), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage1_1/residual_unit1_1/conv2d_3_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:01:27.724027: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.321 = (f32[64,128,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,8,8]{3,2,1,0} %bitcast.78495, f32[128,128,3,3]{3,2,1,0} %bitcast.62436, f32[128]{0} %bitcast.78555), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage1_1/attention1_1/mask1_1/down_sampling1_1/conv2d_12_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:01:27.799714: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.322 = (f32[64,128,4,4]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,4,4]{3,2,1,0} %bitcast.78618, f32[128,128,3,3]{3,2,1,0} %bitcast.62451, f32[128]{0} %bitcast.78678), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage1_1/attention1_1/mask1_1/down_sampling1_1/conv2d_13_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:01:28.011100: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.328 = (f32[64,32,4,4]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,32,4,4]{3,2,1,0} %bitcast.79916, f32[32,32,3,3]{3,2,1,0} %bitcast.62855, f32[32]{0} %bitcast.79976), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage1_1/attention1_1/mask1_1/mask_residual2_1/residual_unit2_3_1/conv2d_19_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:01:28.174984: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.343 = (f32[64,64,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,64,8,8]{3,2,1,0} %bitcast.81752, f32[64,64,3,3]{3,2,1,0} %bitcast.63926, f32[64]{0} %bitcast.81812), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage2_1/residual_unit2_5_1/conv2d_28_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:01:28.309825: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.346 = (f32[64,256,4,4]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,256,4,4]{3,2,1,0} %bitcast.83582, f32[256,256,3,3]{3,2,1,0} %bitcast.64032, f32[256]{0} %bitcast.83642), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage2_1/attention2_1/mask2_1/down_sampling2_1/conv2d_37_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:01:28.712681: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.358 = (f32[64,64,4,4]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,64,4,4]{3,2,1,0} %bitcast.82555, f32[64,64,3,3]{3,2,1,0} %bitcast.64822, f32[64]{0} %bitcast.82615), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage2_1/attention2_1/trunk2_1/residual_unit2_6_1/conv2d_32_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "I0000 00:00:1765735308.675074    5022 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m780/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.3586 - loss: 1.92602025-12-14 18:02:33.900339: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.315 = (f32[16,32,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,3,32,32]{3,2,1,0} %bitcast.62081, f32[32,3,3,3]{3,2,1,0} %bitcast.62088, f32[32]{0} %bitcast.76005), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/conv1_1/conv2d_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:02:33.924852: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.316 = (f32[16,32,16,16]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,32,16,16]{3,2,1,0} %bitcast.76169, f32[32,32,3,3]{3,2,1,0} %bitcast.62162, f32[32]{0} %bitcast.76229), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage1_1/conv1_1_1/conv2d_1_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:02:34.137666: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.321 = (f32[16,128,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,128,8,8]{3,2,1,0} %bitcast.78538, f32[128,128,3,3]{3,2,1,0} %bitcast.62478, f32[128]{0} %bitcast.78598), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage1_1/attention1_1/mask1_1/down_sampling1_1/conv2d_12_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:02:34.196823: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.322 = (f32[16,128,4,4]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,128,4,4]{3,2,1,0} %bitcast.78661, f32[128,128,3,3]{3,2,1,0} %bitcast.62493, f32[128]{0} %bitcast.78721), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage1_1/attention1_1/mask1_1/down_sampling1_1/conv2d_13_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:02:34.382884: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.328 = (f32[16,32,4,4]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,32,4,4]{3,2,1,0} %bitcast.79959, f32[32,32,3,3]{3,2,1,0} %bitcast.62897, f32[32]{0} %bitcast.80019), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage1_1/attention1_1/mask1_1/mask_residual2_1/residual_unit2_3_1/conv2d_19_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:02:34.550023: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.343 = (f32[16,64,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,64,8,8]{3,2,1,0} %bitcast.81795, f32[64,64,3,3]{3,2,1,0} %bitcast.63968, f32[64]{0} %bitcast.81855), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage2_1/residual_unit2_5_1/conv2d_28_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:02:34.659413: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.346 = (f32[16,256,4,4]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,256,4,4]{3,2,1,0} %bitcast.83625, f32[256,256,3,3]{3,2,1,0} %bitcast.64074, f32[256]{0} %bitcast.83685), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage2_1/attention2_1/mask2_1/down_sampling2_1/conv2d_37_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-12-14 18:02:35.012810: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=2} for conv %cudnn-conv-bias-activation.358 = (f32[16,64,4,4]{3,2,1,0}, u8[0]{0}) custom-call(f32[16,64,4,4]{3,2,1,0} %bitcast.82598, f32[64,64,3,3]{3,2,1,0} %bitcast.64864, f32[64]{0} %bitcast.82658), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"residual_attention_model92_1/stage2_1/attention2_1/trunk2_1/residual_unit2_6_1/conv2d_32_1/convolution\" source_file=\"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3588 - loss: 1.9254\n",
      "Epoch 1: val_accuracy improved from -inf to 0.52970, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 97ms/step - accuracy: 0.3589 - loss: 1.9252 - val_accuracy: 0.5297 - val_loss: 1.4426 - learning_rate: 1.0000e-04\n",
      "Epoch 2/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5199 - loss: 1.4538\n",
      "Epoch 2: val_accuracy improved from 0.52970 to 0.59290, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.5200 - loss: 1.4537 - val_accuracy: 0.5929 - val_loss: 1.2885 - learning_rate: 1.0000e-04\n",
      "Epoch 3/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5757 - loss: 1.3274\n",
      "Epoch 3: val_accuracy improved from 0.59290 to 0.62890, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.5757 - loss: 1.3274 - val_accuracy: 0.6289 - val_loss: 1.2311 - learning_rate: 1.0000e-04\n",
      "Epoch 4/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6179 - loss: 1.2333\n",
      "Epoch 4: val_accuracy did not improve from 0.62890\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - accuracy: 0.6179 - loss: 1.2333 - val_accuracy: 0.6132 - val_loss: 1.2585 - learning_rate: 1.0000e-04\n",
      "Epoch 5/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6402 - loss: 1.1810\n",
      "Epoch 5: val_accuracy improved from 0.62890 to 0.65780, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.6402 - loss: 1.1810 - val_accuracy: 0.6578 - val_loss: 1.1656 - learning_rate: 1.0000e-04\n",
      "Epoch 6/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6677 - loss: 1.1122\n",
      "Epoch 6: val_accuracy improved from 0.65780 to 0.68700, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.6677 - loss: 1.1121 - val_accuracy: 0.6870 - val_loss: 1.0981 - learning_rate: 1.0000e-04\n",
      "Epoch 7/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6919 - loss: 1.0544\n",
      "Epoch 7: val_accuracy improved from 0.68700 to 0.69480, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.6919 - loss: 1.0543 - val_accuracy: 0.6948 - val_loss: 1.0814 - learning_rate: 1.0000e-04\n",
      "Epoch 8/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7135 - loss: 1.0078\n",
      "Epoch 8: val_accuracy improved from 0.69480 to 0.69690, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.7135 - loss: 1.0078 - val_accuracy: 0.6969 - val_loss: 1.0649 - learning_rate: 1.0000e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7288 - loss: 0.9720\n",
      "Epoch 9: val_accuracy improved from 0.69690 to 0.75420, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.7288 - loss: 0.9720 - val_accuracy: 0.7542 - val_loss: 0.9245 - learning_rate: 1.0000e-04\n",
      "Epoch 10/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7482 - loss: 0.9316\n",
      "Epoch 10: val_accuracy did not improve from 0.75420\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.7482 - loss: 0.9315 - val_accuracy: 0.7422 - val_loss: 0.9642 - learning_rate: 1.0000e-04\n",
      "Epoch 11/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7559 - loss: 0.9048\n",
      "Epoch 11: val_accuracy did not improve from 0.75420\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.7559 - loss: 0.9048 - val_accuracy: 0.7412 - val_loss: 0.9845 - learning_rate: 1.0000e-04\n",
      "Epoch 12/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7707 - loss: 0.8780\n",
      "Epoch 12: val_accuracy improved from 0.75420 to 0.76080, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.7707 - loss: 0.8779 - val_accuracy: 0.7608 - val_loss: 0.9178 - learning_rate: 1.0000e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7856 - loss: 0.8451\n",
      "Epoch 13: val_accuracy improved from 0.76080 to 0.78180, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.7856 - loss: 0.8451 - val_accuracy: 0.7818 - val_loss: 0.8685 - learning_rate: 1.0000e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7937 - loss: 0.8236\n",
      "Epoch 14: val_accuracy did not improve from 0.78180\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - accuracy: 0.7937 - loss: 0.8236 - val_accuracy: 0.7793 - val_loss: 0.8799 - learning_rate: 1.0000e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7998 - loss: 0.8081\n",
      "Epoch 15: val_accuracy improved from 0.78180 to 0.78890, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.7998 - loss: 0.8081 - val_accuracy: 0.7889 - val_loss: 0.8576 - learning_rate: 1.0000e-04\n",
      "Epoch 16/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8089 - loss: 0.7802\n",
      "Epoch 16: val_accuracy improved from 0.78890 to 0.78990, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.8089 - loss: 0.7802 - val_accuracy: 0.7899 - val_loss: 0.8544 - learning_rate: 1.0000e-04\n",
      "Epoch 17/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8133 - loss: 0.7714\n",
      "Epoch 17: val_accuracy improved from 0.78990 to 0.79870, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.8133 - loss: 0.7714 - val_accuracy: 0.7987 - val_loss: 0.8099 - learning_rate: 1.0000e-04\n",
      "Epoch 18/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8240 - loss: 0.7480\n",
      "Epoch 18: val_accuracy improved from 0.79870 to 0.80120, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - accuracy: 0.8240 - loss: 0.7480 - val_accuracy: 0.8012 - val_loss: 0.8119 - learning_rate: 1.0000e-04\n",
      "Epoch 19/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8306 - loss: 0.7341\n",
      "Epoch 19: val_accuracy did not improve from 0.80120\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - accuracy: 0.8306 - loss: 0.7341 - val_accuracy: 0.7989 - val_loss: 0.8096 - learning_rate: 1.0000e-04\n",
      "Epoch 20/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8366 - loss: 0.7205\n",
      "Epoch 20: val_accuracy did not improve from 0.80120\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.8366 - loss: 0.7205 - val_accuracy: 0.7987 - val_loss: 0.8273 - learning_rate: 1.0000e-04\n",
      "Epoch 21/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8416 - loss: 0.7069\n",
      "Epoch 21: val_accuracy did not improve from 0.80120\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.8416 - loss: 0.7069 - val_accuracy: 0.7984 - val_loss: 0.8341 - learning_rate: 1.0000e-04\n",
      "Epoch 22/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8466 - loss: 0.6897\n",
      "Epoch 22: val_accuracy improved from 0.80120 to 0.81370, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.8467 - loss: 0.6897 - val_accuracy: 0.8137 - val_loss: 0.7937 - learning_rate: 1.0000e-04\n",
      "Epoch 23/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8545 - loss: 0.6793\n",
      "Epoch 23: val_accuracy did not improve from 0.81370\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.8545 - loss: 0.6793 - val_accuracy: 0.8100 - val_loss: 0.8067 - learning_rate: 1.0000e-04\n",
      "Epoch 24/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8599 - loss: 0.6641\n",
      "Epoch 24: val_accuracy improved from 0.81370 to 0.81590, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.8599 - loss: 0.6641 - val_accuracy: 0.8159 - val_loss: 0.7717 - learning_rate: 1.0000e-04\n",
      "Epoch 25/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8641 - loss: 0.6481\n",
      "Epoch 25: val_accuracy improved from 0.81590 to 0.82010, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.8641 - loss: 0.6481 - val_accuracy: 0.8201 - val_loss: 0.7726 - learning_rate: 1.0000e-04\n",
      "Epoch 26/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8685 - loss: 0.6427\n",
      "Epoch 26: val_accuracy did not improve from 0.82010\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.8685 - loss: 0.6427 - val_accuracy: 0.8201 - val_loss: 0.7661 - learning_rate: 1.0000e-04\n",
      "Epoch 27/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8699 - loss: 0.6373\n",
      "Epoch 27: val_accuracy did not improve from 0.82010\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.8699 - loss: 0.6372 - val_accuracy: 0.8104 - val_loss: 0.8041 - learning_rate: 1.0000e-04\n",
      "Epoch 28/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8727 - loss: 0.6277\n",
      "Epoch 28: val_accuracy improved from 0.82010 to 0.83030, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.8727 - loss: 0.6277 - val_accuracy: 0.8303 - val_loss: 0.7523 - learning_rate: 1.0000e-04\n",
      "Epoch 29/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8801 - loss: 0.6106\n",
      "Epoch 29: val_accuracy did not improve from 0.83030\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.8801 - loss: 0.6106 - val_accuracy: 0.8176 - val_loss: 0.7961 - learning_rate: 1.0000e-04\n",
      "Epoch 30/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8805 - loss: 0.6122\n",
      "Epoch 30: val_accuracy did not improve from 0.83030\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.8805 - loss: 0.6122 - val_accuracy: 0.8220 - val_loss: 0.7667 - learning_rate: 1.0000e-04\n",
      "Epoch 31/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8874 - loss: 0.6009\n",
      "Epoch 31: val_accuracy did not improve from 0.83030\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.8874 - loss: 0.6009 - val_accuracy: 0.8265 - val_loss: 0.7477 - learning_rate: 1.0000e-04\n",
      "Epoch 32/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8908 - loss: 0.5865\n",
      "Epoch 32: val_accuracy improved from 0.83030 to 0.83210, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.8908 - loss: 0.5865 - val_accuracy: 0.8321 - val_loss: 0.7473 - learning_rate: 1.0000e-04\n",
      "Epoch 33/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8930 - loss: 0.5813\n",
      "Epoch 33: val_accuracy did not improve from 0.83210\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.8930 - loss: 0.5813 - val_accuracy: 0.8273 - val_loss: 0.7654 - learning_rate: 1.0000e-04\n",
      "Epoch 34/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8937 - loss: 0.5751\n",
      "Epoch 34: val_accuracy improved from 0.83210 to 0.83520, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.8937 - loss: 0.5751 - val_accuracy: 0.8352 - val_loss: 0.7445 - learning_rate: 1.0000e-04\n",
      "Epoch 35/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9011 - loss: 0.5611\n",
      "Epoch 35: val_accuracy improved from 0.83520 to 0.83650, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.9011 - loss: 0.5611 - val_accuracy: 0.8365 - val_loss: 0.7359 - learning_rate: 1.0000e-04\n",
      "Epoch 36/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9061 - loss: 0.5533\n",
      "Epoch 36: val_accuracy improved from 0.83650 to 0.84050, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.9061 - loss: 0.5533 - val_accuracy: 0.8405 - val_loss: 0.7268 - learning_rate: 1.0000e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9073 - loss: 0.5503\n",
      "Epoch 37: val_accuracy did not improve from 0.84050\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.9073 - loss: 0.5502 - val_accuracy: 0.8405 - val_loss: 0.7226 - learning_rate: 1.0000e-04\n",
      "Epoch 38/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9093 - loss: 0.5426\n",
      "Epoch 38: val_accuracy did not improve from 0.84050\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.9093 - loss: 0.5426 - val_accuracy: 0.8308 - val_loss: 0.7619 - learning_rate: 1.0000e-04\n",
      "Epoch 39/40\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9101 - loss: 0.5369\n",
      "Epoch 39: val_accuracy did not improve from 0.84050\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.9101 - loss: 0.5369 - val_accuracy: 0.8312 - val_loss: 0.7608 - learning_rate: 1.0000e-04\n",
      "Epoch 40/40\n",
      "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9157 - loss: 0.5282\n",
      "Epoch 40: val_accuracy did not improve from 0.84050\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.9157 - loss: 0.5282 - val_accuracy: 0.8398 - val_loss: 0.7350 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "============================================================\n",
      "Final Test Accuracy: 84.05%\n",
      "Final Test Loss: 0.7268\n",
      "============================================================\n",
      "\n",
      "Best Training Accuracy: 91.53%\n",
      "Best Validation Accuracy: 84.05%\n",
      "Final Test Accuracy: 84.05%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/attention92_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python -m models.cifar 12.14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1765741786119,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "yMP0jq5QaTg2",
    "outputId": "a046ac7e-1865-4bdb-ea81-e56c37efdaed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " checkpoints\t        plot_results.py    'train_cifar_new 12.13.py'\n",
      " compare_attention.py   README.md\t   'train_cifar_new 12.14.py'\n",
      " demo.py\t        README-proj.md\t    train_cifar_new_ivy.py\n",
      " demo_ten.py\t        README_TF.md\t    train_cifar_new.py\n",
      " models\t\t        run_all_models.py   train_cifar_tf.py\n",
      " parameter.py\t        select_best.py\t    train_imagenet.py\n",
      " plot.py\t        train_best.py\t    untitled.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcPO-UyRaXKt"
   },
   "outputs": [],
   "source": [
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1765741811244,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "tBCbfV5QacYV",
    "outputId": "e19382ba-fccd-4c49-809c-a2211d133394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/           plot_results.py    'train_cifar_new 12.13.py'\n",
      " compare_attention.py   README.md          'train_cifar_new 12.14.py'\n",
      " demo.py                README-proj.md      train_cifar_new_ivy.py\n",
      " demo_ten.py            README_TF.md        train_cifar_new.py\n",
      " \u001b[01;34mmodels\u001b[0m/                run_all_models.py   train_cifar_tf.py\n",
      " parameter.py           select_best.py      train_imagenet.py\n",
      " plot.py                train_best.py       untitled.txt\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6513,
     "status": "ok",
     "timestamp": 1765741845787,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "e_FSjeNqadQF",
    "outputId": "4d237fe2-0856-470a-96ab-581b5b725abd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-14 19:50:39.563857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765741839.582749   33555 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765741839.588316   33555 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765741839.604374   33555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765741839.604400   33555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765741839.604403   33555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765741839.604405   33555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-14 19:50:39.608885: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/e4040-fall2025-project-swye/train_cifar_new.py\", line 13, in <module>\n",
      "    from models.resnet128_tf import ResNet128\n",
      "ModuleNotFoundError: No module named 'models.resnet128_tf'\n"
     ]
    }
   ],
   "source": [
    "!python train_cifar_new.py --model resnet92_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277507,
     "status": "ok",
     "timestamp": 1765742241531,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "xLbsrcGYakLL",
    "outputId": "1a63f323-250e-475c-f2b6-adfe77b9505b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-14 19:52:44.299219: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765741964.317116   34112 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765741964.322751   34112 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765741964.336949   34112 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765741964.336975   34112 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765741964.336978   34112 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765741964.336980   34112 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-14 19:52:44.341452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: resnet92\n",
      "Epochs: 10\n",
      "Batch size: 128\n",
      "Learning rate: 0.1\n",
      "Momentum: 0.9\n",
      "Weight decay: 0.0001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 50000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "2025-12-14 19:52:53.331030: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765741973.331205   34112 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "\n",
      "Building resnet92 model...\n",
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'res_net92', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "\u001b[1mModel: \"res_net92\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_7 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_8 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_9 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_55          │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\n",
      "Total parameters: 0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 1/10\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765741993.416706   34194 service.cc:152] XLA service 0x7c0e68018630 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765741993.416761   34194 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-14 19:53:13.877560: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765741995.494500   34194 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1765742009.080620   34194 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.2099 - loss: 3.8569\n",
      "Epoch 1: val_accuracy improved from -inf to 0.34750, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 103ms/step - accuracy: 0.2100 - loss: 3.8546 - val_accuracy: 0.3475 - val_loss: 2.6160 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 2/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3792 - loss: 1.7339\n",
      "Epoch 2: val_accuracy improved from 0.34750 to 0.45140, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.3793 - loss: 1.7336 - val_accuracy: 0.4514 - val_loss: 1.4870 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 3/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4457 - loss: 1.5175\n",
      "Epoch 3: val_accuracy improved from 0.45140 to 0.48490, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.4458 - loss: 1.5173 - val_accuracy: 0.4849 - val_loss: 1.4384 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 4/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4906 - loss: 1.4038\n",
      "Epoch 4: val_accuracy improved from 0.48490 to 0.52280, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - accuracy: 0.4906 - loss: 1.4036 - val_accuracy: 0.5228 - val_loss: 1.3192 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 5/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5218 - loss: 1.3169\n",
      "Epoch 5: val_accuracy improved from 0.52280 to 0.58490, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 54ms/step - accuracy: 0.5218 - loss: 1.3168 - val_accuracy: 0.5849 - val_loss: 1.1537 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 6/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5550 - loss: 1.2347\n",
      "Epoch 6: val_accuracy improved from 0.58490 to 0.60640, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.5551 - loss: 1.2346 - val_accuracy: 0.6064 - val_loss: 1.0999 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 7/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5872 - loss: 1.1548\n",
      "Epoch 7: val_accuracy improved from 0.60640 to 0.61580, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 54ms/step - accuracy: 0.5873 - loss: 1.1548 - val_accuracy: 0.6158 - val_loss: 1.0858 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 8/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6018 - loss: 1.1051\n",
      "Epoch 8: val_accuracy improved from 0.61580 to 0.63080, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 54ms/step - accuracy: 0.6018 - loss: 1.1050 - val_accuracy: 0.6308 - val_loss: 1.0799 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 9/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6286 - loss: 1.0378\n",
      "Epoch 9: val_accuracy improved from 0.63080 to 0.64740, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 54ms/step - accuracy: 0.6286 - loss: 1.0378 - val_accuracy: 0.6474 - val_loss: 1.1414 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 10/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6481 - loss: 0.9876\n",
      "Epoch 10: val_accuracy improved from 0.64740 to 0.66620, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 54ms/step - accuracy: 0.6482 - loss: 0.9875 - val_accuracy: 0.6662 - val_loss: 1.6824 - learning_rate: 0.1000\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "============================================================\n",
      "Final Test Accuracy: 66.62%\n",
      "Final Test Loss: 1.6824\n",
      "============================================================\n",
      "\n",
      "Best Training Accuracy: 65.01%\n",
      "Best Validation Accuracy: 66.62%\n",
      "Final Test Accuracy: 66.62%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/resnet92_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python train_cifar_new.py --model resnet92_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271114,
     "status": "ok",
     "timestamp": 1765742557106,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "311Z2JtWcJx_",
    "outputId": "f9b638b3-a9fb-4cb6-8b05-43181d4824f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-14 19:58:06.266372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765742286.284780   36003 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765742286.290315   36003 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765742286.304447   36003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765742286.304472   36003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765742286.304475   36003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765742286.304477   36003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-14 19:58:06.308769: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: resnet128\n",
      "Epochs: 10\n",
      "Batch size: 128\n",
      "Learning rate: 0.1\n",
      "Momentum: 0.9\n",
      "Weight decay: 0.0001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 50000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "2025-12-14 19:58:15.211761: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765742295.211946   36003 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "\n",
      "Building resnet128 model...\n",
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'res_net128', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "\u001b[1mModel: \"res_net128\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_7 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_8 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_9 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_58          │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\n",
      "Total parameters: 0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 1/10\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765742316.143290   36083 service.cc:152] XLA service 0x7c7efc003e30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765742316.143327   36083 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-14 19:58:36.637700: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765742318.315665   36083 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1765742331.609595   36083 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.2127 - loss: 3.4989\n",
      "Epoch 1: val_accuracy improved from -inf to 0.31610, saving model to ./checkpoints/resnet128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 103ms/step - accuracy: 0.2128 - loss: 3.4969 - val_accuracy: 0.3161 - val_loss: 2.3900 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 2/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3637 - loss: 1.7585\n",
      "Epoch 2: val_accuracy improved from 0.31610 to 0.43960, saving model to ./checkpoints/resnet128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - accuracy: 0.3637 - loss: 1.7584 - val_accuracy: 0.4396 - val_loss: 1.5596 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 3/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4229 - loss: 1.5855\n",
      "Epoch 3: val_accuracy improved from 0.43960 to 0.48170, saving model to ./checkpoints/resnet128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.4229 - loss: 1.5854 - val_accuracy: 0.4817 - val_loss: 1.5081 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 4/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4561 - loss: 1.4899\n",
      "Epoch 4: val_accuracy improved from 0.48170 to 0.49530, saving model to ./checkpoints/resnet128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 51ms/step - accuracy: 0.4561 - loss: 1.4899 - val_accuracy: 0.4953 - val_loss: 1.3713 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 5/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4892 - loss: 1.3963\n",
      "Epoch 5: val_accuracy improved from 0.49530 to 0.53090, saving model to ./checkpoints/resnet128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 51ms/step - accuracy: 0.4892 - loss: 1.3963 - val_accuracy: 0.5309 - val_loss: 1.3058 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 6/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5127 - loss: 1.3457\n",
      "Epoch 6: val_accuracy improved from 0.53090 to 0.53570, saving model to ./checkpoints/resnet128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.5127 - loss: 1.3457 - val_accuracy: 0.5357 - val_loss: 1.2799 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 7/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5327 - loss: 1.2906\n",
      "Epoch 7: val_accuracy improved from 0.53570 to 0.57990, saving model to ./checkpoints/resnet128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.5327 - loss: 1.2905 - val_accuracy: 0.5799 - val_loss: 1.1702 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 8/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5644 - loss: 1.1996\n",
      "Epoch 8: val_accuracy did not improve from 0.57990\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 50ms/step - accuracy: 0.5644 - loss: 1.1996 - val_accuracy: 0.4730 - val_loss: 8.7467 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 9/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5788 - loss: 1.1742\n",
      "Epoch 9: val_accuracy improved from 0.57990 to 0.61710, saving model to ./checkpoints/resnet128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 51ms/step - accuracy: 0.5788 - loss: 1.1742 - val_accuracy: 0.6171 - val_loss: 1.1160 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 10/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6057 - loss: 1.0978\n",
      "Epoch 10: val_accuracy improved from 0.61710 to 0.61990, saving model to ./checkpoints/resnet128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.6057 - loss: 1.0977 - val_accuracy: 0.6199 - val_loss: 1.1049 - learning_rate: 0.1000\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "============================================================\n",
      "Final Test Accuracy: 61.99%\n",
      "Final Test Loss: 1.1049\n",
      "============================================================\n",
      "\n",
      "Best Training Accuracy: 60.91%\n",
      "Best Validation Accuracy: 61.99%\n",
      "Final Test Accuracy: 61.99%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/resnet128_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python train_cifar_new.py --model resnet128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUCzVJ-dRFiO"
   },
   "source": [
    "### run resnet164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299747,
     "status": "ok",
     "timestamp": 1765743136875,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "0xxTJXX2cROx",
    "outputId": "b05c1f04-17ed-4750-e338-14090fcbc1dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-14 20:07:17.331854: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765742837.349844   38820 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765742837.355276   38820 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765742837.369436   38820 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765742837.369489   38820 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765742837.369499   38820 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765742837.369503   38820 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-14 20:07:17.373778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: resnet164\n",
      "Epochs: 10\n",
      "Batch size: 128\n",
      "Learning rate: 0.1\n",
      "Momentum: 0.9\n",
      "Weight decay: 0.0001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 50000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "2025-12-14 20:07:26.313532: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765742846.313660   38820 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "\n",
      "Building resnet164 model...\n",
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'res_net164', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "\u001b[1mModel: \"res_net164\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_7 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_8 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_9 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_67          │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\n",
      "Total parameters: 0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 1/10\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765742869.570858   38900 service.cc:152] XLA service 0x7855600023c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765742869.570891   38900 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-14 20:07:50.088321: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765742871.967076   38900 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1765742885.968250   38900 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.1870 - loss: 3.6108\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25710, saving model to ./checkpoints/resnet164_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 112ms/step - accuracy: 0.1871 - loss: 3.6088 - val_accuracy: 0.2571 - val_loss: 2.9494 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 2/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.3350 - loss: 1.8593\n",
      "Epoch 2: val_accuracy improved from 0.25710 to 0.43730, saving model to ./checkpoints/resnet164_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - accuracy: 0.3351 - loss: 1.8591 - val_accuracy: 0.4373 - val_loss: 1.5803 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 3/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4147 - loss: 1.6115\n",
      "Epoch 3: val_accuracy improved from 0.43730 to 0.46550, saving model to ./checkpoints/resnet164_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - accuracy: 0.4148 - loss: 1.6113 - val_accuracy: 0.4655 - val_loss: 1.5004 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 4/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4535 - loss: 1.5084\n",
      "Epoch 4: val_accuracy improved from 0.46550 to 0.48260, saving model to ./checkpoints/resnet164_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.4536 - loss: 1.5083 - val_accuracy: 0.4826 - val_loss: 1.4358 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 5/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4825 - loss: 1.4291\n",
      "Epoch 5: val_accuracy improved from 0.48260 to 0.53600, saving model to ./checkpoints/resnet164_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.4825 - loss: 1.4291 - val_accuracy: 0.5360 - val_loss: 1.2933 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 6/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5053 - loss: 1.3565\n",
      "Epoch 6: val_accuracy did not improve from 0.53600\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - accuracy: 0.5053 - loss: 1.3565 - val_accuracy: 0.5099 - val_loss: 1.5604 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 7/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5248 - loss: 1.3124\n",
      "Epoch 7: val_accuracy improved from 0.53600 to 0.57270, saving model to ./checkpoints/resnet164_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 58ms/step - accuracy: 0.5249 - loss: 1.3123 - val_accuracy: 0.5727 - val_loss: 1.2164 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 8/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5535 - loss: 1.2494\n",
      "Epoch 8: val_accuracy improved from 0.57270 to 0.60110, saving model to ./checkpoints/resnet164_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 59ms/step - accuracy: 0.5535 - loss: 1.2494 - val_accuracy: 0.6011 - val_loss: 1.1183 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 9/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5650 - loss: 1.2129\n",
      "Epoch 9: val_accuracy did not improve from 0.60110\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - accuracy: 0.5650 - loss: 1.2130 - val_accuracy: 0.4882 - val_loss: 1.6192 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 10/10\n",
      "\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5805 - loss: 1.1779\n",
      "Epoch 10: val_accuracy did not improve from 0.60110\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 57ms/step - accuracy: 0.5805 - loss: 1.1779 - val_accuracy: 0.5929 - val_loss: 1.1521 - learning_rate: 0.1000\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "============================================================\n",
      "Final Test Accuracy: 60.11%\n",
      "Final Test Loss: 1.1183\n",
      "============================================================\n",
      "\n",
      "Best Training Accuracy: 58.37%\n",
      "Best Validation Accuracy: 60.11%\n",
      "Final Test Accuracy: 60.11%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/resnet164_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python train_cifar_new.py --model resnet164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MawCchmfRTQy"
   },
   "source": [
    "## run attention164 arl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxZ8v-hTfivb"
   },
   "outputs": [],
   "source": [
    "!python train.py --model attention164_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1765743738432,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "zOUaK0F8gSWj",
    "outputId": "59c2c72b-51fd-4114-e806-693c9f599649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   train_cifar_new.py\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mmodels/cifar_14.py\u001b[m\n",
      "\t\u001b[31mmodels/resnet128_tf.py\u001b[m\n",
      "\t\u001b[31mmodels/resnet164_tf.py\u001b[m\n",
      "\t\u001b[31mmodels/resnet92_tf.py\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1765744383130,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "Jujvxoh-hzHL",
    "outputId": "88dbd4c0-939a-400b-b899-335eac3c934d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   models/layers_tf.py\u001b[m\n",
      "\t\u001b[31mmodified:   train_cifar_new.py\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mmodels/cifar_14.py\u001b[m\n",
      "\t\u001b[31mmodels/resnet128_tf.py\u001b[m\n",
      "\t\u001b[31mmodels/resnet164_tf.py\u001b[m\n",
      "\t\u001b[31mmodels/resnet92_tf.py\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "! git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1765744407055,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "on0QZWjJkRM3",
    "outputId": "38e4748e-ae76-4f6d-b7fa-f71ffe2fe663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   models/layers_tf.py\u001b[m\n",
      "\t\u001b[31mmodified:   train_cifar_new.py\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mmodels/resnet128_tf.py\u001b[m\n",
      "\t\u001b[31mmodels/resnet164_tf.py\u001b[m\n",
      "\t\u001b[31mmodels/resnet92_tf.py\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1765744423132,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "Vo5s16kBkXCr",
    "outputId": "8952b2c2-9fec-4177-acd7-30f1251ef6f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git: 'add.' is not a git command. See 'git --help'.\n",
      "\n",
      "The most similar command is\n",
      "\tadd\n"
     ]
    }
   ],
   "source": [
    "!git add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxE2LuHyka8-"
   },
   "outputs": [],
   "source": [
    "! git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1765744655762,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "dfLJSedckc0F",
    "outputId": "b520c46f-1bce-4922-942d-809eec8c7c10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author identity unknown\n",
      "\n",
      "*** Please tell me who you are.\n",
      "\n",
      "Run\n",
      "\n",
      "  git config --global user.email \"you@example.com\"\n",
      "  git config --global user.name \"Your Name\"\n",
      "\n",
      "to set your account's default identity.\n",
      "Omit --global to set the identity only in this repository.\n",
      "\n",
      "fatal: unable to auto-detect email address (got 'root@b3de5db50796.(none)')\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"add baseline model, resnet92, resnet128, resnet164 respectivaly and updated the training part of codes. All the resnet models tries to be the same structure of with\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cecqE4ALlTwr"
   },
   "outputs": [],
   "source": [
    "!git config user.name \"MeiYue158\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1765744933593,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "mGv9e26fmREW",
    "outputId": "6705161c-c268-4ef5-a92c-e64e109872b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: unexpected EOF while looking for matching `\"'\n",
      "/bin/bash: -c: line 2: syntax error: unexpected end of file\n"
     ]
    }
   ],
   "source": [
    "!git config user.email “my2903@columbia.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0O25gCiumXk1"
   },
   "outputs": [],
   "source": [
    "!git config user.email \"my2903@columbia.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1765745372625,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "sXbSYoxSmjVH",
    "outputId": "5a03e1d2-865b-463b-dbc5-f945cc1ca0f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main bc19453] add baseline model, resnet92, resnet128, resnet164 respectivaly and updated the training part of codes. All the resnet models tries to be the same structure of with the one with attention model\n",
      " 5 files changed, 418 insertions(+), 37 deletions(-)\n",
      " create mode 100644 models/resnet128_tf.py\n",
      " create mode 100644 models/resnet164_tf.py\n",
      " create mode 100644 models/resnet92_tf.py\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"add baseline model, resnet92, resnet128, resnet164 respectivaly and updated the training part of codes. All the resnet models tries to be the same structure of with the one with attention model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1765745382527,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "VfFn_qQQoCx6",
    "outputId": "f0641aeb-b215-47c6-cb08-d4b48ebfb614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To https://github.com/ecbme4040/e4040-fall2025-project-swye.git\n",
      " \u001b[31m! [rejected]       \u001b[m main -> main (fetch first)\n",
      "\u001b[31merror: failed to push some refs to 'https://github.com/ecbme4040/e4040-fall2025-project-swye.git'\n",
      "\u001b[m\u001b[33mhint: Updates were rejected because the remote contains work that you do\u001b[m\n",
      "\u001b[33mhint: not have locally. This is usually caused by another repository pushing\u001b[m\n",
      "\u001b[33mhint: to the same ref. You may want to first integrate the remote changes\u001b[m\n",
      "\u001b[33mhint: (e.g., 'git pull ...') before pushing again.\u001b[m\n",
      "\u001b[33mhint: See the 'Note about fast-forwards' in 'git push --help' for details.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1158,
     "status": "ok",
     "timestamp": 1765745458556,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "jJy7q2DxoE_E",
    "outputId": "ac42626f-dfff-41fd-f1b1-b594089cb5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 21, done.\u001b[K\n",
      "remote: Counting objects:   4% (1/21)\u001b[K\rremote: Counting objects:   9% (2/21)\u001b[K\rremote: Counting objects:  14% (3/21)\u001b[K\rremote: Counting objects:  19% (4/21)\u001b[K\rremote: Counting objects:  23% (5/21)\u001b[K\rremote: Counting objects:  28% (6/21)\u001b[K\rremote: Counting objects:  33% (7/21)\u001b[K\rremote: Counting objects:  38% (8/21)\u001b[K\rremote: Counting objects:  42% (9/21)\u001b[K\rremote: Counting objects:  47% (10/21)\u001b[K\rremote: Counting objects:  52% (11/21)\u001b[K\rremote: Counting objects:  57% (12/21)\u001b[K\rremote: Counting objects:  61% (13/21)\u001b[K\rremote: Counting objects:  66% (14/21)\u001b[K\rremote: Counting objects:  71% (15/21)\u001b[K\rremote: Counting objects:  76% (16/21)\u001b[K\rremote: Counting objects:  80% (17/21)\u001b[K\rremote: Counting objects:  85% (18/21)\u001b[K\rremote: Counting objects:  90% (19/21)\u001b[K\rremote: Counting objects:  95% (20/21)\u001b[K\rremote: Counting objects: 100% (21/21)\u001b[K\rremote: Counting objects: 100% (21/21), done.\u001b[K\n",
      "remote: Compressing objects:  16% (1/6)\u001b[K\rremote: Compressing objects:  33% (2/6)\u001b[K\rremote: Compressing objects:  50% (3/6)\u001b[K\rremote: Compressing objects:  66% (4/6)\u001b[K\rremote: Compressing objects:  83% (5/6)\u001b[K\rremote: Compressing objects: 100% (6/6)\u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 14 (delta 11), reused 11 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects:   7% (1/14)\rUnpacking objects:  14% (2/14)\rUnpacking objects:  21% (3/14)\rUnpacking objects:  28% (4/14)\rUnpacking objects:  35% (5/14)\rUnpacking objects:  42% (6/14)\rUnpacking objects:  50% (7/14)\rUnpacking objects:  57% (8/14)\rUnpacking objects:  64% (9/14)\rUnpacking objects:  71% (10/14)\rUnpacking objects:  78% (11/14)\rUnpacking objects:  85% (12/14)\rUnpacking objects:  92% (13/14)\rUnpacking objects: 100% (14/14)\rUnpacking objects: 100% (14/14), 3.40 KiB | 871.00 KiB/s, done.\n",
      "From https://github.com/ecbme4040/e4040-fall2025-project-swye\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   65d81c0..8f38ef5  main       -> origin/main\n",
      "\u001b[33mhint: You have divergent branches and need to specify how to reconcile them.\u001b[m\n",
      "\u001b[33mhint: You can do so by running one of the following commands sometime before\u001b[m\n",
      "\u001b[33mhint: your next pull:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase false  # merge (the default strategy)\u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase true   # rebase\u001b[m\n",
      "\u001b[33mhint:   git config pull.ff only       # fast-forward only\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: You can replace \"git config\" with \"git config --global\" to set a default\u001b[m\n",
      "\u001b[33mhint: preference for all repositories. You can also pass --rebase, --no-rebase,\u001b[m\n",
      "\u001b[33mhint: or --ff-only on the command line to override the configured default per\u001b[m\n",
      "\u001b[33mhint: invocation.\u001b[m\n",
      "fatal: Need to specify how to reconcile divergent branches.\n"
     ]
    }
   ],
   "source": [
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 906,
     "status": "ok",
     "timestamp": 1765745493492,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "HPoGd2SwoXf4",
    "outputId": "b0f82464-5afd-4284-eebb-b72df38a2a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/ecbme4040/e4040-fall2025-project-swye\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Rebasing (1/1)\rAuto-merging train_cifar_new.py\n",
      "CONFLICT (content): Merge conflict in train_cifar_new.py\n",
      "error: could not apply bc19453... add baseline model, resnet92, resnet128, resnet164 respectivaly and updated the training part of codes. All the resnet models tries to be the same structure of with the one with attention model\n",
      "\u001b[33mhint: Resolve all conflicts manually, mark them as resolved with\u001b[m\n",
      "\u001b[33mhint: \"git add/rm <conflicted_files>\", then run \"git rebase --continue\".\u001b[m\n",
      "\u001b[33mhint: You can instead skip this commit: run \"git rebase --skip\".\u001b[m\n",
      "\u001b[33mhint: To abort and get back to the state before \"git rebase\", run \"git rebase --abort\".\u001b[m\n",
      "Could not apply bc19453... add baseline model, resnet92, resnet128, resnet164 respectivaly and updated the training part of codes. All the resnet models tries to be the same structure of with the one with attention model\n"
     ]
    }
   ],
   "source": [
    "!git pull --rebase origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1765745886544,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "heMYwckHogFQ",
    "outputId": "9375bbf7-4760-45e1-ee77-9e802df57f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'e4040-fall2025-project-swye'\n",
      "/content/e4040-fall2025-project-swye\n"
     ]
    }
   ],
   "source": [
    "%cd e4040-fall2025-project-swye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1765745898637,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "6ulhXB4iprt2",
    "outputId": "d5aa78c9-7e40-4bf6-ccd1-6a7ffbcbb2c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " checkpoints\t        plot_results.py     train_cifar_100.py\n",
      " compare_attention.py   README.md\t   'train_cifar_new 12.13.py'\n",
      " demo.py\t        README-proj.md\t   'train_cifar_new 12.14.py'\n",
      " demo_ten.py\t        README_TF.md\t    train_cifar_new.py\n",
      " models\t\t        run_all_models.py   train_cifar_tf.py\n",
      " parameter.py\t        select_best.py\t    train_imagenet.py\n",
      " plot.py\t        train_best.py\t    untitled.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJDtwaQlqDLd"
   },
   "outputs": [],
   "source": [
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1765745910806,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "_9kXlgKmqE5q",
    "outputId": "2c552846-669c-406e-dfac-74566517feb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " checkpoints\t        plot_results.py     train_cifar_100.py\n",
      " compare_attention.py   README.md\t   'train_cifar_new 12.13.py'\n",
      " demo.py\t        README-proj.md\t   'train_cifar_new 12.14.py'\n",
      " demo_ten.py\t        README_TF.md\t    train_cifar_new.py\n",
      " models\t\t        run_all_models.py   train_cifar_tf.py\n",
      " parameter.py\t        select_best.py\t    train_imagenet.py\n",
      " plot.py\t        train_best.py\t    untitled.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1765745917068,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "bCEhYGByqGHk",
    "outputId": "e3f5c777-a3a8-42ff-9501-0693098b076c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1765745920870,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "OhdYI-t5qHrX",
    "outputId": "14c53728-4825-4c77-ce9c-0437596072ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34me4040-fall2025-project-swye\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1765745940461,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "_PfBxboXqIn1",
    "outputId": "f8de7b0a-0c8c-4604-ec4c-d704aff2d3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/e4040-fall2025-project-swye\n"
     ]
    }
   ],
   "source": [
    "cd e4040-fall2025-project-swye/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1765745980507,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "d9ZyjI_eqQJJ",
    "outputId": "4a708b87-406b-438c-b529-ed82496aa95f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31minteractive rebase in progress; onto \u001b[m8f38ef5\n",
      "Last command done (1 command done):\n",
      "   pick bc19453 add baseline model, resnet92, resnet128, resnet164 respectivaly and updated the training part of codes. All the resnet models tries to be the same structure of with the one with attention model\n",
      "No commands remaining.\n",
      "You are currently rebasing branch 'main' on '8f38ef5'.\n",
      "  (fix conflicts and then run \"git rebase --continue\")\n",
      "  (use \"git rebase --skip\" to skip this patch)\n",
      "  (use \"git rebase --abort\" to check out the original branch)\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git restore --staged <file>...\" to unstage)\n",
      "\t\u001b[32mmodified:   models/layers_tf.py\u001b[m\n",
      "\t\u001b[32mnew file:   models/resnet128_tf.py\u001b[m\n",
      "\t\u001b[32mnew file:   models/resnet164_tf.py\u001b[m\n",
      "\t\u001b[32mnew file:   models/resnet92_tf.py\u001b[m\n",
      "\n",
      "Unmerged paths:\n",
      "  (use \"git restore --staged <file>...\" to unstage)\n",
      "  (use \"git add <file>...\" to mark resolution)\n",
      "\t\u001b[31mboth modified:   train_cifar_new.py\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKXm47d_qSHu"
   },
   "outputs": [],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1765746007924,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "8nTwTMEfqYu3",
    "outputId": "ff810797-779c-4d83-fb74-533194e71c0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[detached HEAD a42ae07] add baseline model, resnet92, resnet128, resnet164 respectivaly and updated the training part of codes. All the resnet models tries to be the same structure of with the one with attention model\n",
      " 5 files changed, 449 insertions(+), 27 deletions(-)\n",
      " create mode 100644 models/resnet128_tf.py\n",
      " create mode 100644 models/resnet164_tf.py\n",
      " create mode 100644 models/resnet92_tf.py\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"add baseline model, resnet92, resnet128, resnet164 respectivaly and updated the training part of codes. All the resnet models tries to be the same structure of with the one with attention model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1765746014772,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "KXcYR2lDqd4O",
    "outputId": "9be984f7-8f9c-43f6-c311-327dc4b468b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 1.21 KiB | 1.21 MiB/s, done.\n",
      "From https://github.com/ecbme4040/e4040-fall2025-project-swye\n",
      "   8f38ef5..9808ee4  main       -> origin/main\n",
      "\u001b[33mhint: You have divergent branches and need to specify how to reconcile them.\u001b[m\n",
      "\u001b[33mhint: You can do so by running one of the following commands sometime before\u001b[m\n",
      "\u001b[33mhint: your next pull:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase false  # merge (the default strategy)\u001b[m\n",
      "\u001b[33mhint:   git config pull.rebase true   # rebase\u001b[m\n",
      "\u001b[33mhint:   git config pull.ff only       # fast-forward only\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: You can replace \"git config\" with \"git config --global\" to set a default\u001b[m\n",
      "\u001b[33mhint: preference for all repositories. You can also pass --rebase, --no-rebase,\u001b[m\n",
      "\u001b[33mhint: or --ff-only on the command line to override the configured default per\u001b[m\n",
      "\u001b[33mhint: invocation.\u001b[m\n",
      "fatal: Need to specify how to reconcile divergent branches.\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1765746057202,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "OrK9w-WyqfSv",
    "outputId": "7162287b-df0b-40e7-b253-e20cf1a7edb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: git config [<options>]\n",
      "\n",
      "Config file location\n",
      "    --global              use global config file\n",
      "    --system              use system config file\n",
      "    --local               use repository config file\n",
      "    --worktree            use per-worktree config file\n",
      "    -f, --file <file>     use given config file\n",
      "    --blob <blob-id>      read config from given blob object\n",
      "\n",
      "Action\n",
      "    --get                 get value: name [value-pattern]\n",
      "    --get-all             get all values: key [value-pattern]\n",
      "    --get-regexp          get values for regexp: name-regex [value-pattern]\n",
      "    --get-urlmatch        get value specific for the URL: section[.var] URL\n",
      "    --replace-all         replace all matching variables: name value [value-pattern]\n",
      "    --add                 add a new variable: name value\n",
      "    --unset               remove a variable: name [value-pattern]\n",
      "    --unset-all           remove all matches: name [value-pattern]\n",
      "    --rename-section      rename section: old-name new-name\n",
      "    --remove-section      remove a section: name\n",
      "    -l, --list            list all\n",
      "    --fixed-value         use string equality when comparing values to 'value-pattern'\n",
      "    -e, --edit            open an editor\n",
      "    --get-color           find the color configured: slot [default]\n",
      "    --get-colorbool       find the color setting: slot [stdout-is-tty]\n",
      "\n",
      "Type\n",
      "    -t, --type <>         value is given this type\n",
      "    --bool                value is \"true\" or \"false\"\n",
      "    --int                 value is decimal number\n",
      "    --bool-or-int         value is --bool or --int\n",
      "    --bool-or-str         value is --bool or string\n",
      "    --path                value is a path (file or directory name)\n",
      "    --expiry-date         value is an expiry date\n",
      "\n",
      "Other\n",
      "    -z, --null            terminate values with NUL byte\n",
      "    --name-only           show variable names only\n",
      "    --includes            respect include directives on lookup\n",
      "    --show-origin         show origin of config (file, standard input, blob, command line)\n",
      "    --show-scope          show scope of config (worktree, local, global, system, command)\n",
      "    --default <value>     with --get, use default value when missing entry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git config --global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 979,
     "status": "ok",
     "timestamp": 1765746327862,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "tbir2TOMqgx3",
    "outputId": "e0a6fece-7098-4a72-ded6-40d669ecfebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/ecbme4040/e4040-fall2025-project-swye\n",
      " * branch            main       -> FETCH_HEAD\n",
      "fatal: It seems that there is already a rebase-merge directory, and\n",
      "I wonder if you are in the middle of another rebase.  If that is the\n",
      "case, please try\n",
      "\tgit rebase (--continue | --abort | --skip)\n",
      "If that is not the case, please\n",
      "\trm -fr \".git/rebase-merge\"\n",
      "and run me again.  I am stopping in case you still have something\n",
      "valuable there.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git pull --rebase origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1765746388164,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "IlcaQv6nrrx4",
    "outputId": "c31ac6da-ad9c-4e9a-b86c-66f8a3e34508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31minteractive rebase in progress; onto \u001b[m8f38ef5\n",
      "Last command done (1 command done):\n",
      "   pick bc19453 add baseline model, resnet92, resnet128, resnet164 respectivaly and updated the training part of codes. All the resnet models tries to be the same structure of with the one with attention model\n",
      "No commands remaining.\n",
      "You are currently editing a commit while rebasing branch 'main' on '8f38ef5'.\n",
      "  (use \"git commit --amend\" to amend the current commit)\n",
      "  (use \"git rebase --continue\" once you are satisfied with your changes)\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94,
     "status": "ok",
     "timestamp": 1765746439162,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "BXVSFgMPr6tZ",
    "outputId": "4f5b39a5-d492-419d-d2fb-77f91d221f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\u001b[KSuccessfully rebased and updated refs/heads/main.\n"
     ]
    }
   ],
   "source": [
    "!git rebase --continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 588,
     "status": "ok",
     "timestamp": 1765746459367,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "4nHV8M4IsK_W",
    "outputId": "0a643a56-b577-4029-9af6-02e9ef733447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To https://github.com/ecbme4040/e4040-fall2025-project-swye.git\n",
      " \u001b[31m! [rejected]       \u001b[m main -> main (non-fast-forward)\n",
      "\u001b[31merror: failed to push some refs to 'https://github.com/ecbme4040/e4040-fall2025-project-swye.git'\n",
      "\u001b[m\u001b[33mhint: Updates were rejected because the tip of your current branch is behind\u001b[m\n",
      "\u001b[33mhint: its remote counterpart. Integrate the remote changes (e.g.\u001b[m\n",
      "\u001b[33mhint: 'git pull ...') before pushing again.\u001b[m\n",
      "\u001b[33mhint: See the 'Note about fast-forwards' in 'git push --help' for details.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1765746569462,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "pdEPkK45smNW",
    "outputId": "4922f544-82f0-4743-83eb-aac8fe380319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 16, done.\n",
      "Counting objects:   7% (1/13)\rCounting objects:  15% (2/13)\rCounting objects:  23% (3/13)\rCounting objects:  30% (4/13)\rCounting objects:  38% (5/13)\rCounting objects:  46% (6/13)\rCounting objects:  53% (7/13)\rCounting objects:  61% (8/13)\rCounting objects:  69% (9/13)\rCounting objects:  76% (10/13)\rCounting objects:  84% (11/13)\rCounting objects:  92% (12/13)\rCounting objects: 100% (13/13)\rCounting objects: 100% (13/13), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects:  12% (1/8)\rCompressing objects:  25% (2/8)\rCompressing objects:  37% (3/8)\rCompressing objects:  50% (4/8)\rCompressing objects:  62% (5/8)\rCompressing objects:  75% (6/8)\rCompressing objects:  87% (7/8)\rCompressing objects: 100% (8/8)\rCompressing objects: 100% (8/8), done.\n",
      "Writing objects:  12% (1/8)\rWriting objects:  25% (2/8)\rWriting objects:  37% (3/8)\rWriting objects:  50% (4/8)\rWriting objects:  62% (5/8)\rWriting objects:  75% (6/8)\rWriting objects:  87% (7/8)\rWriting objects: 100% (8/8)\rWriting objects: 100% (8/8), 3.64 KiB | 3.64 MiB/s, done.\n",
      "Total 8 (delta 6), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (6/6), completed with 4 local objects.\u001b[K\n",
      "To https://github.com/ecbme4040/e4040-fall2025-project-swye.git\n",
      " + 9808ee4...a42ae07 main -> main (forced update)\n"
     ]
    }
   ],
   "source": [
    "!git push --force-with-lease origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1765747249752,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "Fqh526yfsmsH",
    "outputId": "9e5ab446-648b-426e-d944-9cea9b07d11a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   train_cifar_new.py\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKpF0LOCvNCW"
   },
   "outputs": [],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1765747378027,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "vhLJ712ZvROT",
    "outputId": "ff6e7d52-722f-4257-9789-b1e081a3d615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 9736e30] made modification of train_cifar_new.py to make sure that the file supports 92, 128 and 156 models of both resnet with and without attention mechanism\n",
      " 1 file changed, 179 insertions(+), 314 deletions(-)\n",
      " rewrite train_cifar_new.py (71%)\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"made modification of train_cifar_new.py to make sure that the file supports 92, 128 and 156 models of both resnet with and without attention mechanism\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1220,
     "status": "ok",
     "timestamp": 1765747392628,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "UpClWPGUvsYK",
    "outputId": "926fdcf4-e459-424a-b9ab-ee4f051e40a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 5, done.\n",
      "Counting objects:  20% (1/5)\rCounting objects:  40% (2/5)\rCounting objects:  60% (3/5)\rCounting objects:  80% (4/5)\rCounting objects: 100% (5/5)\rCounting objects: 100% (5/5), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects:  33% (1/3)\rCompressing objects:  66% (2/3)\rCompressing objects: 100% (3/3)\rCompressing objects: 100% (3/3), done.\n",
      "Writing objects:  33% (1/3)\rWriting objects:  66% (2/3)\rWriting objects: 100% (3/3)\rWriting objects: 100% (3/3), 1.12 KiB | 1.12 MiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/ecbme4040/e4040-fall2025-project-swye.git\n",
      "   a42ae07..9736e30  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1765752638950,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "bxiKbtd1vvrI",
    "outputId": "fe51a90f-785e-4e60-8c85-72a82b83c0d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 3 (delta 2), reused 2 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 1.38 KiB | 1.38 MiB/s, done.\n",
      "From https://github.com/ecbme4040/e4040-fall2025-project-swye\n",
      "   9736e30..7d6dbc0  main       -> origin/main\n"
     ]
    }
   ],
   "source": [
    "!git fetch origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1765753848004,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "GYoaefGNDwiT",
    "outputId": "2a9d32c3-ddf2-4259-8944-0c2e41557777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/           plot_results.py     train_cifar_100.py\n",
      " compare_attention.py   README.md          'train_cifar_new 12.13.py'\n",
      " demo.py                README-proj.md     'train_cifar_new 12.14.py'\n",
      " demo_ten.py            README_TF.md        train_cifar_new.py\n",
      " \u001b[01;34mmodels\u001b[0m/                run_all_models.py   train_cifar_tf.py\n",
      " parameter.py           select_best.py      train_imagenet.py\n",
      " plot.py                train_best.py       untitled.txt\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381652,
     "status": "ok",
     "timestamp": 1765754572443,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "wywmlUEZIX9F",
    "outputId": "ffca32cf-aa43-44f2-afcd-0ce4d7bfdaf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-14 23:16:31.069640: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765754191.087741   86160 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765754191.093428   86160 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765754191.107980   86160 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765754191.108008   86160 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765754191.108011   86160 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765754191.108013   86160 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-14 23:16:31.112260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: resnet92\n",
      "Epochs: 40\n",
      "Batch size: 64\n",
      "Learning rate: 0.001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 45000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "\n",
      "Building resnet92 model...\n",
      "2025-12-14 23:16:40.098970: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765754200.099099   86160 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1765754203.026269   86160 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "\u001b[1mModel: \"res_net92\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m213,504\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m844,800\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_7 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m3,360,768\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_8 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_9 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │    \u001b[32m14,974,976\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_55          │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m22,870,218\u001b[0m (87.24 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m22,824,650\u001b[0m (87.07 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m45,568\u001b[0m (178.00 KB)\n",
      "\n",
      "Total parameters: 22,870,218\n",
      "Epoch 1/40\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765754228.270891   86247 service.cc:152] XLA service 0x117100003650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765754228.270936   86247 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-14 23:17:08.852764: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765754246.086973   86247 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.3078 - loss: 2.0008\n",
      "Epoch 1: val_accuracy improved from -inf to 0.37340, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 77ms/step - accuracy: 0.3079 - loss: 2.0006 - val_accuracy: 0.3734 - val_loss: 3.5125 - learning_rate: 0.0010\n",
      "Epoch 2/40\n",
      "\u001b[1m703/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3920 - loss: 1.7334\n",
      "Epoch 2: val_accuracy improved from 0.37340 to 0.46560, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 47ms/step - accuracy: 0.3922 - loss: 1.7331 - val_accuracy: 0.4656 - val_loss: 1.8012 - learning_rate: 0.0010\n",
      "Epoch 3/40\n",
      "\u001b[1m703/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.4709 - loss: 1.5566\n",
      "Epoch 3: val_accuracy improved from 0.46560 to 0.50840, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 48ms/step - accuracy: 0.4709 - loss: 1.5565 - val_accuracy: 0.5084 - val_loss: 1.3914 - learning_rate: 0.0010\n",
      "Epoch 4/40\n",
      "\u001b[1m703/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.4891 - loss: 1.4684\n",
      "Epoch 4: val_accuracy improved from 0.50840 to 0.61100, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 48ms/step - accuracy: 0.4891 - loss: 1.4682 - val_accuracy: 0.6110 - val_loss: 1.1101 - learning_rate: 0.0010\n",
      "Epoch 5/40\n",
      "\u001b[1m703/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.4529 - loss: 1.6253\n",
      "Epoch 5: val_accuracy did not improve from 0.61100\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 46ms/step - accuracy: 0.4530 - loss: 1.6249 - val_accuracy: 0.5342 - val_loss: 1.6927 - learning_rate: 0.0010\n",
      "Epoch 6/40\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5477 - loss: 1.2978\n",
      "Epoch 6: val_accuracy did not improve from 0.61100\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 47ms/step - accuracy: 0.5478 - loss: 1.2976 - val_accuracy: 0.6000 - val_loss: 1.2049 - learning_rate: 0.0010\n",
      "Epoch 7/40\n",
      "\u001b[1m703/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5829 - loss: 1.2073\n",
      "Epoch 7: val_accuracy improved from 0.61100 to 0.62140, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 48ms/step - accuracy: 0.5830 - loss: 1.2071 - val_accuracy: 0.6214 - val_loss: 1.1433 - learning_rate: 0.0010\n",
      "Epoch 8/40\n",
      "\u001b[1m703/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6240 - loss: 1.0807\n",
      "Epoch 8: val_accuracy improved from 0.62140 to 0.65380, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 48ms/step - accuracy: 0.6241 - loss: 1.0806 - val_accuracy: 0.6538 - val_loss: 0.9974 - learning_rate: 0.0010\n",
      "Epoch 9/40\n",
      "\u001b[1m703/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6436 - loss: 1.0375\n",
      "Epoch 9: val_accuracy improved from 0.65380 to 0.69940, saving model to ./checkpoints/resnet92_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 48ms/step - accuracy: 0.6436 - loss: 1.0375 - val_accuracy: 0.6994 - val_loss: 0.8997 - learning_rate: 0.0010\n",
      "Epoch 10/40\n",
      "\u001b[1m 41/704\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 45ms/step - accuracy: 0.6580 - loss: 0.9696Traceback (most recent call last):\n",
      "  File \"/content/e4040-fall2025-project-swye/train_cifar_new.py\", line 285, in <module>\n",
      "    main()\n",
      "  File \"/content/e4040-fall2025-project-swye/train_cifar_new.py\", line 223, in main\n",
      "    history=model.fit(train_ds,epochs=args.epochs,validation_data=val_ds,callbacks=callbacks_list,verbose=1)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n",
      "    logs = self.train_function(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 221, in function\n",
      "    if not opt_outputs.has_value():\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/optional_ops.py\", line 176, in has_value\n",
      "    return gen_optional_ops.optional_has_value(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_optional_ops.py\", line 172, in optional_has_value\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!git push origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1765754581695,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "Zz-CvUyIIcAj",
    "outputId": "18f7540d-ab06-40d8-de8f-bd5bac5b0456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is behind 'origin/main' by 1 commit, and can be fast-forwarded.\n",
      "  (use \"git pull\" to update your local branch)\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   train_cifar_new.py\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpbCRMeMLLFZ"
   },
   "outputs": [],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1765754760646,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "Vwc21C3KLrOz",
    "outputId": "0fdccf64-895e-4755-cd07-b46bd198f571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main f47e4ae] Add error-rate reporting derived from Top-1 / Top-5 accuracy\n",
      " 1 file changed, 195 insertions(+), 88 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Add error-rate reporting derived from Top-1 / Top-5 accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1765754771550,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "AuJTenflL2xg",
    "outputId": "887c66a0-1e5f-4852-f052-f1f0aadcd760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To https://github.com/ecbme4040/e4040-fall2025-project-swye.git\n",
      " \u001b[31m! [rejected]       \u001b[m main -> main (fetch first)\n",
      "\u001b[31merror: failed to push some refs to 'https://github.com/ecbme4040/e4040-fall2025-project-swye.git'\n",
      "\u001b[m\u001b[33mhint: Updates were rejected because the remote contains work that you do\u001b[m\n",
      "\u001b[33mhint: not have locally. This is usually caused by another repository pushing\u001b[m\n",
      "\u001b[33mhint: to the same ref. You may want to first integrate the remote changes\u001b[m\n",
      "\u001b[33mhint: (e.g., 'git pull ...') before pushing again.\u001b[m\n",
      "\u001b[33mhint: See the 'Note about fast-forwards' in 'git push --help' for details.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1765754827200,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "-_nfielNL5OF",
    "outputId": "13cd3098-d4e0-4661-b62c-adb6a4ef12fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch and 'origin/main' have diverged,\n",
      "and have 1 and 1 different commits each, respectively.\n",
      "  (use \"git pull\" to merge the remote branch into yours)\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1765754840016,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "oBpiPg_XMHBo",
    "outputId": "5bbec097-2803-43f5-c8ec-3d0869cc258a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 308 bytes | 308.00 KiB/s, done.\n",
      "From https://github.com/ecbme4040/e4040-fall2025-project-swye\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   7d6dbc0..85e809d  main       -> origin/main\n",
      "Rebasing (1/1)\r\r\u001b[KSuccessfully rebased and updated refs/heads/main.\n"
     ]
    }
   ],
   "source": [
    "git pull --rebase origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124,
     "status": "ok",
     "timestamp": 1765754858682,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "JRsrivrbMJ5-",
    "outputId": "57bf2435-f31e-4218-ed6d-0bf2cb4c01f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `newline'\n",
      "/bin/bash: -c: line 1: `git add <fixed-files>'\n"
     ]
    }
   ],
   "source": [
    "!git add <fixed-files>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1765754869176,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "7NvzVm2hMOtT",
    "outputId": "8ffcf810-4dd4-4abe-9b94-1ea860a2ecb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: No rebase in progress?\n"
     ]
    }
   ],
   "source": [
    "!git rebase --continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1260,
     "status": "ok",
     "timestamp": 1765754930787,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "S1urvttwMRRe",
    "outputId": "c6aa6cfe-32d2-4cfb-8164-51c4a5c6fcb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 5, done.\n",
      "Counting objects:  20% (1/5)\rCounting objects:  40% (2/5)\rCounting objects:  60% (3/5)\rCounting objects:  80% (4/5)\rCounting objects: 100% (5/5)\rCounting objects: 100% (5/5), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects:  33% (1/3)\rCompressing objects:  66% (2/3)\rCompressing objects: 100% (3/3)\rCompressing objects: 100% (3/3), done.\n",
      "Writing objects:  33% (1/3)\rWriting objects:  66% (2/3)\rWriting objects: 100% (3/3)\rWriting objects: 100% (3/3), 2.97 KiB | 2.97 MiB/s, done.\n",
      "Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
      "To https://github.com/ecbme4040/e4040-fall2025-project-swye.git\n",
      "   85e809d..6107bf6  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push origin main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6351OCgrRkYh"
   },
   "source": [
    "## run model attention56 arl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292936,
     "status": "ok",
     "timestamp": 1765756461603,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "w2OhveJSMgCI",
    "outputId": "de8844fa-7fb8-4a1c-c61d-af944d9fd6e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-14 23:49:29.547930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765756169.567333   95068 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765756169.573211   95068 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765756169.587984   95068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765756169.588006   95068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765756169.588009   95068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765756169.588011   95068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-14 23:49:29.592395: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention56\n",
      "Epochs: 5\n",
      "Batch size: 128\n",
      "Learning rate: 0.1\n",
      "Momentum: 0.9\n",
      "Weight decay: 0.0001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 50000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "2025-12-14 23:49:38.009071: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765756178.009199   95068 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "\n",
      "Building attention56 model...\n",
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'residual_attention_model56', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "\u001b[1mModel: \"residual_attention_model56\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module                │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_1              │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_2              │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_115         │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\n",
      "Total parameters: 0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 1/5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765756218.976939   95156 service.cc:152] XLA service 0x129d70030480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765756218.976977   95156 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-14 23:50:20.042274: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765756223.183985   95156 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1765756247.692557   95156 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.2177 - loss: 3.5636 - top_5_accuracy: 0.7105\n",
      "Epoch 1: val_accuracy improved from -inf to 0.35020, saving model to ./checkpoints/attention56_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 183ms/step - accuracy: 0.2178 - loss: 3.5613 - top_5_accuracy: 0.7107 - val_accuracy: 0.3502 - val_loss: 1.8876 - val_top_5_accuracy: 0.8650 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 2/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3705 - loss: 1.7305 - top_5_accuracy: 0.8716\n",
      "Epoch 2: val_accuracy improved from 0.35020 to 0.44200, saving model to ./checkpoints/attention56_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 86ms/step - accuracy: 0.3706 - loss: 1.7304 - top_5_accuracy: 0.8716 - val_accuracy: 0.4420 - val_loss: 1.5304 - val_top_5_accuracy: 0.9106 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 3/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4306 - loss: 1.5382 - top_5_accuracy: 0.9099\n",
      "Epoch 3: val_accuracy improved from 0.44200 to 0.47460, saving model to ./checkpoints/attention56_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 88ms/step - accuracy: 0.4307 - loss: 1.5382 - top_5_accuracy: 0.9099 - val_accuracy: 0.4746 - val_loss: 1.5696 - val_top_5_accuracy: 0.9213 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 4/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.4816 - loss: 1.4180 - top_5_accuracy: 0.9248\n",
      "Epoch 4: val_accuracy improved from 0.47460 to 0.54310, saving model to ./checkpoints/attention56_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 88ms/step - accuracy: 0.4816 - loss: 1.4179 - top_5_accuracy: 0.9249 - val_accuracy: 0.5431 - val_loss: 1.2547 - val_top_5_accuracy: 0.9455 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 5/5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5196 - loss: 1.3158 - top_5_accuracy: 0.9368\n",
      "Epoch 5: val_accuracy improved from 0.54310 to 0.57670, saving model to ./checkpoints/attention56_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 87ms/step - accuracy: 0.5196 - loss: 1.3157 - top_5_accuracy: 0.9368 - val_accuracy: 0.5767 - val_loss: 1.1774 - val_top_5_accuracy: 0.9527 - learning_rate: 0.1000\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "============================================================\n",
      "Final Test Top-1 Accuracy: 57.67%\n",
      "Final Test Top-1 Error: 42.33%\n",
      "Final Test Top-5 Accuracy: 95.27%\n",
      "Final Test Top-5 Error: 4.73%\n",
      "Final Test Loss: 1.1774\n",
      "============================================================\n",
      "\n",
      "Best Training Top-1 Accuracy: 52.75%\n",
      "Best Training Top-1 Error: 47.25%\n",
      "Best Validation Top-1 Accuracy: 57.67%\n",
      "Best Validation Top-1 Error: 42.33%\n",
      "Best Training Top-5 Accuracy: 93.87%\n",
      "Best Training Top-5 Error: 6.13%\n",
      "Best Validation Top-5 Accuracy: 95.27%\n",
      "Best Validation Top-5 Error: 4.73%\n",
      "Final Test Top-1 Accuracy: 57.67%\n",
      "Final Test Top-1 Error: 42.33%\n",
      "\n",
      "Plots saved to ./results/plots/\n",
      "Results appended to results.csv\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/attention56_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python train_cifar_new.py --model attention56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1230,
     "status": "ok",
     "timestamp": 1765758738039,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "4kRJfnOXNQLC",
    "outputId": "89226def-a9d4-4faf-ad25-dd57386dfb8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 660 bytes | 660.00 KiB/s, done.\n",
      "From https://github.com/ecbme4040/e4040-fall2025-project-swye\n",
      "   6107bf6..83418f4  main       -> origin/main\n"
     ]
    }
   ],
   "source": [
    "!git fetch origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdTEcCapRpIO"
   },
   "source": [
    "### run model attention 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25364,
     "status": "ok",
     "timestamp": 1765758833841,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "kIof49fBbBi_",
    "outputId": "5df6ea2d-dd6d-4938-9fa9-ff9a03fc0a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 00:33:28.813631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765758808.833355  106290 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765758808.839348  106290 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765758808.855756  106290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765758808.855781  106290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765758808.855784  106290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765758808.855786  106290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 00:33:28.860136: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention128\n",
      "Epochs: 40\n",
      "Batch size: 64\n",
      "Learning rate: 0.001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 45000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "\n",
      "Building attention128 model...\n",
      "2025-12-15 00:33:38.368115: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765758818.368258  106290 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1765758825.540898  106290 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "\u001b[1mModel: \"residual_attention_model128\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module                │ ?                      │     \u001b[32m1,061,888\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_1              │ ?                      │     \u001b[32m3,922,944\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_2              │ ?                      │    \u001b[32m12,251,136\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m6,037,504\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_133         │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m26,749,642\u001b[0m (102.04 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m26,671,050\u001b[0m (101.74 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m78,592\u001b[0m (307.00 KB)\n",
      "\n",
      "Total parameters: 26,749,642\n",
      "Epoch 1/40\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/math_ops.py\", line 1012, in cast\n",
      "    x = ops.convert_to_tensor(x, name=\"x\")\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/profiler/trace.py\", line 183, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\", line 736, in convert_to_tensor\n",
      "    return tensor_conversion_registry.convert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 209, in convert\n",
      "    return overload(dtype, name)  #  pylint: disable=not-callable\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/core.py\", line 84, in __tf_tensor__\n",
      "    return tf.convert_to_tensor(self.value, dtype=dtype, name=name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/tensor_conversion.py\", line 161, in convert_to_tensor_v2_with_dispatch\n",
      "    return convert_to_tensor_v2(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/tensor_conversion.py\", line 171, in convert_to_tensor_v2\n",
      "    return tensor_conversion_registry.convert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 234, in convert\n",
      "    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 2378, in _dense_var_to_tensor\n",
      "    return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1624, in _dense_var_to_tensor\n",
      "    return self.value()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 658, in value\n",
      "    return self._read_variable_op()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 843, in _read_variable_op\n",
      "    result = read_and_set_handle(no_copy)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 833, in read_and_set_handle\n",
      "    result = gen_resource_variable_ops.read_variable_op(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 548, in read_variable_op\n",
      "    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/op_def_library.py\", line 778, in _apply_op_helper\n",
      "    _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/op_def_library.py\", line 551, in _ExtractInputsAndAttrs\n",
      "    values = ops.convert_to_tensor(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/profiler/trace.py\", line 183, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\", line 736, in convert_to_tensor\n",
      "    return tensor_conversion_registry.convert(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 209, in convert\n",
      "    return overload(dtype, name)  #  pylint: disable=not-callable\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\", line 630, in __tf_tensor__\n",
      "    return graph.capture(self, name=name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/func_graph.py\", line 619, in capture\n",
      "    return self._function_captures.capture_by_value(self, tensor, name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/core/function/capture/capture_container.py\", line 141, in capture_by_value\n",
      "    return self._create_placeholder_helper(graph, tensor, name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/core/function/capture/capture_container.py\", line 286, in _create_placeholder_helper\n",
      "    self.add_or_replace(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/core/function/capture/capture_container.py\", line 176, in add_or_replace\n",
      "    self._by_val_tracetype[key] = trace_type.from_value(external)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\", line 145, in from_value\n",
      "    generated_type = value.__tf_tracing_type__(context)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/tensor.py\", line 754, in __tf_tracing_type__\n",
      "    spec = TensorSpec(self.shape, dtype)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/tensor.py\", line 866, in __init__\n",
      "    self._dtype = dtypes.as_dtype(dtype)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/dtypes.py\", line 830, in as_dtype\n",
      "    @tf_export(\"dtypes.as_dtype\", \"as_dtype\")\n",
      "    \n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/e4040-fall2025-project-swye/train_cifar_new 12.14.py\", line 299, in <module>\n",
      "    main()\n",
      "  File \"/content/e4040-fall2025-project-swye/train_cifar_new 12.14.py\", line 243, in main\n",
      "    history=model.fit(train_ds,epochs=args.epochs,validation_data=val_ds,callbacks=callbacks_list,verbose=1)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n",
      "    logs = self.train_function(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n",
      "    opt_outputs = multi_step_on_iterator(iterator)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 889, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 696, in _initialize\n",
      "    self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 178, in trace_function\n",
      "    concrete_function = _maybe_define_function(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 283, in _maybe_define_function\n",
      "    concrete_function = _create_concrete_function(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 310, in _create_concrete_function\n",
      "    traced_func_graph = func_graph_module.func_graph_from_py_func(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/func_graph.py\", line 1060, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 599, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py\", line 41, in autograph_handler\n",
      "    return api.converted_call(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\", line 339, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n",
      "    one_step_on_data(iterator.get_next())\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 889, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 696, in _initialize\n",
      "    self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 178, in trace_function\n",
      "    concrete_function = _maybe_define_function(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 283, in _maybe_define_function\n",
      "    concrete_function = _create_concrete_function(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 310, in _create_concrete_function\n",
      "    traced_func_graph = func_graph_module.func_graph_from_py_func(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/func_graph.py\", line 1060, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 599, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py\", line 41, in autograph_handler\n",
      "    return api.converted_call(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\", line 339, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\", line 459, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 114, in one_step_on_data\n",
      "    outputs = self.distribute_strategy.run(step_function, args=(data,))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1673, in run\n",
      "    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 3263, in call_for_each_replica\n",
      "    return self._call_for_each_replica(fn, args, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 4061, in _call_for_each_replica\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 58, in train_step\n",
      "    y_pred = self(x, training=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
      "    outputs = super().__call__(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
      "    return call_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/e4040-fall2025-project-swye/models/attention128_tf.py\", line 103, in call\n",
      "    x = self.att3(x, training=training)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
      "    outputs = super().__call__(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
      "    return call_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/e4040-fall2025-project-swye/models/layers_tf.py\", line 254, in call\n",
      "    m = self.mask(inputs, training=training)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
      "    outputs = super().__call__(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
      "    return call_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/e4040-fall2025-project-swye/models/layers_tf.py\", line 183, in call\n",
      "    out = block(out, training=training)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
      "    outputs = super().__call__(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
      "    return call_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/e4040-fall2025-project-swye/models/layers_tf.py\", line 64, in call\n",
      "    out = self.bn2(out, training=training)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
      "    outputs = super().__call__(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
      "    return call_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/layers/normalization/batch_normalization.py\", line 251, in call\n",
      "    moving_variance = ops.cast(self.moving_variance, inputs.dtype)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/ops/core.py\", line 803, in cast\n",
      "    return backend.core.cast(x, dtype)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/core.py\", line 217, in cast\n",
      "    return tf.cast(x, dtype=dtype)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/dispatch.py\", line 1260, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/math_ops.py\", line 1000, in cast\n",
      "    with ops.name_scope(name, \"Cast\", [x]) as name:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\", line 5630, in __exit__\n",
      "    self._name_scope.__exit__(*exc_info)\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train_cifar_new\\ 12.14.py --model attention128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1129535,
     "status": "ok",
     "timestamp": 1765765674222,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "VGmcxu18bTBr",
    "outputId": "a23425d8-5417-4157-9363-8b6b4e067d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 01:34:13.877807: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765762453.896517  124025 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765762453.902135  124025 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765762453.916469  124025 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765762453.916495  124025 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765762453.916498  124025 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765762453.916500  124025 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 01:34:13.920795: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention128\n",
      "Epochs: 50\n",
      "Batch size: 64\n",
      "Learning rate: 0.001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 45000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "\n",
      "Building attention128 model...\n",
      "2025-12-15 01:34:23.089631: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765762463.089772  124025 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1765762469.909775  124025 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "\u001b[1mModel: \"residual_attention_model128\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module                │ ?                      │     \u001b[32m1,061,888\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_1              │ ?                      │     \u001b[32m3,922,944\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_2              │ ?                      │    \u001b[32m12,251,136\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m6,037,504\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_133         │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m26,749,642\u001b[0m (102.04 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m26,671,050\u001b[0m (101.74 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m78,592\u001b[0m (307.00 KB)\n",
      "\n",
      "Total parameters: 26,749,642\n",
      "Epoch 1/50\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765762526.643470  124112 service.cc:152] XLA service 0xde64805b8c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765762526.643515  124112 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-15 01:35:28.483946: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765762571.834069  124112 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 1.9631 - top1_accuracy: 0.3114 - top_5_accuracy: 0.8209\n",
      "Epoch 1: val_top1_accuracy improved from -inf to 0.41660, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 168ms/step - loss: 1.9629 - top1_accuracy: 0.3115 - top_5_accuracy: 0.8209 - val_loss: 2.1524 - val_top1_accuracy: 0.4166 - val_top_5_accuracy: 0.8982 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.6258 - top1_accuracy: 0.4309 - top_5_accuracy: 0.9030\n",
      "Epoch 2: val_top1_accuracy improved from 0.41660 to 0.51640, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 1.6258 - top1_accuracy: 0.4309 - top_5_accuracy: 0.9030 - val_loss: 1.5725 - val_top1_accuracy: 0.5164 - val_top_5_accuracy: 0.9374 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.4456 - top1_accuracy: 0.5047 - top_5_accuracy: 0.9343\n",
      "Epoch 3: val_top1_accuracy did not improve from 0.51640\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 1.4456 - top1_accuracy: 0.5047 - top_5_accuracy: 0.9343 - val_loss: 178.3428 - val_top1_accuracy: 0.1920 - val_top_5_accuracy: 0.6464 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.4256 - top1_accuracy: 0.5131 - top_5_accuracy: 0.9343\n",
      "Epoch 4: val_top1_accuracy did not improve from 0.51640\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 86ms/step - loss: 1.4256 - top1_accuracy: 0.5130 - top_5_accuracy: 0.9343 - val_loss: 15.2127 - val_top1_accuracy: 0.3000 - val_top_5_accuracy: 0.8188 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.5803 - top1_accuracy: 0.4761 - top_5_accuracy: 0.9232\n",
      "Epoch 5: val_top1_accuracy improved from 0.51640 to 0.61880, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 1.5800 - top1_accuracy: 0.4762 - top_5_accuracy: 0.9232 - val_loss: 1.0930 - val_top1_accuracy: 0.6188 - val_top_5_accuracy: 0.9646 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.2539 - top1_accuracy: 0.5789 - top_5_accuracy: 0.9528\n",
      "Epoch 6: val_top1_accuracy did not improve from 0.61880\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 84ms/step - loss: 1.2540 - top1_accuracy: 0.5788 - top_5_accuracy: 0.9528 - val_loss: 2.0398 - val_top1_accuracy: 0.4642 - val_top_5_accuracy: 0.8906 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.2529 - top1_accuracy: 0.5671 - top_5_accuracy: 0.9453\n",
      "Epoch 7: val_top1_accuracy improved from 0.61880 to 0.64240, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 1.2528 - top1_accuracy: 0.5672 - top_5_accuracy: 0.9453 - val_loss: 1.1692 - val_top1_accuracy: 0.6424 - val_top_5_accuracy: 0.9756 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.0678 - top1_accuracy: 0.6319 - top_5_accuracy: 0.9655\n",
      "Epoch 8: val_top1_accuracy did not improve from 0.64240\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 83ms/step - loss: 1.0678 - top1_accuracy: 0.6319 - top_5_accuracy: 0.9655 - val_loss: 2.6800 - val_top1_accuracy: 0.6320 - val_top_5_accuracy: 0.9610 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9783 - top1_accuracy: 0.6629 - top_5_accuracy: 0.9701\n",
      "Epoch 9: val_top1_accuracy improved from 0.64240 to 0.66920, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.9783 - top1_accuracy: 0.6629 - top_5_accuracy: 0.9701 - val_loss: 1.3773 - val_top1_accuracy: 0.6692 - val_top_5_accuracy: 0.9768 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9148 - top1_accuracy: 0.6825 - top_5_accuracy: 0.9750\n",
      "Epoch 10: val_top1_accuracy improved from 0.66920 to 0.69400, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.9148 - top1_accuracy: 0.6825 - top_5_accuracy: 0.9750 - val_loss: 0.8729 - val_top1_accuracy: 0.6940 - val_top_5_accuracy: 0.9836 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.1585 - top1_accuracy: 0.6066 - top_5_accuracy: 0.9594\n",
      "Epoch 11: val_top1_accuracy improved from 0.69400 to 0.71620, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 1.1583 - top1_accuracy: 0.6066 - top_5_accuracy: 0.9594 - val_loss: 0.8112 - val_top1_accuracy: 0.7162 - val_top_5_accuracy: 0.9842 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8556 - top1_accuracy: 0.7044 - top_5_accuracy: 0.9784\n",
      "Epoch 12: val_top1_accuracy improved from 0.71620 to 0.71680, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 86ms/step - loss: 0.8556 - top1_accuracy: 0.7045 - top_5_accuracy: 0.9784 - val_loss: 0.8727 - val_top1_accuracy: 0.7168 - val_top_5_accuracy: 0.9790 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8260 - top1_accuracy: 0.7131 - top_5_accuracy: 0.9789\n",
      "Epoch 13: val_top1_accuracy improved from 0.71680 to 0.75820, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 86ms/step - loss: 0.8260 - top1_accuracy: 0.7131 - top_5_accuracy: 0.9789 - val_loss: 0.7089 - val_top1_accuracy: 0.7582 - val_top_5_accuracy: 0.9866 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7352 - top1_accuracy: 0.7436 - top_5_accuracy: 0.9848\n",
      "Epoch 14: val_top1_accuracy improved from 0.75820 to 0.77100, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 86ms/step - loss: 0.7352 - top1_accuracy: 0.7436 - top_5_accuracy: 0.9848 - val_loss: 0.6531 - val_top1_accuracy: 0.7710 - val_top_5_accuracy: 0.9858 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6910 - top1_accuracy: 0.7559 - top_5_accuracy: 0.9862\n",
      "Epoch 15: val_top1_accuracy did not improve from 0.77100\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 84ms/step - loss: 0.6910 - top1_accuracy: 0.7559 - top_5_accuracy: 0.9862 - val_loss: 0.6698 - val_top1_accuracy: 0.7638 - val_top_5_accuracy: 0.9864 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9376 - top1_accuracy: 0.6816 - top_5_accuracy: 0.9689\n",
      "Epoch 16: val_top1_accuracy did not improve from 0.77100\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 84ms/step - loss: 0.9377 - top1_accuracy: 0.6816 - top_5_accuracy: 0.9689 - val_loss: 1.0689 - val_top1_accuracy: 0.6938 - val_top_5_accuracy: 0.9798 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7831 - top1_accuracy: 0.7276 - top_5_accuracy: 0.9817\n",
      "Epoch 17: val_top1_accuracy did not improve from 0.77100\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 84ms/step - loss: 0.7830 - top1_accuracy: 0.7277 - top_5_accuracy: 0.9817 - val_loss: 0.7062 - val_top1_accuracy: 0.7582 - val_top_5_accuracy: 0.9874 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.6756 - top1_accuracy: 0.7645 - top_5_accuracy: 0.9856\n",
      "Epoch 18: val_top1_accuracy improved from 0.77100 to 0.78240, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.6756 - top1_accuracy: 0.7645 - top_5_accuracy: 0.9856 - val_loss: 0.6166 - val_top1_accuracy: 0.7824 - val_top_5_accuracy: 0.9904 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6179 - top1_accuracy: 0.7861 - top_5_accuracy: 0.9885\n",
      "Epoch 19: val_top1_accuracy improved from 0.78240 to 0.79280, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.6179 - top1_accuracy: 0.7861 - top_5_accuracy: 0.9885 - val_loss: 0.5927 - val_top1_accuracy: 0.7928 - val_top_5_accuracy: 0.9900 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.5881 - top1_accuracy: 0.7966 - top_5_accuracy: 0.9892\n",
      "Epoch 20: val_top1_accuracy improved from 0.79280 to 0.79360, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.5881 - top1_accuracy: 0.7966 - top_5_accuracy: 0.9892 - val_loss: 0.6011 - val_top1_accuracy: 0.7936 - val_top_5_accuracy: 0.9886 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6020 - top1_accuracy: 0.7925 - top_5_accuracy: 0.9892\n",
      "Epoch 21: val_top1_accuracy improved from 0.79360 to 0.81200, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.6019 - top1_accuracy: 0.7925 - top_5_accuracy: 0.9892 - val_loss: 0.5454 - val_top1_accuracy: 0.8120 - val_top_5_accuracy: 0.9896 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.5587 - top1_accuracy: 0.8052 - top_5_accuracy: 0.9899\n",
      "Epoch 22: val_top1_accuracy did not improve from 0.81200\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.5587 - top1_accuracy: 0.8052 - top_5_accuracy: 0.9899 - val_loss: 0.6041 - val_top1_accuracy: 0.7928 - val_top_5_accuracy: 0.9900 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.5252 - top1_accuracy: 0.8166 - top_5_accuracy: 0.9910\n",
      "Epoch 23: val_top1_accuracy did not improve from 0.81200\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.5252 - top1_accuracy: 0.8166 - top_5_accuracy: 0.9910 - val_loss: 0.5751 - val_top1_accuracy: 0.8022 - val_top_5_accuracy: 0.9900 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.5027 - top1_accuracy: 0.8269 - top_5_accuracy: 0.9928\n",
      "Epoch 24: val_top1_accuracy did not improve from 0.81200\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.5027 - top1_accuracy: 0.8269 - top_5_accuracy: 0.9928 - val_loss: 0.5707 - val_top1_accuracy: 0.8050 - val_top_5_accuracy: 0.9896 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.5636 - top1_accuracy: 0.8077 - top_5_accuracy: 0.9903\n",
      "Epoch 25: val_top1_accuracy improved from 0.81200 to 0.81280, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.5635 - top1_accuracy: 0.8077 - top_5_accuracy: 0.9903 - val_loss: 0.5437 - val_top1_accuracy: 0.8128 - val_top_5_accuracy: 0.9902 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.4947 - top1_accuracy: 0.8256 - top_5_accuracy: 0.9920\n",
      "Epoch 26: val_top1_accuracy improved from 0.81280 to 0.82000, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.4947 - top1_accuracy: 0.8256 - top_5_accuracy: 0.9920 - val_loss: 0.5368 - val_top1_accuracy: 0.8200 - val_top_5_accuracy: 0.9910 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.4642 - top1_accuracy: 0.8402 - top_5_accuracy: 0.9926\n",
      "Epoch 27: val_top1_accuracy improved from 0.82000 to 0.82520, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.4642 - top1_accuracy: 0.8402 - top_5_accuracy: 0.9926 - val_loss: 0.5162 - val_top1_accuracy: 0.8252 - val_top_5_accuracy: 0.9928 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.4218 - top1_accuracy: 0.8507 - top_5_accuracy: 0.9954\n",
      "Epoch 28: val_top1_accuracy did not improve from 0.82520\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 84ms/step - loss: 0.4218 - top1_accuracy: 0.8507 - top_5_accuracy: 0.9954 - val_loss: 0.5749 - val_top1_accuracy: 0.8114 - val_top_5_accuracy: 0.9904 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.5035 - top1_accuracy: 0.8259 - top_5_accuracy: 0.9928\n",
      "Epoch 29: val_top1_accuracy did not improve from 0.82520\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 84ms/step - loss: 0.5034 - top1_accuracy: 0.8259 - top_5_accuracy: 0.9928 - val_loss: 0.7313 - val_top1_accuracy: 0.8188 - val_top_5_accuracy: 0.9922 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.4112 - top1_accuracy: 0.8563 - top_5_accuracy: 0.9947\n",
      "Epoch 30: val_top1_accuracy improved from 0.82520 to 0.83240, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 0.4112 - top1_accuracy: 0.8563 - top_5_accuracy: 0.9947 - val_loss: 0.4955 - val_top1_accuracy: 0.8324 - val_top_5_accuracy: 0.9930 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3846 - top1_accuracy: 0.8677 - top_5_accuracy: 0.9956\n",
      "Epoch 31: val_top1_accuracy did not improve from 0.83240\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.3846 - top1_accuracy: 0.8677 - top_5_accuracy: 0.9956 - val_loss: 0.5125 - val_top1_accuracy: 0.8280 - val_top_5_accuracy: 0.9928 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.5802 - top1_accuracy: 0.8024 - top_5_accuracy: 0.9894\n",
      "Epoch 32: val_top1_accuracy did not improve from 0.83240\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 86ms/step - loss: 0.5800 - top1_accuracy: 0.8024 - top_5_accuracy: 0.9894 - val_loss: 0.4923 - val_top1_accuracy: 0.8324 - val_top_5_accuracy: 0.9940 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3640 - top1_accuracy: 0.8757 - top_5_accuracy: 0.9963\n",
      "Epoch 33: val_top1_accuracy did not improve from 0.83240\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.3640 - top1_accuracy: 0.8757 - top_5_accuracy: 0.9963 - val_loss: 0.5094 - val_top1_accuracy: 0.8294 - val_top_5_accuracy: 0.9922 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3488 - top1_accuracy: 0.8774 - top_5_accuracy: 0.9967\n",
      "Epoch 34: val_top1_accuracy did not improve from 0.83240\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.3488 - top1_accuracy: 0.8774 - top_5_accuracy: 0.9967 - val_loss: 0.5881 - val_top1_accuracy: 0.8222 - val_top_5_accuracy: 0.9922 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3397 - top1_accuracy: 0.8798 - top_5_accuracy: 0.9966\n",
      "Epoch 35: val_top1_accuracy did not improve from 0.83240\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.3397 - top1_accuracy: 0.8798 - top_5_accuracy: 0.9966 - val_loss: 0.5943 - val_top1_accuracy: 0.8128 - val_top_5_accuracy: 0.9916 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2925 - top1_accuracy: 0.8987 - top_5_accuracy: 0.9977\n",
      "Epoch 36: val_top1_accuracy improved from 0.83240 to 0.85440, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.2925 - top1_accuracy: 0.8987 - top_5_accuracy: 0.9977 - val_loss: 0.4549 - val_top1_accuracy: 0.8544 - val_top_5_accuracy: 0.9930 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2578 - top1_accuracy: 0.9093 - top_5_accuracy: 0.9980\n",
      "Epoch 37: val_top1_accuracy did not improve from 0.85440\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.2578 - top1_accuracy: 0.9093 - top_5_accuracy: 0.9980 - val_loss: 0.4573 - val_top1_accuracy: 0.8544 - val_top_5_accuracy: 0.9938 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2476 - top1_accuracy: 0.9157 - top_5_accuracy: 0.9980\n",
      "Epoch 38: val_top1_accuracy improved from 0.85440 to 0.85800, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.2476 - top1_accuracy: 0.9157 - top_5_accuracy: 0.9980 - val_loss: 0.4593 - val_top1_accuracy: 0.8580 - val_top_5_accuracy: 0.9934 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2409 - top1_accuracy: 0.9165 - top_5_accuracy: 0.9989\n",
      "Epoch 39: val_top1_accuracy did not improve from 0.85800\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.2409 - top1_accuracy: 0.9165 - top_5_accuracy: 0.9989 - val_loss: 0.4759 - val_top1_accuracy: 0.8536 - val_top_5_accuracy: 0.9922 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2186 - top1_accuracy: 0.9214 - top_5_accuracy: 0.9987\n",
      "Epoch 40: val_top1_accuracy did not improve from 0.85800\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.2186 - top1_accuracy: 0.9214 - top_5_accuracy: 0.9987 - val_loss: 0.4662 - val_top1_accuracy: 0.8546 - val_top_5_accuracy: 0.9926 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2169 - top1_accuracy: 0.9252 - top_5_accuracy: 0.9989\n",
      "Epoch 41: val_top1_accuracy improved from 0.85800 to 0.85980, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.2169 - top1_accuracy: 0.9252 - top_5_accuracy: 0.9989 - val_loss: 0.4700 - val_top1_accuracy: 0.8598 - val_top_5_accuracy: 0.9916 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2064 - top1_accuracy: 0.9244 - top_5_accuracy: 0.9992\n",
      "Epoch 42: val_top1_accuracy did not improve from 0.85980\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 86ms/step - loss: 0.2064 - top1_accuracy: 0.9244 - top_5_accuracy: 0.9992 - val_loss: 0.9025 - val_top1_accuracy: 0.8390 - val_top_5_accuracy: 0.9930 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2023 - top1_accuracy: 0.9279 - top_5_accuracy: 0.9987\n",
      "Epoch 43: val_top1_accuracy improved from 0.85980 to 0.86020, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 0.2023 - top1_accuracy: 0.9279 - top_5_accuracy: 0.9987 - val_loss: 0.4787 - val_top1_accuracy: 0.8602 - val_top_5_accuracy: 0.9946 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1938 - top1_accuracy: 0.9318 - top_5_accuracy: 0.9988\n",
      "Epoch 44: val_top1_accuracy did not improve from 0.86020\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 86ms/step - loss: 0.1938 - top1_accuracy: 0.9318 - top_5_accuracy: 0.9988 - val_loss: 0.4888 - val_top1_accuracy: 0.8552 - val_top_5_accuracy: 0.9942 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1819 - top1_accuracy: 0.9356 - top_5_accuracy: 0.9990\n",
      "Epoch 45: val_top1_accuracy did not improve from 0.86020\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.1820 - top1_accuracy: 0.9356 - top_5_accuracy: 0.9990 - val_loss: 0.4831 - val_top1_accuracy: 0.8586 - val_top_5_accuracy: 0.9940 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1747 - top1_accuracy: 0.9371 - top_5_accuracy: 0.9995\n",
      "Epoch 46: val_top1_accuracy did not improve from 0.86020\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.1747 - top1_accuracy: 0.9371 - top_5_accuracy: 0.9995 - val_loss: 0.4856 - val_top1_accuracy: 0.8584 - val_top_5_accuracy: 0.9932 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1802 - top1_accuracy: 0.9365 - top_5_accuracy: 0.9989\n",
      "Epoch 47: val_top1_accuracy improved from 0.86020 to 0.86080, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 89ms/step - loss: 0.1802 - top1_accuracy: 0.9365 - top_5_accuracy: 0.9989 - val_loss: 0.4885 - val_top1_accuracy: 0.8608 - val_top_5_accuracy: 0.9936 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1681 - top1_accuracy: 0.9418 - top_5_accuracy: 0.9992\n",
      "Epoch 48: val_top1_accuracy did not improve from 0.86080\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 86ms/step - loss: 0.1681 - top1_accuracy: 0.9418 - top_5_accuracy: 0.9992 - val_loss: 0.5272 - val_top1_accuracy: 0.8536 - val_top_5_accuracy: 0.9942 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1640 - top1_accuracy: 0.9416 - top_5_accuracy: 0.9996\n",
      "Epoch 49: val_top1_accuracy did not improve from 0.86080\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 86ms/step - loss: 0.1640 - top1_accuracy: 0.9416 - top_5_accuracy: 0.9996 - val_loss: 0.4857 - val_top1_accuracy: 0.8590 - val_top_5_accuracy: 0.9936 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1589 - top1_accuracy: 0.9433 - top_5_accuracy: 0.9995\n",
      "Epoch 50: val_top1_accuracy did not improve from 0.86080\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 85ms/step - loss: 0.1589 - top1_accuracy: 0.9433 - top_5_accuracy: 0.9995 - val_loss: 0.5202 - val_top1_accuracy: 0.8538 - val_top_5_accuracy: 0.9942 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/e4040-fall2025-project-swye/train_cifar_new 12.14.py\", line 299, in <module>\n",
      "    main()\n",
      "  File \"/content/e4040-fall2025-project-swye/train_cifar_new 12.14.py\", line 257, in main\n",
      "    test_top1_acc=results[names.index(\"top1_accuracy\")]\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: 'top1_accuracy' is not in list\n"
     ]
    }
   ],
   "source": [
    "!python \"train_cifar_new 12.14.py\" --model attention128 --epochs 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257685,
     "status": "ok",
     "timestamp": 1765762453637,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "cIyMagJGb2G9",
    "outputId": "bd67723a-2303-4c62-c31f-bca0f781a6b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 01:29:56.722615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765762196.742827  122673 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765762196.748994  122673 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765762196.764788  122673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765762196.764814  122673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765762196.764817  122673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765762196.764819  122673 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 01:29:56.769438: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention128\n",
      "Epochs: 3\n",
      "Batch size: 128\n",
      "Learning rate: 0.1\n",
      "Momentum: 0.9\n",
      "Weight decay: 0.0001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 50000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "2025-12-15 01:30:05.231195: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765762205.231337  122673 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "\n",
      "Building attention128 model...\n",
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'residual_attention_model128', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "\u001b[1mModel: \"residual_attention_model128\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module                │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_1              │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_2              │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_133         │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ ?                      │   \u001b[32m0\u001b[0m (unbuilt) │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "\n",
      "Total parameters: 0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 1/3\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765762251.567127  122756 service.cc:152] XLA service 0x124304002ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765762251.567163  122756 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-15 01:30:52.956082: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765762256.657684  122756 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1765762284.159766  122756 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.2133 - loss: 3.6311 - top_5_accuracy: 0.7073\n",
      "Epoch 1: val_accuracy improved from -inf to 0.35040, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 212ms/step - accuracy: 0.2135 - loss: 3.6288 - top_5_accuracy: 0.7075 - val_accuracy: 0.3504 - val_loss: 2.3673 - val_top_5_accuracy: 0.8655 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 2/3\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.3811 - loss: 1.7354 - top_5_accuracy: 0.8752\n",
      "Epoch 2: val_accuracy improved from 0.35040 to 0.43840, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 103ms/step - accuracy: 0.3811 - loss: 1.7352 - top_5_accuracy: 0.8752 - val_accuracy: 0.4384 - val_loss: 1.5847 - val_top_5_accuracy: 0.9085 - learning_rate: 0.1000\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.1.\n",
      "Epoch 3/3\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.4401 - loss: 1.5318 - top_5_accuracy: 0.9082\n",
      "Epoch 3: val_accuracy improved from 0.43840 to 0.50130, saving model to ./checkpoints/attention128_best_model.weights.h5\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 101ms/step - accuracy: 0.4402 - loss: 1.5317 - top_5_accuracy: 0.9082 - val_accuracy: 0.5013 - val_loss: 1.3810 - val_top_5_accuracy: 0.9330 - learning_rate: 0.1000\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "============================================================\n",
      "Final Test Top-1 Accuracy: 50.13%\n",
      "Final Test Top-1 Error: 49.87%\n",
      "Final Test Top-5 Accuracy: 93.30%\n",
      "Final Test Top-5 Error: 6.70%\n",
      "Final Test Loss: 1.3810\n",
      "============================================================\n",
      "\n",
      "Best Training Top-1 Accuracy: 45.01%\n",
      "Best Training Top-1 Error: 54.99%\n",
      "Best Validation Top-1 Accuracy: 50.13%\n",
      "Best Validation Top-1 Error: 49.87%\n",
      "Best Training Top-5 Accuracy: 91.36%\n",
      "Best Training Top-5 Error: 8.64%\n",
      "Best Validation Top-5 Accuracy: 93.30%\n",
      "Best Validation Top-5 Error: 6.70%\n",
      "Final Test Top-1 Accuracy: 50.13%\n",
      "Final Test Top-1 Error: 49.87%\n",
      "\n",
      "Plots saved to ./results/plots/\n",
      "Results appended to results.csv\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/attention128_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python \"train_cifar_new.py\" --model attention128 --epochs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1765766031448,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "cDB__G0RoOD9",
    "outputId": "7497d344-4a98-48bd-89a2-d95cb6572f72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is behind 'origin/main' by 1 commit, and can be fast-forwarded.\n",
      "  (use \"git pull\" to update your local branch)\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   train_cifar_new.py\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mresults.csv\u001b[m\n",
      "\t\u001b[31mresults/\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxV1ZMEypOkf"
   },
   "outputs": [],
   "source": [
    "!git restore train_cifar_new.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1765810506917,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "LR5k0aT93ToX",
    "outputId": "c06d9bdf-fd8b-480c-eb68-a875267726c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASnTe26sRwr_"
   },
   "source": [
    "### run attention164 arl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3424383,
     "status": "ok",
     "timestamp": 1765769678551,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "v9uH6maS3YlZ",
    "outputId": "2ad5b9e4-39c5-4c5c-a45a-5df27f0fd232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 02:37:34.565344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765766254.585006  141547 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765766254.591156  141547 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765766254.607487  141547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765766254.607515  141547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765766254.607518  141547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765766254.607520  141547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 02:37:34.612490: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention164\n",
      "Epochs: 50\n",
      "Batch size: 64\n",
      "Learning rate: 0.001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 45000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "\n",
      "Building attention164 model...\n",
      "2025-12-15 02:37:44.212030: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765766264.212164  141547 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1765766271.833191  141547 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "\u001b[1mModel: \"residual_attention_model164\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module                │ ?                      │     \u001b[32m1,133,056\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_1              │ ?                      │     \u001b[32m4,204,544\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_2              │ ?                      │    \u001b[32m13,371,392\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m6,037,504\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_142         │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m28,222,666\u001b[0m (107.66 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m28,138,698\u001b[0m (107.34 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m83,968\u001b[0m (328.00 KB)\n",
      "\n",
      "Total parameters: 28,222,666\n",
      "Epoch 1/50\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765766333.776852  141636 service.cc:152] XLA service 0x138ef0063320 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765766333.776887  141636 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-15 02:38:55.769154: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765766379.511590  141636 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 1.9907 - top1_accuracy: 0.3101\n",
      "Epoch 1: val_top1_accuracy improved from -inf to 0.42580, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 168ms/step - loss: 1.9904 - top1_accuracy: 0.3102 - val_loss: 1.6722 - val_top1_accuracy: 0.4258 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1.7507 - top1_accuracy: 0.4039\n",
      "Epoch 2: val_top1_accuracy did not improve from 0.42580\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 91ms/step - loss: 1.7507 - top1_accuracy: 0.4039 - val_loss: 207.2824 - val_top1_accuracy: 0.2912 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.6251 - top1_accuracy: 0.4383\n",
      "Epoch 3: val_top1_accuracy improved from 0.42580 to 0.46340, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 1.6250 - top1_accuracy: 0.4384 - val_loss: 4.9195 - val_top1_accuracy: 0.4634 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.4748 - top1_accuracy: 0.4977\n",
      "Epoch 4: val_top1_accuracy improved from 0.46340 to 0.52480, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 1.4748 - top1_accuracy: 0.4977 - val_loss: 1.3421 - val_top1_accuracy: 0.5248 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.4629 - top1_accuracy: 0.4884\n",
      "Epoch 5: val_top1_accuracy improved from 0.52480 to 0.53100, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 1.4627 - top1_accuracy: 0.4884 - val_loss: 1.8594 - val_top1_accuracy: 0.5310 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.2316 - top1_accuracy: 0.5860\n",
      "Epoch 6: val_top1_accuracy improved from 0.53100 to 0.60040, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 93ms/step - loss: 1.2316 - top1_accuracy: 0.5860 - val_loss: 2.0428 - val_top1_accuracy: 0.6004 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.1287 - top1_accuracy: 0.6150\n",
      "Epoch 7: val_top1_accuracy improved from 0.60040 to 0.65480, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 1.1287 - top1_accuracy: 0.6150 - val_loss: 1.0469 - val_top1_accuracy: 0.6548 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.1171 - top1_accuracy: 0.6271\n",
      "Epoch 8: val_top1_accuracy did not improve from 0.65480\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.1170 - top1_accuracy: 0.6271 - val_loss: 18.5086 - val_top1_accuracy: 0.5174 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 1.0678 - top1_accuracy: 0.6368\n",
      "Epoch 9: val_top1_accuracy improved from 0.65480 to 0.67860, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 1.0677 - top1_accuracy: 0.6368 - val_loss: 0.9169 - val_top1_accuracy: 0.6786 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.9533 - top1_accuracy: 0.6739\n",
      "Epoch 10: val_top1_accuracy improved from 0.67860 to 0.70600, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 93ms/step - loss: 0.9533 - top1_accuracy: 0.6739 - val_loss: 0.8464 - val_top1_accuracy: 0.7060 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.9958 - top1_accuracy: 0.6627\n",
      "Epoch 11: val_top1_accuracy did not improve from 0.70600\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.9958 - top1_accuracy: 0.6627 - val_loss: 2.6675 - val_top1_accuracy: 0.6372 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.8985 - top1_accuracy: 0.6948\n",
      "Epoch 12: val_top1_accuracy did not improve from 0.70600\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.8985 - top1_accuracy: 0.6948 - val_loss: 1.6523 - val_top1_accuracy: 0.6160 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.8748 - top1_accuracy: 0.7028\n",
      "Epoch 13: val_top1_accuracy improved from 0.70600 to 0.75580, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 0.8747 - top1_accuracy: 0.7028 - val_loss: 0.6995 - val_top1_accuracy: 0.7558 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7341 - top1_accuracy: 0.7473\n",
      "Epoch 14: val_top1_accuracy improved from 0.75580 to 0.76020, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 0.7341 - top1_accuracy: 0.7473 - val_loss: 0.7033 - val_top1_accuracy: 0.7602 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7871 - top1_accuracy: 0.7320\n",
      "Epoch 15: val_top1_accuracy did not improve from 0.76020\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.7871 - top1_accuracy: 0.7320 - val_loss: 0.7867 - val_top1_accuracy: 0.7518 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.6888 - top1_accuracy: 0.7608\n",
      "Epoch 16: val_top1_accuracy did not improve from 0.76020\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.6888 - top1_accuracy: 0.7608 - val_loss: 0.6979 - val_top1_accuracy: 0.7570 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.6508 - top1_accuracy: 0.7758\n",
      "Epoch 17: val_top1_accuracy improved from 0.76020 to 0.79680, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 0.6508 - top1_accuracy: 0.7758 - val_loss: 0.6011 - val_top1_accuracy: 0.7968 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.6076 - top1_accuracy: 0.7876\n",
      "Epoch 18: val_top1_accuracy did not improve from 0.79680\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.6077 - top1_accuracy: 0.7876 - val_loss: 0.6444 - val_top1_accuracy: 0.7768 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7162 - top1_accuracy: 0.7549\n",
      "Epoch 19: val_top1_accuracy did not improve from 0.79680\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.7161 - top1_accuracy: 0.7549 - val_loss: 0.5918 - val_top1_accuracy: 0.7938 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.5504 - top1_accuracy: 0.8095\n",
      "Epoch 20: val_top1_accuracy did not improve from 0.79680\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.5504 - top1_accuracy: 0.8095 - val_loss: 0.6172 - val_top1_accuracy: 0.7926 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.5427 - top1_accuracy: 0.8123\n",
      "Epoch 21: val_top1_accuracy improved from 0.79680 to 0.81100, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 0.5427 - top1_accuracy: 0.8123 - val_loss: 0.5422 - val_top1_accuracy: 0.8110 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.5189 - top1_accuracy: 0.8184\n",
      "Epoch 22: val_top1_accuracy improved from 0.81100 to 0.81200, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 0.5189 - top1_accuracy: 0.8184 - val_loss: 0.5371 - val_top1_accuracy: 0.8120 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.5015 - top1_accuracy: 0.8285\n",
      "Epoch 23: val_top1_accuracy did not improve from 0.81200\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.5016 - top1_accuracy: 0.8285 - val_loss: 0.7080 - val_top1_accuracy: 0.7586 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.5148 - top1_accuracy: 0.8220\n",
      "Epoch 24: val_top1_accuracy improved from 0.81200 to 0.81980, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 0.5148 - top1_accuracy: 0.8220 - val_loss: 0.5178 - val_top1_accuracy: 0.8198 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.4596 - top1_accuracy: 0.8388\n",
      "Epoch 25: val_top1_accuracy improved from 0.81980 to 0.82380, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 93ms/step - loss: 0.4596 - top1_accuracy: 0.8388 - val_loss: 0.5151 - val_top1_accuracy: 0.8238 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.4568 - top1_accuracy: 0.8424\n",
      "Epoch 26: val_top1_accuracy did not improve from 0.82380\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.4569 - top1_accuracy: 0.8423 - val_loss: 0.5450 - val_top1_accuracy: 0.8162 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.4995 - top1_accuracy: 0.8267\n",
      "Epoch 27: val_top1_accuracy improved from 0.82380 to 0.82680, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 0.4994 - top1_accuracy: 0.8267 - val_loss: 0.5034 - val_top1_accuracy: 0.8268 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.4093 - top1_accuracy: 0.8569\n",
      "Epoch 28: val_top1_accuracy did not improve from 0.82680\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.4093 - top1_accuracy: 0.8569 - val_loss: 0.6534 - val_top1_accuracy: 0.7926 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.4141 - top1_accuracy: 0.8553\n",
      "Epoch 29: val_top1_accuracy did not improve from 0.82680\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.4141 - top1_accuracy: 0.8553 - val_loss: 0.5918 - val_top1_accuracy: 0.8068 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.6894 - top1_accuracy: 0.7699\n",
      "Epoch 30: val_top1_accuracy improved from 0.82680 to 0.82940, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 0.6892 - top1_accuracy: 0.7700 - val_loss: 0.5168 - val_top1_accuracy: 0.8294 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.4596 - top1_accuracy: 0.8411\n",
      "Epoch 31: val_top1_accuracy improved from 0.82940 to 0.84900, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 93ms/step - loss: 0.4596 - top1_accuracy: 0.8412 - val_loss: 0.4748 - val_top1_accuracy: 0.8490 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.3488 - top1_accuracy: 0.8790\n",
      "Epoch 32: val_top1_accuracy did not improve from 0.84900\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.3488 - top1_accuracy: 0.8789 - val_loss: 0.5126 - val_top1_accuracy: 0.8366 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.3491 - top1_accuracy: 0.8790\n",
      "Epoch 33: val_top1_accuracy did not improve from 0.84900\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.3491 - top1_accuracy: 0.8790 - val_loss: 0.5147 - val_top1_accuracy: 0.8328 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.3484 - top1_accuracy: 0.8767\n",
      "Epoch 34: val_top1_accuracy did not improve from 0.84900\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.3484 - top1_accuracy: 0.8767 - val_loss: 0.4817 - val_top1_accuracy: 0.8462 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.3441 - top1_accuracy: 0.8803\n",
      "Epoch 35: val_top1_accuracy did not improve from 0.84900\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.3441 - top1_accuracy: 0.8803 - val_loss: 0.5152 - val_top1_accuracy: 0.8330 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.3325 - top1_accuracy: 0.8847\n",
      "Epoch 36: val_top1_accuracy did not improve from 0.84900\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.3325 - top1_accuracy: 0.8847 - val_loss: 0.5150 - val_top1_accuracy: 0.8356 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.2723 - top1_accuracy: 0.9055\n",
      "Epoch 37: val_top1_accuracy improved from 0.84900 to 0.86140, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 93ms/step - loss: 0.2723 - top1_accuracy: 0.9055 - val_loss: 0.4343 - val_top1_accuracy: 0.8614 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.2407 - top1_accuracy: 0.9133\n",
      "Epoch 38: val_top1_accuracy did not improve from 0.86140\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.2407 - top1_accuracy: 0.9133 - val_loss: 0.4578 - val_top1_accuracy: 0.8572 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.2299 - top1_accuracy: 0.9191\n",
      "Epoch 39: val_top1_accuracy did not improve from 0.86140\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.2299 - top1_accuracy: 0.9191 - val_loss: 0.4540 - val_top1_accuracy: 0.8606 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.2151 - top1_accuracy: 0.9245\n",
      "Epoch 40: val_top1_accuracy improved from 0.86140 to 0.86420, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 0.2151 - top1_accuracy: 0.9245 - val_loss: 0.4489 - val_top1_accuracy: 0.8642 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.2131 - top1_accuracy: 0.9249\n",
      "Epoch 41: val_top1_accuracy did not improve from 0.86420\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.2131 - top1_accuracy: 0.9249 - val_loss: 0.4616 - val_top1_accuracy: 0.8606 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.2123 - top1_accuracy: 0.9238\n",
      "Epoch 42: val_top1_accuracy did not improve from 0.86420\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.2123 - top1_accuracy: 0.9238 - val_loss: 0.4821 - val_top1_accuracy: 0.8538 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.2023 - top1_accuracy: 0.9275\n",
      "Epoch 43: val_top1_accuracy did not improve from 0.86420\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.2023 - top1_accuracy: 0.9275 - val_loss: 0.4698 - val_top1_accuracy: 0.8640 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1911 - top1_accuracy: 0.9322\n",
      "Epoch 44: val_top1_accuracy did not improve from 0.86420\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.1911 - top1_accuracy: 0.9322 - val_loss: 0.4885 - val_top1_accuracy: 0.8608 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1886 - top1_accuracy: 0.9329\n",
      "Epoch 45: val_top1_accuracy improved from 0.86420 to 0.86460, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 0.1886 - top1_accuracy: 0.9329 - val_loss: 0.4779 - val_top1_accuracy: 0.8646 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1740 - top1_accuracy: 0.9394\n",
      "Epoch 46: val_top1_accuracy did not improve from 0.86460\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.1740 - top1_accuracy: 0.9394 - val_loss: 0.4769 - val_top1_accuracy: 0.8632 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1722 - top1_accuracy: 0.9394\n",
      "Epoch 47: val_top1_accuracy did not improve from 0.86460\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.1722 - top1_accuracy: 0.9394 - val_loss: 0.5023 - val_top1_accuracy: 0.8622 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1755 - top1_accuracy: 0.9367\n",
      "Epoch 48: val_top1_accuracy did not improve from 0.86460\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.1755 - top1_accuracy: 0.9367 - val_loss: 0.4945 - val_top1_accuracy: 0.8596 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1704 - top1_accuracy: 0.9401\n",
      "Epoch 49: val_top1_accuracy did not improve from 0.86460\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.1704 - top1_accuracy: 0.9401 - val_loss: 0.4900 - val_top1_accuracy: 0.8586 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1749 - top1_accuracy: 0.9375\n",
      "Epoch 50: val_top1_accuracy did not improve from 0.86460\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.1748 - top1_accuracy: 0.9375 - val_loss: 0.4977 - val_top1_accuracy: 0.8640 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "metrics_names: ['loss', 'compile_metrics']\n",
      "results: [0.5126797556877136, 0.8514999747276306]\n",
      "Final Test Accuracy: 85.15%\n",
      "Test Top-1 Error: 14.850002527236938\n",
      "Test Top-5 Error: 0.62000155\n",
      "============================================================\n",
      "Final Test Loss: 0.5127\n",
      "============================================================\n",
      "Training Top-1 Error: 5.904442071914673\n",
      "Validation Top-1 Error: 13.539999723434448\n",
      "\n",
      "Best Training Accuracy: 94.10%\n",
      "Best Validation Accuracy: 86.46%\n",
      "Final Test Accuracy: 85.15%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/attention164_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python \"train_cifar_new 12.14.py\" --model attention164 --epochs 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1765810476720,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "aFr321dRdjBP",
    "outputId": "3c6accfd-28a1-4f6a-8590-fc80f7d39267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any of the parent directories): .git\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc9R9VBGhQvj"
   },
   "outputs": [],
   "source": [
    "!ls ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1765810711293,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "PjpHKgXahR7U",
    "outputId": "908c94f7-7573-4e94-e0e8-090a4dc581eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention128_tf.py  attention56_tf.py  __init__.py    resnet128_tf.py\n",
      "attention164_tf.py  attention92.py     __init__tf.py  resnet164_tf.py\n",
      "attention56.py      attention92_tf.py  layers_tf.py   resnet92_tf.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1765810714755,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "ZIP9LzEjhSnz",
    "outputId": "47d1aed6-174a-493b-ad95-e9317e82048b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/e4040-fall2025-project-swye\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5uG9noCR3ap"
   },
   "source": [
    "### run attention164 nal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3335014,
     "status": "ok",
     "timestamp": 1765814130078,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "kXJmSgPohTfu",
    "outputId": "eff1fdf7-4787-4d81-e639-91d42829641f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 14:59:55.412341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765810795.430788    5392 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765810795.436447    5392 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765810795.450632    5392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765810795.450660    5392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765810795.450662    5392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765810795.450664    5392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 14:59:55.455075: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention164\n",
      "Epochs: 50\n",
      "Batch size: 64\n",
      "Learning rate: 0.001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
      "Training samples: 45000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "\n",
      "Building attention164 model...\n",
      "2025-12-15 15:00:12.550422: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765810812.552409    5392 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1765810820.840976    5392 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "\u001b[1mModel: \"residual_attention_model164\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module                │ ?                      │     \u001b[32m1,133,056\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_1              │ ?                      │     \u001b[32m4,204,544\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_2              │ ?                      │    \u001b[32m13,371,392\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m6,037,504\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_142         │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m28,222,666\u001b[0m (107.66 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m28,138,698\u001b[0m (107.34 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m83,968\u001b[0m (328.00 KB)\n",
      "\n",
      "Total parameters: 28,222,666\n",
      "Epoch 1/50\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765810882.666144    5504 service.cc:152] XLA service 0x7e35dc003510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765810882.666191    5504 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-15 15:01:24.609425: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765810927.866557    5504 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 2.0925 - top1_accuracy: 0.2634\n",
      "Epoch 1: val_top1_accuracy improved from -inf to 0.36660, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 164ms/step - loss: 2.0922 - top1_accuracy: 0.2634 - val_loss: 1.8302 - val_top1_accuracy: 0.3666 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.8947 - top1_accuracy: 0.3458\n",
      "Epoch 2: val_top1_accuracy did not improve from 0.36660\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 1.8946 - top1_accuracy: 0.3458 - val_loss: 1.9639 - val_top1_accuracy: 0.3088 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.7825 - top1_accuracy: 0.3677\n",
      "Epoch 3: val_top1_accuracy did not improve from 0.36660\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 1.7825 - top1_accuracy: 0.3677 - val_loss: 2195.2039 - val_top1_accuracy: 0.1366 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.7808 - top1_accuracy: 0.3781\n",
      "Epoch 4: val_top1_accuracy did not improve from 0.36660\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 1.7807 - top1_accuracy: 0.3781 - val_loss: 2.2159 - val_top1_accuracy: 0.2638 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.7151 - top1_accuracy: 0.3995\n",
      "Epoch 5: val_top1_accuracy improved from 0.36660 to 0.47160, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 1.7151 - top1_accuracy: 0.3995 - val_loss: 2.4435 - val_top1_accuracy: 0.4716 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.6668 - top1_accuracy: 0.4193\n",
      "Epoch 6: val_top1_accuracy did not improve from 0.47160\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 1.6668 - top1_accuracy: 0.4193 - val_loss: 1.5882 - val_top1_accuracy: 0.4132 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.7389 - top1_accuracy: 0.3875\n",
      "Epoch 7: val_top1_accuracy did not improve from 0.47160\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 1.7390 - top1_accuracy: 0.3875 - val_loss: 1.9306 - val_top1_accuracy: 0.2864 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.7578 - top1_accuracy: 0.3633\n",
      "Epoch 8: val_top1_accuracy improved from 0.47160 to 0.49920, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.7576 - top1_accuracy: 0.3633 - val_loss: 1.3428 - val_top1_accuracy: 0.4992 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.5610 - top1_accuracy: 0.4476\n",
      "Epoch 9: val_top1_accuracy improved from 0.49920 to 0.50820, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.5610 - top1_accuracy: 0.4476 - val_loss: 1.3967 - val_top1_accuracy: 0.5082 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.4385 - top1_accuracy: 0.4843\n",
      "Epoch 10: val_top1_accuracy improved from 0.50820 to 0.53260, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.4384 - top1_accuracy: 0.4843 - val_loss: 1.7783 - val_top1_accuracy: 0.5326 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.3696 - top1_accuracy: 0.5131\n",
      "Epoch 11: val_top1_accuracy improved from 0.53260 to 0.56500, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.3696 - top1_accuracy: 0.5131 - val_loss: 1.1614 - val_top1_accuracy: 0.5650 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.3786 - top1_accuracy: 0.5030\n",
      "Epoch 12: val_top1_accuracy did not improve from 0.56500\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 1.3785 - top1_accuracy: 0.5030 - val_loss: 2.0935 - val_top1_accuracy: 0.5254 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.2765 - top1_accuracy: 0.5430\n",
      "Epoch 13: val_top1_accuracy did not improve from 0.56500\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 1.2764 - top1_accuracy: 0.5430 - val_loss: 2.5924 - val_top1_accuracy: 0.4798 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.1628 - top1_accuracy: 0.5852\n",
      "Epoch 14: val_top1_accuracy did not improve from 0.56500\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 1.1627 - top1_accuracy: 0.5853 - val_loss: 1.3711 - val_top1_accuracy: 0.5596 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.1960 - top1_accuracy: 0.5774\n",
      "Epoch 15: val_top1_accuracy improved from 0.56500 to 0.63820, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.1960 - top1_accuracy: 0.5774 - val_loss: 1.0089 - val_top1_accuracy: 0.6382 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.0538 - top1_accuracy: 0.6276\n",
      "Epoch 16: val_top1_accuracy improved from 0.63820 to 0.66060, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.0538 - top1_accuracy: 0.6276 - val_loss: 0.9591 - val_top1_accuracy: 0.6606 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9990 - top1_accuracy: 0.6446\n",
      "Epoch 17: val_top1_accuracy improved from 0.66060 to 0.69980, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 90ms/step - loss: 0.9990 - top1_accuracy: 0.6447 - val_loss: 0.8413 - val_top1_accuracy: 0.6998 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9151 - top1_accuracy: 0.6762\n",
      "Epoch 18: val_top1_accuracy did not improve from 0.69980\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.9151 - top1_accuracy: 0.6762 - val_loss: 0.9247 - val_top1_accuracy: 0.6870 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8509 - top1_accuracy: 0.7012\n",
      "Epoch 19: val_top1_accuracy did not improve from 0.69980\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.8508 - top1_accuracy: 0.7012 - val_loss: 1.0212 - val_top1_accuracy: 0.6988 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7997 - top1_accuracy: 0.7195\n",
      "Epoch 20: val_top1_accuracy improved from 0.69980 to 0.73440, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.7997 - top1_accuracy: 0.7195 - val_loss: 0.7653 - val_top1_accuracy: 0.7344 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7778 - top1_accuracy: 0.7307\n",
      "Epoch 21: val_top1_accuracy improved from 0.73440 to 0.75960, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.7778 - top1_accuracy: 0.7307 - val_loss: 0.7142 - val_top1_accuracy: 0.7596 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7827 - top1_accuracy: 0.7304\n",
      "Epoch 22: val_top1_accuracy did not improve from 0.75960\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.7827 - top1_accuracy: 0.7304 - val_loss: 0.7041 - val_top1_accuracy: 0.7530 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7079 - top1_accuracy: 0.7551\n",
      "Epoch 23: val_top1_accuracy did not improve from 0.75960\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.7079 - top1_accuracy: 0.7551 - val_loss: 0.9659 - val_top1_accuracy: 0.7378 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6567 - top1_accuracy: 0.7670\n",
      "Epoch 24: val_top1_accuracy did not improve from 0.75960\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.6566 - top1_accuracy: 0.7670 - val_loss: 0.7631 - val_top1_accuracy: 0.7482 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6542 - top1_accuracy: 0.7741\n",
      "Epoch 25: val_top1_accuracy did not improve from 0.75960\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.6542 - top1_accuracy: 0.7741 - val_loss: 1.6915 - val_top1_accuracy: 0.7578 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6270 - top1_accuracy: 0.7817\n",
      "Epoch 26: val_top1_accuracy improved from 0.75960 to 0.79100, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.6270 - top1_accuracy: 0.7817 - val_loss: 0.6178 - val_top1_accuracy: 0.7910 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5768 - top1_accuracy: 0.7980\n",
      "Epoch 27: val_top1_accuracy improved from 0.79100 to 0.79960, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.5768 - top1_accuracy: 0.7980 - val_loss: 0.5714 - val_top1_accuracy: 0.7996 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5565 - top1_accuracy: 0.8086\n",
      "Epoch 28: val_top1_accuracy did not improve from 0.79960\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.5565 - top1_accuracy: 0.8086 - val_loss: 0.5905 - val_top1_accuracy: 0.7980 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5578 - top1_accuracy: 0.8053\n",
      "Epoch 29: val_top1_accuracy improved from 0.79960 to 0.81360, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.5577 - top1_accuracy: 0.8053 - val_loss: 0.5413 - val_top1_accuracy: 0.8136 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5091 - top1_accuracy: 0.8237\n",
      "Epoch 30: val_top1_accuracy did not improve from 0.81360\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.5091 - top1_accuracy: 0.8237 - val_loss: 0.8286 - val_top1_accuracy: 0.7890 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5059 - top1_accuracy: 0.8245\n",
      "Epoch 31: val_top1_accuracy did not improve from 0.81360\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.5059 - top1_accuracy: 0.8245 - val_loss: 0.7652 - val_top1_accuracy: 0.8044 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.6259 - top1_accuracy: 0.7890\n",
      "Epoch 32: val_top1_accuracy improved from 0.81360 to 0.82760, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.6258 - top1_accuracy: 0.7891 - val_loss: 0.5071 - val_top1_accuracy: 0.8276 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4633 - top1_accuracy: 0.8399\n",
      "Epoch 33: val_top1_accuracy did not improve from 0.82760\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.4634 - top1_accuracy: 0.8399 - val_loss: 0.6528 - val_top1_accuracy: 0.7936 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5168 - top1_accuracy: 0.8213\n",
      "Epoch 34: val_top1_accuracy improved from 0.82760 to 0.82820, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.5168 - top1_accuracy: 0.8213 - val_loss: 0.4901 - val_top1_accuracy: 0.8282 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4354 - top1_accuracy: 0.8485\n",
      "Epoch 35: val_top1_accuracy did not improve from 0.82820\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.4354 - top1_accuracy: 0.8485 - val_loss: 0.5265 - val_top1_accuracy: 0.8186 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4889 - top1_accuracy: 0.8275\n",
      "Epoch 36: val_top1_accuracy improved from 0.82820 to 0.82900, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.4889 - top1_accuracy: 0.8275 - val_loss: 0.7021 - val_top1_accuracy: 0.8290 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4072 - top1_accuracy: 0.8554\n",
      "Epoch 37: val_top1_accuracy did not improve from 0.82900\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.4072 - top1_accuracy: 0.8554 - val_loss: 0.8655 - val_top1_accuracy: 0.8118 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4111 - top1_accuracy: 0.8564\n",
      "Epoch 38: val_top1_accuracy did not improve from 0.82900\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.4112 - top1_accuracy: 0.8564 - val_loss: 0.5788 - val_top1_accuracy: 0.8208 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5396 - top1_accuracy: 0.8148\n",
      "Epoch 39: val_top1_accuracy did not improve from 0.82900\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.5394 - top1_accuracy: 0.8149 - val_loss: 0.4947 - val_top1_accuracy: 0.8288 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3777 - top1_accuracy: 0.8660\n",
      "Epoch 40: val_top1_accuracy improved from 0.82900 to 0.83160, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.3777 - top1_accuracy: 0.8660 - val_loss: 0.5181 - val_top1_accuracy: 0.8316 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3963 - top1_accuracy: 0.8616\n",
      "Epoch 41: val_top1_accuracy improved from 0.83160 to 0.83320, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.3963 - top1_accuracy: 0.8616 - val_loss: 0.5045 - val_top1_accuracy: 0.8332 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3623 - top1_accuracy: 0.8752\n",
      "Epoch 42: val_top1_accuracy improved from 0.83320 to 0.84340, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.3623 - top1_accuracy: 0.8752 - val_loss: 0.4654 - val_top1_accuracy: 0.8434 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3544 - top1_accuracy: 0.8785\n",
      "Epoch 43: val_top1_accuracy did not improve from 0.84340\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.3544 - top1_accuracy: 0.8785 - val_loss: 0.4803 - val_top1_accuracy: 0.8430 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3422 - top1_accuracy: 0.8805\n",
      "Epoch 44: val_top1_accuracy did not improve from 0.84340\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.3423 - top1_accuracy: 0.8805 - val_loss: 0.5128 - val_top1_accuracy: 0.8270 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3506 - top1_accuracy: 0.8775\n",
      "Epoch 45: val_top1_accuracy did not improve from 0.84340\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.3505 - top1_accuracy: 0.8775 - val_loss: 0.4748 - val_top1_accuracy: 0.8424 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3276 - top1_accuracy: 0.8870\n",
      "Epoch 46: val_top1_accuracy did not improve from 0.84340\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.3276 - top1_accuracy: 0.8870 - val_loss: 0.5058 - val_top1_accuracy: 0.8356 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3057 - top1_accuracy: 0.8931\n",
      "Epoch 47: val_top1_accuracy did not improve from 0.84340\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.3057 - top1_accuracy: 0.8931 - val_loss: 0.4914 - val_top1_accuracy: 0.8422 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2666 - top1_accuracy: 0.9076\n",
      "Epoch 48: val_top1_accuracy improved from 0.84340 to 0.85680, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.2666 - top1_accuracy: 0.9076 - val_loss: 0.4637 - val_top1_accuracy: 0.8568 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2220 - top1_accuracy: 0.9205\n",
      "Epoch 49: val_top1_accuracy improved from 0.85680 to 0.85800, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.2220 - top1_accuracy: 0.9205 - val_loss: 0.4780 - val_top1_accuracy: 0.8580 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2199 - top1_accuracy: 0.9222\n",
      "Epoch 50: val_top1_accuracy improved from 0.85800 to 0.86140, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.2199 - top1_accuracy: 0.9222 - val_loss: 0.4587 - val_top1_accuracy: 0.8614 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "metrics_names: ['loss', 'compile_metrics']\n",
      "results: [0.4937790632247925, 0.8553000092506409]\n",
      "Final Test Accuracy: 85.53%\n",
      "Test Top-1 Error: 14.469999074935913\n",
      "Test Top-5 Error: 0.6799996\n",
      "============================================================\n",
      "Final Test Loss: 0.4938\n",
      "============================================================\n",
      "Training Top-1 Error: 7.966667413711548\n",
      "Validation Top-1 Error: 13.859999179840088\n",
      "\n",
      "Best Training Accuracy: 92.03%\n",
      "Best Validation Accuracy: 86.14%\n",
      "Final Test Accuracy: 85.53%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/attention164_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python \"train_cifar_new 12.14.py\" \\\n",
    "  --model attention164 \\\n",
    "  --att_type nal \\\n",
    "  --epochs 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Skb9gGJGR7if"
   },
   "source": [
    "### run latest attention 164 arl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3323849,
     "status": "ok",
     "timestamp": 1765825337622,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "1nPG7XI5hViw",
    "outputId": "07ab5094-f23f-407c-dee6-1bd4e00398eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 18:06:54.140251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765822014.160511   53123 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765822014.166686   53123 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765822014.182128   53123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765822014.182153   53123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765822014.182156   53123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765822014.182158   53123 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 18:06:54.186755: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention164\n",
      "Epochs: 50\n",
      "Batch size: 64\n",
      "Learning rate: 0.001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 45000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "\n",
      "Building attention164 model...\n",
      "2025-12-15 18:07:03.704465: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765822023.704660   53123 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1765822031.378227   53123 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "\u001b[1mModel: \"residual_attention_model164\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module                │ ?                      │     \u001b[32m1,133,056\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_1              │ ?                      │     \u001b[32m4,204,544\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_2              │ ?                      │    \u001b[32m13,371,392\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m6,037,504\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_142         │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m28,222,666\u001b[0m (107.66 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m28,138,698\u001b[0m (107.34 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m83,968\u001b[0m (328.00 KB)\n",
      "\n",
      "Total parameters: 28,222,666\n",
      "Epoch 1/50\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765822093.655389   53212 service.cc:152] XLA service 0x7d35a0066560 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765822093.655427   53212 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-15 18:08:15.549256: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765822138.030344   53212 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 1.9920 - top1_accuracy: 0.3064\n",
      "Epoch 1: val_top1_accuracy improved from -inf to 0.46820, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 164ms/step - loss: 1.9917 - top1_accuracy: 0.3065 - val_loss: 3.1612 - val_top1_accuracy: 0.4682 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.5931 - top1_accuracy: 0.4541\n",
      "Epoch 2: val_top1_accuracy did not improve from 0.46820\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 1.5931 - top1_accuracy: 0.4541 - val_loss: 3.9424 - val_top1_accuracy: 0.3142 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.6640 - top1_accuracy: 0.4291\n",
      "Epoch 3: val_top1_accuracy improved from 0.46820 to 0.51920, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 1.6639 - top1_accuracy: 0.4291 - val_loss: 1.8987 - val_top1_accuracy: 0.5192 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.4403 - top1_accuracy: 0.5077\n",
      "Epoch 4: val_top1_accuracy improved from 0.51920 to 0.57460, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.4403 - top1_accuracy: 0.5077 - val_loss: 1.3635 - val_top1_accuracy: 0.5746 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.2593 - top1_accuracy: 0.5617\n",
      "Epoch 5: val_top1_accuracy improved from 0.57460 to 0.59440, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.2593 - top1_accuracy: 0.5617 - val_loss: 1.1280 - val_top1_accuracy: 0.5944 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.3100 - top1_accuracy: 0.5552\n",
      "Epoch 6: val_top1_accuracy improved from 0.59440 to 0.62200, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.3100 - top1_accuracy: 0.5552 - val_loss: 5.5658 - val_top1_accuracy: 0.6220 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.4015 - top1_accuracy: 0.5003\n",
      "Epoch 7: val_top1_accuracy improved from 0.62200 to 0.63720, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.4013 - top1_accuracy: 0.5003 - val_loss: 1.2709 - val_top1_accuracy: 0.6372 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.2225 - top1_accuracy: 0.5884\n",
      "Epoch 8: val_top1_accuracy did not improve from 0.63720\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 1.2225 - top1_accuracy: 0.5884 - val_loss: 1.2099 - val_top1_accuracy: 0.5852 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.1764 - top1_accuracy: 0.5917\n",
      "Epoch 9: val_top1_accuracy improved from 0.63720 to 0.70120, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.1762 - top1_accuracy: 0.5917 - val_loss: 0.8404 - val_top1_accuracy: 0.7012 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9507 - top1_accuracy: 0.6692\n",
      "Epoch 10: val_top1_accuracy did not improve from 0.70120\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.9506 - top1_accuracy: 0.6692 - val_loss: 0.8993 - val_top1_accuracy: 0.6864 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9919 - top1_accuracy: 0.6579\n",
      "Epoch 11: val_top1_accuracy did not improve from 0.70120\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.9919 - top1_accuracy: 0.6579 - val_loss: 1.9059 - val_top1_accuracy: 0.6852 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9242 - top1_accuracy: 0.6798\n",
      "Epoch 12: val_top1_accuracy did not improve from 0.70120\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.9242 - top1_accuracy: 0.6798 - val_loss: 1.1342 - val_top1_accuracy: 0.6540 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.5741 - top1_accuracy: 0.4530\n",
      "Epoch 13: val_top1_accuracy did not improve from 0.70120\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 1.5738 - top1_accuracy: 0.4531 - val_loss: 1.0619 - val_top1_accuracy: 0.6136 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.1081 - top1_accuracy: 0.6112\n",
      "Epoch 14: val_top1_accuracy did not improve from 0.70120\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 1.1081 - top1_accuracy: 0.6112 - val_loss: 0.8694 - val_top1_accuracy: 0.6920 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9299 - top1_accuracy: 0.6723\n",
      "Epoch 15: val_top1_accuracy improved from 0.70120 to 0.71220, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.9299 - top1_accuracy: 0.6723 - val_loss: 0.8062 - val_top1_accuracy: 0.7122 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8500 - top1_accuracy: 0.6979\n",
      "Epoch 16: val_top1_accuracy improved from 0.71220 to 0.72940, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.8500 - top1_accuracy: 0.6979 - val_loss: 0.7587 - val_top1_accuracy: 0.7294 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8145 - top1_accuracy: 0.7133\n",
      "Epoch 17: val_top1_accuracy did not improve from 0.72940\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.8145 - top1_accuracy: 0.7133 - val_loss: 0.7863 - val_top1_accuracy: 0.7232 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9207 - top1_accuracy: 0.6832\n",
      "Epoch 18: val_top1_accuracy improved from 0.72940 to 0.74960, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.9206 - top1_accuracy: 0.6832 - val_loss: 0.7141 - val_top1_accuracy: 0.7496 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7415 - top1_accuracy: 0.7384\n",
      "Epoch 19: val_top1_accuracy improved from 0.74960 to 0.75400, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.7415 - top1_accuracy: 0.7384 - val_loss: 0.7115 - val_top1_accuracy: 0.7540 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7017 - top1_accuracy: 0.7545\n",
      "Epoch 20: val_top1_accuracy improved from 0.75400 to 0.75420, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.7018 - top1_accuracy: 0.7545 - val_loss: 0.7037 - val_top1_accuracy: 0.7542 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6894 - top1_accuracy: 0.7611\n",
      "Epoch 21: val_top1_accuracy improved from 0.75420 to 0.76640, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.6894 - top1_accuracy: 0.7611 - val_loss: 0.6619 - val_top1_accuracy: 0.7664 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6602 - top1_accuracy: 0.7685\n",
      "Epoch 22: val_top1_accuracy did not improve from 0.76640\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.6601 - top1_accuracy: 0.7685 - val_loss: 0.8036 - val_top1_accuracy: 0.7562 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6337 - top1_accuracy: 0.7729\n",
      "Epoch 23: val_top1_accuracy improved from 0.76640 to 0.76920, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.6337 - top1_accuracy: 0.7729 - val_loss: 0.7090 - val_top1_accuracy: 0.7692 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6288 - top1_accuracy: 0.7835\n",
      "Epoch 24: val_top1_accuracy did not improve from 0.76920\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.6288 - top1_accuracy: 0.7835 - val_loss: 0.6661 - val_top1_accuracy: 0.7680 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6281 - top1_accuracy: 0.7783\n",
      "Epoch 25: val_top1_accuracy improved from 0.76920 to 0.77440, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.6281 - top1_accuracy: 0.7783 - val_loss: 0.6551 - val_top1_accuracy: 0.7744 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5703 - top1_accuracy: 0.7993\n",
      "Epoch 26: val_top1_accuracy improved from 0.77440 to 0.78620, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.5703 - top1_accuracy: 0.7993 - val_loss: 0.6274 - val_top1_accuracy: 0.7862 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6438 - top1_accuracy: 0.7749\n",
      "Epoch 27: val_top1_accuracy improved from 0.78620 to 0.79220, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.6437 - top1_accuracy: 0.7749 - val_loss: 0.6097 - val_top1_accuracy: 0.7922 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5308 - top1_accuracy: 0.8129\n",
      "Epoch 28: val_top1_accuracy improved from 0.79220 to 0.79680, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.5308 - top1_accuracy: 0.8129 - val_loss: 0.5846 - val_top1_accuracy: 0.7968 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5184 - top1_accuracy: 0.8169\n",
      "Epoch 29: val_top1_accuracy improved from 0.79680 to 0.81080, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.5184 - top1_accuracy: 0.8169 - val_loss: 0.5711 - val_top1_accuracy: 0.8108 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.5032 - top1_accuracy: 0.8229\n",
      "Epoch 30: val_top1_accuracy did not improve from 0.81080\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.5032 - top1_accuracy: 0.8229 - val_loss: 0.5575 - val_top1_accuracy: 0.8082 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4734 - top1_accuracy: 0.8332\n",
      "Epoch 31: val_top1_accuracy did not improve from 0.81080\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.4734 - top1_accuracy: 0.8332 - val_loss: 0.7479 - val_top1_accuracy: 0.7984 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4611 - top1_accuracy: 0.8377\n",
      "Epoch 32: val_top1_accuracy improved from 0.81080 to 0.81420, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.4611 - top1_accuracy: 0.8377 - val_loss: 0.5516 - val_top1_accuracy: 0.8142 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4379 - top1_accuracy: 0.8449\n",
      "Epoch 33: val_top1_accuracy did not improve from 0.81420\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.4380 - top1_accuracy: 0.8449 - val_loss: 0.5508 - val_top1_accuracy: 0.8118 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4414 - top1_accuracy: 0.8453\n",
      "Epoch 34: val_top1_accuracy did not improve from 0.81420\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.4414 - top1_accuracy: 0.8453 - val_loss: 0.7389 - val_top1_accuracy: 0.7934 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4624 - top1_accuracy: 0.8377\n",
      "Epoch 35: val_top1_accuracy did not improve from 0.81420\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.4623 - top1_accuracy: 0.8377 - val_loss: 0.6689 - val_top1_accuracy: 0.8120 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4324 - top1_accuracy: 0.8478\n",
      "Epoch 36: val_top1_accuracy improved from 0.81420 to 0.82140, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.4324 - top1_accuracy: 0.8478 - val_loss: 0.5424 - val_top1_accuracy: 0.8214 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4081 - top1_accuracy: 0.8559\n",
      "Epoch 37: val_top1_accuracy did not improve from 0.82140\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.4081 - top1_accuracy: 0.8559 - val_loss: 0.6447 - val_top1_accuracy: 0.7966 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4463 - top1_accuracy: 0.8432\n",
      "Epoch 38: val_top1_accuracy did not improve from 0.82140\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.4464 - top1_accuracy: 0.8432 - val_loss: 1.1234 - val_top1_accuracy: 0.8138 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4145 - top1_accuracy: 0.8539\n",
      "Epoch 39: val_top1_accuracy did not improve from 0.82140\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.4145 - top1_accuracy: 0.8539 - val_loss: 0.7532 - val_top1_accuracy: 0.7818 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3972 - top1_accuracy: 0.8619\n",
      "Epoch 40: val_top1_accuracy did not improve from 0.82140\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.3972 - top1_accuracy: 0.8619 - val_loss: 0.9188 - val_top1_accuracy: 0.8188 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3673 - top1_accuracy: 0.8706\n",
      "Epoch 41: val_top1_accuracy improved from 0.82140 to 0.82240, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.3673 - top1_accuracy: 0.8706 - val_loss: 0.6369 - val_top1_accuracy: 0.8224 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3426 - top1_accuracy: 0.8791\n",
      "Epoch 42: val_top1_accuracy improved from 0.82240 to 0.83220, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.3426 - top1_accuracy: 0.8791 - val_loss: 0.5086 - val_top1_accuracy: 0.8322 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3307 - top1_accuracy: 0.8828\n",
      "Epoch 43: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 0.3307 - top1_accuracy: 0.8828 - val_loss: 0.5774 - val_top1_accuracy: 0.8256 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3486 - top1_accuracy: 0.8787\n",
      "Epoch 44: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.3486 - top1_accuracy: 0.8787 - val_loss: 0.5471 - val_top1_accuracy: 0.8306 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3228 - top1_accuracy: 0.8857\n",
      "Epoch 45: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.3228 - top1_accuracy: 0.8857 - val_loss: 0.6243 - val_top1_accuracy: 0.8290 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3131 - top1_accuracy: 0.8896\n",
      "Epoch 46: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.3131 - top1_accuracy: 0.8896 - val_loss: 1.7932 - val_top1_accuracy: 0.8160 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3044 - top1_accuracy: 0.8931\n",
      "Epoch 47: val_top1_accuracy did not improve from 0.83220\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.3044 - top1_accuracy: 0.8931 - val_loss: 2.8765 - val_top1_accuracy: 0.8032 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2645 - top1_accuracy: 0.9052\n",
      "Epoch 48: val_top1_accuracy improved from 0.83220 to 0.83860, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.2644 - top1_accuracy: 0.9052 - val_loss: 0.5167 - val_top1_accuracy: 0.8386 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2492 - top1_accuracy: 0.9102\n",
      "Epoch 49: val_top1_accuracy improved from 0.83860 to 0.84180, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.2491 - top1_accuracy: 0.9102 - val_loss: 0.5325 - val_top1_accuracy: 0.8418 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2210 - top1_accuracy: 0.9211\n",
      "Epoch 50: val_top1_accuracy did not improve from 0.84180\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - loss: 0.2210 - top1_accuracy: 0.9211 - val_loss: 0.5909 - val_top1_accuracy: 0.8364 - learning_rate: 2.5000e-04\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "metrics_names: ['loss', 'compile_metrics']\n",
      "results: [0.5408583283424377, 0.8378999829292297]\n",
      "Final Test Accuracy: 83.79%\n",
      "Test Top-1 Error: 16.210001707077026\n",
      "Test Top-5 Error: 0.8700013\n",
      "============================================================\n",
      "Final Test Loss: 0.5409\n",
      "============================================================\n",
      "Training Top-1 Error: 7.840001583099365\n",
      "Validation Top-1 Error: 15.820002555847168\n",
      "\n",
      "Best Training Accuracy: 92.16%\n",
      "Best Validation Accuracy: 84.18%\n",
      "Final Test Accuracy: 83.79%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/attention164_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python \"train_cifar_new 12.14.py\" --model attention164 --epochs 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIdBDdZWSCJx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uetDPhC-R_4J"
   },
   "source": [
    "### run latest attention 164 nal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3360178,
     "status": "ok",
     "timestamp": 1765829014446,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "tSk81jcaaQTA",
    "outputId": "c1013bab-9f8f-4c49-d0f9-b230b4a19373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 19:07:34.564575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765825654.583811   69966 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765825654.589568   69966 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765825654.605198   69966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765825654.605222   69966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765825654.605224   69966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765825654.605227   69966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 19:07:34.609822: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention164\n",
      "Epochs: 50\n",
      "Batch size: 64\n",
      "Learning rate: 0.001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 45000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "\n",
      "Building attention164 model...\n",
      "2025-12-15 19:07:44.201259: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765825664.201406   69966 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1765825671.928881   69966 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "\u001b[1mModel: \"residual_attention_model164\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module                │ ?                      │     \u001b[32m1,133,056\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_1              │ ?                      │     \u001b[32m4,204,544\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_2              │ ?                      │    \u001b[32m13,371,392\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m6,037,504\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_142         │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m28,222,666\u001b[0m (107.66 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m28,138,698\u001b[0m (107.34 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m83,968\u001b[0m (328.00 KB)\n",
      "\n",
      "Total parameters: 28,222,666\n",
      "Epoch 1/50\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765825734.566816   70057 service.cc:152] XLA service 0x7bbb04061310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765825734.566859   70057 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-15 19:08:56.588730: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765825779.961084   70057 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 2.0892 - top1_accuracy: 0.2646\n",
      "Epoch 1: val_top1_accuracy improved from -inf to 0.35140, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 171ms/step - loss: 2.0889 - top1_accuracy: 0.2647 - val_loss: 2.4047 - val_top1_accuracy: 0.3514 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.7442 - top1_accuracy: 0.3728\n",
      "Epoch 2: val_top1_accuracy improved from 0.35140 to 0.46640, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 92ms/step - loss: 1.7441 - top1_accuracy: 0.3729 - val_loss: 5.6907 - val_top1_accuracy: 0.4664 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.5873 - top1_accuracy: 0.4451\n",
      "Epoch 3: val_top1_accuracy did not improve from 0.46640\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 1.5873 - top1_accuracy: 0.4451 - val_loss: 40.3702 - val_top1_accuracy: 0.2310 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.6207 - top1_accuracy: 0.4364\n",
      "Epoch 4: val_top1_accuracy improved from 0.46640 to 0.51800, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.6206 - top1_accuracy: 0.4364 - val_loss: 1.2963 - val_top1_accuracy: 0.5180 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.4997 - top1_accuracy: 0.4791\n",
      "Epoch 5: val_top1_accuracy improved from 0.51800 to 0.55940, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 90ms/step - loss: 1.4996 - top1_accuracy: 0.4791 - val_loss: 1.2208 - val_top1_accuracy: 0.5594 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.4765 - top1_accuracy: 0.4859\n",
      "Epoch 6: val_top1_accuracy did not improve from 0.55940\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 1.4765 - top1_accuracy: 0.4859 - val_loss: 1.3625 - val_top1_accuracy: 0.5282 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.4407 - top1_accuracy: 0.4902\n",
      "Epoch 7: val_top1_accuracy did not improve from 0.55940\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 1.4407 - top1_accuracy: 0.4902 - val_loss: 19.0510 - val_top1_accuracy: 0.3730 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.5877 - top1_accuracy: 0.4606\n",
      "Epoch 8: val_top1_accuracy did not improve from 0.55940\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 1.5876 - top1_accuracy: 0.4606 - val_loss: 1.9081 - val_top1_accuracy: 0.5154 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.5336 - top1_accuracy: 0.4677\n",
      "Epoch 9: val_top1_accuracy did not improve from 0.55940\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 1.5335 - top1_accuracy: 0.4677 - val_loss: 10.3491 - val_top1_accuracy: 0.3856 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.5061 - top1_accuracy: 0.4694\n",
      "Epoch 10: val_top1_accuracy did not improve from 0.55940\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 87ms/step - loss: 1.5061 - top1_accuracy: 0.4694 - val_loss: 2.8027 - val_top1_accuracy: 0.3452 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.4119 - top1_accuracy: 0.4912\n",
      "Epoch 11: val_top1_accuracy improved from 0.55940 to 0.60600, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.4117 - top1_accuracy: 0.4912 - val_loss: 1.1050 - val_top1_accuracy: 0.6060 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.1873 - top1_accuracy: 0.5796\n",
      "Epoch 12: val_top1_accuracy improved from 0.60600 to 0.61720, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.1873 - top1_accuracy: 0.5796 - val_loss: 1.3721 - val_top1_accuracy: 0.6172 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.0903 - top1_accuracy: 0.6151\n",
      "Epoch 13: val_top1_accuracy improved from 0.61720 to 0.62760, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.0903 - top1_accuracy: 0.6151 - val_loss: 1.0532 - val_top1_accuracy: 0.6276 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.0499 - top1_accuracy: 0.6292\n",
      "Epoch 14: val_top1_accuracy improved from 0.62760 to 0.68820, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 1.0498 - top1_accuracy: 0.6292 - val_loss: 0.8938 - val_top1_accuracy: 0.6882 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.9667 - top1_accuracy: 0.6547\n",
      "Epoch 15: val_top1_accuracy did not improve from 0.68820\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 0.9667 - top1_accuracy: 0.6547 - val_loss: 1.8109 - val_top1_accuracy: 0.5964 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.9730 - top1_accuracy: 0.6582\n",
      "Epoch 16: val_top1_accuracy improved from 0.68820 to 0.70360, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 91ms/step - loss: 0.9729 - top1_accuracy: 0.6583 - val_loss: 0.8408 - val_top1_accuracy: 0.7036 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8978 - top1_accuracy: 0.6850\n",
      "Epoch 17: val_top1_accuracy improved from 0.70360 to 0.72080, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 91ms/step - loss: 0.8977 - top1_accuracy: 0.6850 - val_loss: 0.7795 - val_top1_accuracy: 0.7208 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8700 - top1_accuracy: 0.6944\n",
      "Epoch 18: val_top1_accuracy improved from 0.72080 to 0.73140, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.8700 - top1_accuracy: 0.6944 - val_loss: 0.7842 - val_top1_accuracy: 0.7314 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8633 - top1_accuracy: 0.7004\n",
      "Epoch 19: val_top1_accuracy improved from 0.73140 to 0.74380, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 90ms/step - loss: 0.8632 - top1_accuracy: 0.7005 - val_loss: 0.7259 - val_top1_accuracy: 0.7438 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7622 - top1_accuracy: 0.7289\n",
      "Epoch 20: val_top1_accuracy improved from 0.74380 to 0.75260, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 90ms/step - loss: 0.7622 - top1_accuracy: 0.7289 - val_loss: 0.7004 - val_top1_accuracy: 0.7526 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7183 - top1_accuracy: 0.7436\n",
      "Epoch 21: val_top1_accuracy did not improve from 0.75260\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.7183 - top1_accuracy: 0.7436 - val_loss: 0.7305 - val_top1_accuracy: 0.7454 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7289 - top1_accuracy: 0.7459\n",
      "Epoch 22: val_top1_accuracy did not improve from 0.75260\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 0.7289 - top1_accuracy: 0.7459 - val_loss: 0.8100 - val_top1_accuracy: 0.7510 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.6781 - top1_accuracy: 0.7616\n",
      "Epoch 23: val_top1_accuracy improved from 0.75260 to 0.77300, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 91ms/step - loss: 0.6781 - top1_accuracy: 0.7616 - val_loss: 0.6476 - val_top1_accuracy: 0.7730 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.6407 - top1_accuracy: 0.7739\n",
      "Epoch 24: val_top1_accuracy did not improve from 0.77300\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 0.6407 - top1_accuracy: 0.7739 - val_loss: 0.9087 - val_top1_accuracy: 0.7664 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6443 - top1_accuracy: 0.7768\n",
      "Epoch 25: val_top1_accuracy did not improve from 0.77300\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.6443 - top1_accuracy: 0.7768 - val_loss: 0.8898 - val_top1_accuracy: 0.7540 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6294 - top1_accuracy: 0.7801\n",
      "Epoch 26: val_top1_accuracy improved from 0.77300 to 0.79660, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.6294 - top1_accuracy: 0.7801 - val_loss: 0.5862 - val_top1_accuracy: 0.7966 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.5802 - top1_accuracy: 0.7970\n",
      "Epoch 27: val_top1_accuracy did not improve from 0.79660\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 0.5802 - top1_accuracy: 0.7970 - val_loss: 0.8152 - val_top1_accuracy: 0.7814 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.5754 - top1_accuracy: 0.7995\n",
      "Epoch 28: val_top1_accuracy improved from 0.79660 to 0.79820, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 90ms/step - loss: 0.5753 - top1_accuracy: 0.7995 - val_loss: 0.5915 - val_top1_accuracy: 0.7982 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5366 - top1_accuracy: 0.8127\n",
      "Epoch 29: val_top1_accuracy did not improve from 0.79820\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.5366 - top1_accuracy: 0.8127 - val_loss: 0.6359 - val_top1_accuracy: 0.7846 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.5118 - top1_accuracy: 0.8216\n",
      "Epoch 30: val_top1_accuracy did not improve from 0.79820\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 0.5118 - top1_accuracy: 0.8216 - val_loss: 0.6565 - val_top1_accuracy: 0.7914 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5342 - top1_accuracy: 0.8141\n",
      "Epoch 31: val_top1_accuracy improved from 0.79820 to 0.79840, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.5341 - top1_accuracy: 0.8141 - val_loss: 0.5954 - val_top1_accuracy: 0.7984 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.5443 - top1_accuracy: 0.8146\n",
      "Epoch 32: val_top1_accuracy improved from 0.79840 to 0.80320, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 90ms/step - loss: 0.5443 - top1_accuracy: 0.8145 - val_loss: 0.5632 - val_top1_accuracy: 0.8032 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.4728 - top1_accuracy: 0.8330\n",
      "Epoch 33: val_top1_accuracy did not improve from 0.80320\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 0.4728 - top1_accuracy: 0.8330 - val_loss: 0.5854 - val_top1_accuracy: 0.8024 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4528 - top1_accuracy: 0.8398\n",
      "Epoch 34: val_top1_accuracy improved from 0.80320 to 0.80920, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.4528 - top1_accuracy: 0.8398 - val_loss: 0.5554 - val_top1_accuracy: 0.8092 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4439 - top1_accuracy: 0.8471\n",
      "Epoch 35: val_top1_accuracy improved from 0.80920 to 0.81460, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.4439 - top1_accuracy: 0.8471 - val_loss: 0.5331 - val_top1_accuracy: 0.8146 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4847 - top1_accuracy: 0.8323\n",
      "Epoch 36: val_top1_accuracy did not improve from 0.81460\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 0.4848 - top1_accuracy: 0.8323 - val_loss: 1.0822 - val_top1_accuracy: 0.7924 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5059 - top1_accuracy: 0.8247\n",
      "Epoch 37: val_top1_accuracy improved from 0.81460 to 0.82120, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.5058 - top1_accuracy: 0.8247 - val_loss: 0.5251 - val_top1_accuracy: 0.8212 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4345 - top1_accuracy: 0.8487\n",
      "Epoch 38: val_top1_accuracy did not improve from 0.82120\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.4345 - top1_accuracy: 0.8487 - val_loss: 0.6496 - val_top1_accuracy: 0.7892 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5123 - top1_accuracy: 0.8242\n",
      "Epoch 39: val_top1_accuracy improved from 0.82120 to 0.82300, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.5122 - top1_accuracy: 0.8242 - val_loss: 0.5136 - val_top1_accuracy: 0.8230 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.4019 - top1_accuracy: 0.8587\n",
      "Epoch 40: val_top1_accuracy improved from 0.82300 to 0.83340, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 90ms/step - loss: 0.4019 - top1_accuracy: 0.8588 - val_loss: 0.5047 - val_top1_accuracy: 0.8334 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3689 - top1_accuracy: 0.8711\n",
      "Epoch 41: val_top1_accuracy did not improve from 0.83340\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.3689 - top1_accuracy: 0.8711 - val_loss: 0.5260 - val_top1_accuracy: 0.8210 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.3782 - top1_accuracy: 0.8650\n",
      "Epoch 42: val_top1_accuracy did not improve from 0.83340\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - loss: 0.3782 - top1_accuracy: 0.8650 - val_loss: 0.5260 - val_top1_accuracy: 0.8234 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4013 - top1_accuracy: 0.8580\n",
      "Epoch 43: val_top1_accuracy did not improve from 0.83340\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.4013 - top1_accuracy: 0.8580 - val_loss: 0.5336 - val_top1_accuracy: 0.8334 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3530 - top1_accuracy: 0.8767\n",
      "Epoch 44: val_top1_accuracy improved from 0.83340 to 0.83380, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.3530 - top1_accuracy: 0.8767 - val_loss: 0.5155 - val_top1_accuracy: 0.8338 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3495 - top1_accuracy: 0.8776\n",
      "Epoch 45: val_top1_accuracy did not improve from 0.83380\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.3495 - top1_accuracy: 0.8776 - val_loss: 0.5578 - val_top1_accuracy: 0.8198 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3680 - top1_accuracy: 0.8731\n",
      "Epoch 46: val_top1_accuracy did not improve from 0.83380\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.3679 - top1_accuracy: 0.8731 - val_loss: 0.5580 - val_top1_accuracy: 0.8190 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3581 - top1_accuracy: 0.8739\n",
      "Epoch 47: val_top1_accuracy improved from 0.83380 to 0.83640, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 90ms/step - loss: 0.3580 - top1_accuracy: 0.8739 - val_loss: 0.5131 - val_top1_accuracy: 0.8364 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3225 - top1_accuracy: 0.8872\n",
      "Epoch 48: val_top1_accuracy did not improve from 0.83640\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.3224 - top1_accuracy: 0.8872 - val_loss: 0.5368 - val_top1_accuracy: 0.8288 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3363 - top1_accuracy: 0.8812\n",
      "Epoch 49: val_top1_accuracy improved from 0.83640 to 0.84400, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - loss: 0.3362 - top1_accuracy: 0.8812 - val_loss: 0.4891 - val_top1_accuracy: 0.8440 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2932 - top1_accuracy: 0.8967\n",
      "Epoch 50: val_top1_accuracy did not improve from 0.84400\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 87ms/step - loss: 0.2933 - top1_accuracy: 0.8967 - val_loss: 0.5355 - val_top1_accuracy: 0.8304 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "metrics_names: ['loss', 'compile_metrics']\n",
      "results: [0.5278294682502747, 0.8356000185012817]\n",
      "Final Test Accuracy: 83.56%\n",
      "Test Top-1 Error: 16.439998149871826\n",
      "Test Top-5 Error: 0.8000016\n",
      "============================================================\n",
      "Final Test Loss: 0.5278\n",
      "============================================================\n",
      "Training Top-1 Error: 10.55111289024353\n",
      "Validation Top-1 Error: 15.600001811981201\n",
      "\n",
      "Best Training Accuracy: 89.45%\n",
      "Best Validation Accuracy: 84.40%\n",
      "Final Test Accuracy: 83.56%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/attention164_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python \"train_cifar_new 12.14.py\" \\\n",
    "  --model attention164 \\\n",
    "  --att_type nal \\\n",
    "  --epochs 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1765833064337,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "iAMzgDWs1oBv",
    "outputId": "16f8e25c-a3a0-4156-c0a1-4d0a5418ec55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   models/resnet164_tf.py\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2he7YqQ2j5s"
   },
   "outputs": [],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1765833440808,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "6cAojdQI3yH5",
    "outputId": "328b5f31-7ead-4832-b15a-547a1efcc123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author identity unknown\n",
      "\n",
      "*** Please tell me who you are.\n",
      "\n",
      "Run\n",
      "\n",
      "  git config --global user.email \"you@example.com\"\n",
      "  git config --global user.name \"Your Name\"\n",
      "\n",
      "to set your account's default identity.\n",
      "Omit --global to set the identity only in this repository.\n",
      "\n",
      "fatal: unable to auto-detect email address (got 'root@8d5f0dbf6016.(none)')\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"changed the attention stage of attention164_tf, deepen the model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4xxqmV53_yX"
   },
   "outputs": [],
   "source": [
    "!git config --global user.name \"MeiYue158\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fhx6idOT4PAo"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"my2903@columbia.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1765833549346,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "5Qk2OXKN4V3P",
    "outputId": "7d8ae649-9caf-4646-ba19-91990f7d9c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 215ccaf] Change attention stage of attention164_tf; deepen the model\n",
      " 1 file changed, 119 insertions(+), 36 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"Change attention stage of attention164_tf; deepen the model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 723,
     "status": "ok",
     "timestamp": 1765833561624,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "3xx_-_VG4aPm",
    "outputId": "19f9b27f-0684-40f6-b322-fcb19a800984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 26, done.\u001b[K\n",
      "remote: Counting objects:   3% (1/26)\u001b[K\rremote: Counting objects:   7% (2/26)\u001b[K\rremote: Counting objects:  11% (3/26)\u001b[K\rremote: Counting objects:  15% (4/26)\u001b[K\rremote: Counting objects:  19% (5/26)\u001b[K\rremote: Counting objects:  23% (6/26)\u001b[K\rremote: Counting objects:  26% (7/26)\u001b[K\rremote: Counting objects:  30% (8/26)\u001b[K\rremote: Counting objects:  34% (9/26)\u001b[K\rremote: Counting objects:  38% (10/26)\u001b[K\rremote: Counting objects:  42% (11/26)\u001b[K\rremote: Counting objects:  46% (12/26)\u001b[K\rremote: Counting objects:  50% (13/26)\u001b[K\rremote: Counting objects:  53% (14/26)\u001b[K\rremote: Counting objects:  57% (15/26)\u001b[K\rremote: Counting objects:  61% (16/26)\u001b[K\rremote: Counting objects:  65% (17/26)\u001b[K\rremote: Counting objects:  69% (18/26)\u001b[K\rremote: Counting objects:  73% (19/26)\u001b[K\rremote: Counting objects:  76% (20/26)\u001b[K\rremote: Counting objects:  80% (21/26)\u001b[K\rremote: Counting objects:  84% (22/26)\u001b[K\rremote: Counting objects:  88% (23/26)\u001b[K\rremote: Counting objects:  92% (24/26)\u001b[K\rremote: Counting objects:  96% (25/26)\u001b[K\rremote: Counting objects: 100% (26/26)\u001b[K\rremote: Counting objects: 100% (26/26), done.\u001b[K\n",
      "remote: Compressing objects:   7% (1/14)\u001b[K\rremote: Compressing objects:  14% (2/14)\u001b[K\rremote: Compressing objects:  21% (3/14)\u001b[K\rremote: Compressing objects:  28% (4/14)\u001b[K\rremote: Compressing objects:  35% (5/14)\u001b[K\rremote: Compressing objects:  42% (6/14)\u001b[K\rremote: Compressing objects:  50% (7/14)\u001b[K\rremote: Compressing objects:  57% (8/14)\u001b[K\rremote: Compressing objects:  64% (9/14)\u001b[K\rremote: Compressing objects:  71% (10/14)\u001b[K\rremote: Compressing objects:  78% (11/14)\u001b[K\rremote: Compressing objects:  85% (12/14)\u001b[K\rremote: Compressing objects:  92% (13/14)\u001b[K\rremote: Compressing objects: 100% (14/14)\u001b[K\rremote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 21 (delta 15), reused 13 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects:   4% (1/21)\rUnpacking objects:   9% (2/21)\rUnpacking objects:  14% (3/21)\rUnpacking objects:  19% (4/21)\rUnpacking objects:  23% (5/21)\rUnpacking objects:  28% (6/21)\rUnpacking objects:  33% (7/21)\rUnpacking objects:  38% (8/21)\rUnpacking objects:  42% (9/21)\rUnpacking objects:  47% (10/21)\rUnpacking objects:  52% (11/21)\rUnpacking objects:  57% (12/21)\rUnpacking objects:  61% (13/21)\rUnpacking objects:  66% (14/21)\rUnpacking objects:  71% (15/21)\rUnpacking objects:  76% (16/21)\rUnpacking objects:  80% (17/21)\rUnpacking objects:  85% (18/21)\rUnpacking objects:  90% (19/21)\rUnpacking objects:  95% (20/21)\rUnpacking objects: 100% (21/21)\rUnpacking objects: 100% (21/21), 2.36 KiB | 402.00 KiB/s, done.\n",
      "From https://github.com/ecbme4040/e4040-fall2025-project-swye\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   9b8eafb..f55e489  main       -> origin/main\n",
      "Rebasing (1/1)\r\r\u001b[KSuccessfully rebased and updated refs/heads/main.\n"
     ]
    }
   ],
   "source": [
    "!git pull --rebase origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqNvK4Df4dKT"
   },
   "outputs": [],
   "source": [
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 769,
     "status": "ok",
     "timestamp": 1765833942539,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "igPnnv9V477Q",
    "outputId": "ad1b6359-73c6-44e2-a5c5-d15709a0d7ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 16, done.\u001b[K\n",
      "remote: Counting objects:   7% (1/14)\u001b[K\rremote: Counting objects:  14% (2/14)\u001b[K\rremote: Counting objects:  21% (3/14)\u001b[K\rremote: Counting objects:  28% (4/14)\u001b[K\rremote: Counting objects:  35% (5/14)\u001b[K\rremote: Counting objects:  42% (6/14)\u001b[K\rremote: Counting objects:  50% (7/14)\u001b[K\rremote: Counting objects:  57% (8/14)\u001b[K\rremote: Counting objects:  64% (9/14)\u001b[K\rremote: Counting objects:  71% (10/14)\u001b[K\rremote: Counting objects:  78% (11/14)\u001b[K\rremote: Counting objects:  85% (12/14)\u001b[K\rremote: Counting objects:  92% (13/14)\u001b[K\rremote: Counting objects: 100% (14/14)\u001b[K\rremote: Counting objects: 100% (14/14), done.\u001b[K\n",
      "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 8 (delta 6), reused 8 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects:  12% (1/8)\rUnpacking objects:  25% (2/8)\rUnpacking objects:  37% (3/8)\rUnpacking objects:  50% (4/8)\rUnpacking objects:  62% (5/8)\rUnpacking objects:  75% (6/8)\rUnpacking objects:  87% (7/8)\rUnpacking objects: 100% (8/8)\rUnpacking objects: 100% (8/8), 872 bytes | 436.00 KiB/s, done.\n",
      "From https://github.com/ecbme4040/e4040-fall2025-project-swye\n",
      "   49c803a..8e3b77b  main       -> origin/main\n",
      "Updating 49c803a..8e3b77b\n",
      "Fast-forward\n",
      " models/attention92_simple.py | 4 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
      " train_cifar_new 12.14.py     | 7 \u001b[32m+++++++\u001b[m\n",
      " 2 files changed, 9 insertions(+), 2 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Yco8MgvSM9s"
   },
   "source": [
    "### run attention92_simple arl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6973,
     "status": "ok",
     "timestamp": 1765834616111,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "5dLWOXFU47zg",
    "outputId": "548ae784-d9fc-45d1-f060-f249966ee997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 21:36:49.485750: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765834609.505369  108686 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765834609.511170  108686 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765834609.526514  108686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765834609.526545  108686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765834609.526548  108686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765834609.526550  108686 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 21:36:49.531037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "usage: train_cifar_new 12.14.py [-h]\n",
      "                                [--model {attention56,attention92,attention128,attention164}]\n",
      "                                [--att_type {arl,nal}] [--epochs EPOCHS]\n",
      "                                [--batch-size BATCH_SIZE] [--lr LR]\n",
      "                                [--save-dir SAVE_DIR]\n",
      "train_cifar_new 12.14.py: error: unrecognized arguments: _simple\n"
     ]
    }
   ],
   "source": [
    "!python \"train_cifar_new 12.14.py\" \\\n",
    "  --model attention92_simple\\\n",
    "  --att_type arl \\\n",
    "  --epochs 50 \\\n",
    "  --batch-size 128 \\\n",
    "  --lr 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1765834024941,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "00UM7XTm6K9U",
    "outputId": "2387d313-37ca-4e8b-f0ea-b99c37deff6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/           README.md                    'train_cifar_new 12.13.py'\n",
      " compare_attention.py   README-proj.md               'train_cifar_new 12.14.py'\n",
      " demo.py                README_TF.md                  train_cifar_new.py\n",
      " demo_ten.py            run_all_models.py             train_cifar_tf.py\n",
      " \u001b[01;34mmodels\u001b[0m/                select_best.py                train_imagenet.py\n",
      " parameter.py           train_best.py                 untitled.txt\n",
      " plot.py                train_cifar_100.py\n",
      " plot_results.py        train_cifar10_robustness.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1765834793695,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "6CDOBGOB6Obt",
    "outputId": "9aef9d83-2820-4adb-d8b8-3fcc3c2b477f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 342 bytes | 342.00 KiB/s, done.\n",
      "From https://github.com/ecbme4040/e4040-fall2025-project-swye\n",
      "   8e3b77b..806d818  main       -> origin/main\n",
      "Updating 8e3b77b..806d818\n",
      "Fast-forward\n",
      " train_cifar_new 12.14.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2299282,
     "status": "ok",
     "timestamp": 1765837133117,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "Lks-mJZr9J9E",
    "outputId": "6276af07-449f-46df-b04b-abb66de11020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 21:40:34.174339: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765834834.193037  109654 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765834834.198855  109654 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765834834.213745  109654 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765834834.213771  109654 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765834834.213774  109654 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765834834.213777  109654 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-15 21:40:34.218192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention92_simple\n",
      "Epochs: 50\n",
      "Batch size: 128\n",
      "Learning rate: 0.001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 45000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "\n",
      "Building attention92_simple model...\n",
      "2025-12-15 21:40:43.839240: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1765834843.839382  109654 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1765834851.150871  109654 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "\u001b[1mModel: \"residual_attention_model92_simple\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module                │ ?                      │     \u001b[32m1,061,888\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_1              │ ?                      │     \u001b[32m3,078,144\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_2              │ ?                      │    \u001b[32m12,251,136\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │    \u001b[32m14,974,976\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_130         │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m34,842,314\u001b[0m (132.91 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m34,756,042\u001b[0m (132.58 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m86,272\u001b[0m (337.00 KB)\n",
      "\n",
      "Total parameters: 34,842,314\n",
      "Epoch 1/50\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765834909.025560  109741 service.cc:152] XLA service 0x7b5688002320 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765834909.025610  109741 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-12-15 21:41:50.777840: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1765834952.605444  109741 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - loss: 1.9731 - top1_accuracy: 0.3045\n",
      "Epoch 1: val_top1_accuracy improved from -inf to 0.36660, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 276ms/step - loss: 1.9725 - top1_accuracy: 0.3047 - val_loss: 4.4208 - val_top1_accuracy: 0.3666 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 1.5418 - top1_accuracy: 0.4503\n",
      "Epoch 2: val_top1_accuracy improved from 0.36660 to 0.43560, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 119ms/step - loss: 1.5418 - top1_accuracy: 0.4503 - val_loss: 5.7754 - val_top1_accuracy: 0.4356 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 1.4032 - top1_accuracy: 0.5068\n",
      "Epoch 3: val_top1_accuracy did not improve from 0.43560\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 114ms/step - loss: 1.4031 - top1_accuracy: 0.5068 - val_loss: 1.7017 - val_top1_accuracy: 0.4228 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 1.4130 - top1_accuracy: 0.5023\n",
      "Epoch 4: val_top1_accuracy improved from 0.43560 to 0.59220, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 119ms/step - loss: 1.4128 - top1_accuracy: 0.5024 - val_loss: 1.1261 - val_top1_accuracy: 0.5922 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 1.3062 - top1_accuracy: 0.5430\n",
      "Epoch 5: val_top1_accuracy improved from 0.59220 to 0.60320, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 120ms/step - loss: 1.3061 - top1_accuracy: 0.5430 - val_loss: 1.0701 - val_top1_accuracy: 0.6032 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 1.1626 - top1_accuracy: 0.5934\n",
      "Epoch 6: val_top1_accuracy did not improve from 0.60320\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 1.1627 - top1_accuracy: 0.5934 - val_loss: 1.3376 - val_top1_accuracy: 0.5720 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 1.2290 - top1_accuracy: 0.5655\n",
      "Epoch 7: val_top1_accuracy improved from 0.60320 to 0.66760, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 1.2288 - top1_accuracy: 0.5655 - val_loss: 0.9418 - val_top1_accuracy: 0.6676 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.9962 - top1_accuracy: 0.6499\n",
      "Epoch 8: val_top1_accuracy improved from 0.66760 to 0.67060, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.9961 - top1_accuracy: 0.6499 - val_loss: 0.8992 - val_top1_accuracy: 0.6706 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.9006 - top1_accuracy: 0.6837\n",
      "Epoch 9: val_top1_accuracy improved from 0.67060 to 0.72860, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.9006 - top1_accuracy: 0.6837 - val_loss: 0.7687 - val_top1_accuracy: 0.7286 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.8386 - top1_accuracy: 0.7060\n",
      "Epoch 10: val_top1_accuracy did not improve from 0.72860\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.8386 - top1_accuracy: 0.7060 - val_loss: 0.8722 - val_top1_accuracy: 0.7006 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.7983 - top1_accuracy: 0.7172\n",
      "Epoch 11: val_top1_accuracy did not improve from 0.72860\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.7982 - top1_accuracy: 0.7172 - val_loss: 0.8508 - val_top1_accuracy: 0.7048 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.7361 - top1_accuracy: 0.7389\n",
      "Epoch 12: val_top1_accuracy improved from 0.72860 to 0.73240, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 0.7361 - top1_accuracy: 0.7389 - val_loss: 0.7715 - val_top1_accuracy: 0.7324 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.7042 - top1_accuracy: 0.7520\n",
      "Epoch 13: val_top1_accuracy did not improve from 0.73240\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.7043 - top1_accuracy: 0.7520 - val_loss: 14.7712 - val_top1_accuracy: 0.3024 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.8521 - top1_accuracy: 0.7067\n",
      "Epoch 14: val_top1_accuracy improved from 0.73240 to 0.73720, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.8519 - top1_accuracy: 0.7068 - val_loss: 0.7415 - val_top1_accuracy: 0.7372 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.7128 - top1_accuracy: 0.7551\n",
      "Epoch 15: val_top1_accuracy improved from 0.73720 to 0.76720, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.7127 - top1_accuracy: 0.7551 - val_loss: 0.6642 - val_top1_accuracy: 0.7672 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.6109 - top1_accuracy: 0.7882\n",
      "Epoch 16: val_top1_accuracy improved from 0.76720 to 0.78880, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.6109 - top1_accuracy: 0.7882 - val_loss: 0.6065 - val_top1_accuracy: 0.7888 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.5903 - top1_accuracy: 0.7944\n",
      "Epoch 17: val_top1_accuracy did not improve from 0.78880\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.5904 - top1_accuracy: 0.7944 - val_loss: 0.7101 - val_top1_accuracy: 0.7582 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.6204 - top1_accuracy: 0.7852\n",
      "Epoch 18: val_top1_accuracy did not improve from 0.78880\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.6204 - top1_accuracy: 0.7853 - val_loss: 0.6910 - val_top1_accuracy: 0.7714 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.5347 - top1_accuracy: 0.8140\n",
      "Epoch 19: val_top1_accuracy improved from 0.78880 to 0.79960, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 0.5347 - top1_accuracy: 0.8140 - val_loss: 0.5931 - val_top1_accuracy: 0.7996 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.5270 - top1_accuracy: 0.8153\n",
      "Epoch 20: val_top1_accuracy did not improve from 0.79960\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.5270 - top1_accuracy: 0.8153 - val_loss: 0.6146 - val_top1_accuracy: 0.7930 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.4946 - top1_accuracy: 0.8276\n",
      "Epoch 21: val_top1_accuracy improved from 0.79960 to 0.80640, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.4946 - top1_accuracy: 0.8276 - val_loss: 0.5944 - val_top1_accuracy: 0.8064 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.5015 - top1_accuracy: 0.8282\n",
      "Epoch 22: val_top1_accuracy improved from 0.80640 to 0.81160, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.5015 - top1_accuracy: 0.8282 - val_loss: 0.5763 - val_top1_accuracy: 0.8116 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.5998 - top1_accuracy: 0.8029\n",
      "Epoch 23: val_top1_accuracy did not improve from 0.81160\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.6000 - top1_accuracy: 0.8028 - val_loss: 0.6604 - val_top1_accuracy: 0.7728 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.5169 - top1_accuracy: 0.8220\n",
      "Epoch 24: val_top1_accuracy improved from 0.81160 to 0.82380, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.5169 - top1_accuracy: 0.8220 - val_loss: 0.5096 - val_top1_accuracy: 0.8238 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.4478 - top1_accuracy: 0.8417\n",
      "Epoch 25: val_top1_accuracy did not improve from 0.82380\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.4478 - top1_accuracy: 0.8417 - val_loss: 0.7211 - val_top1_accuracy: 0.7896 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.5399 - top1_accuracy: 0.8178\n",
      "Epoch 26: val_top1_accuracy did not improve from 0.82380\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.5398 - top1_accuracy: 0.8178 - val_loss: 0.5397 - val_top1_accuracy: 0.8194 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.4260 - top1_accuracy: 0.8530\n",
      "Epoch 27: val_top1_accuracy improved from 0.82380 to 0.82520, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.4261 - top1_accuracy: 0.8530 - val_loss: 0.5211 - val_top1_accuracy: 0.8252 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.4368 - top1_accuracy: 0.8486\n",
      "Epoch 28: val_top1_accuracy improved from 0.82520 to 0.83220, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.4368 - top1_accuracy: 0.8486 - val_loss: 0.4987 - val_top1_accuracy: 0.8322 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.3793 - top1_accuracy: 0.8648\n",
      "Epoch 29: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.3793 - top1_accuracy: 0.8648 - val_loss: 0.5055 - val_top1_accuracy: 0.8312 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.3601 - top1_accuracy: 0.8748\n",
      "Epoch 30: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 115ms/step - loss: 0.3601 - top1_accuracy: 0.8748 - val_loss: 0.5212 - val_top1_accuracy: 0.8222 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.3569 - top1_accuracy: 0.8750\n",
      "Epoch 31: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.3569 - top1_accuracy: 0.8750 - val_loss: 9.6774 - val_top1_accuracy: 0.7370 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.5177 - top1_accuracy: 0.8296\n",
      "Epoch 32: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 115ms/step - loss: 0.5178 - top1_accuracy: 0.8296 - val_loss: 8.2532 - val_top1_accuracy: 0.7042 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.5251 - top1_accuracy: 0.8218\n",
      "Epoch 33: val_top1_accuracy did not improve from 0.83220\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 115ms/step - loss: 0.5250 - top1_accuracy: 0.8218 - val_loss: 0.5241 - val_top1_accuracy: 0.8218 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.3753 - top1_accuracy: 0.8711\n",
      "Epoch 34: val_top1_accuracy improved from 0.83220 to 0.85100, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.3753 - top1_accuracy: 0.8711 - val_loss: 0.4517 - val_top1_accuracy: 0.8510 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.3076 - top1_accuracy: 0.8902\n",
      "Epoch 35: val_top1_accuracy improved from 0.85100 to 0.85180, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.3076 - top1_accuracy: 0.8902 - val_loss: 0.4499 - val_top1_accuracy: 0.8518 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.2823 - top1_accuracy: 0.9002\n",
      "Epoch 36: val_top1_accuracy did not improve from 0.85180\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.2823 - top1_accuracy: 0.9002 - val_loss: 0.4813 - val_top1_accuracy: 0.8446 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.2639 - top1_accuracy: 0.9058\n",
      "Epoch 37: val_top1_accuracy improved from 0.85180 to 0.85420, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.2639 - top1_accuracy: 0.9058 - val_loss: 0.4556 - val_top1_accuracy: 0.8542 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.2441 - top1_accuracy: 0.9135\n",
      "Epoch 38: val_top1_accuracy did not improve from 0.85420\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.2441 - top1_accuracy: 0.9135 - val_loss: 0.4468 - val_top1_accuracy: 0.8540 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.2416 - top1_accuracy: 0.9148\n",
      "Epoch 39: val_top1_accuracy did not improve from 0.85420\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.2416 - top1_accuracy: 0.9148 - val_loss: 0.4888 - val_top1_accuracy: 0.8504 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.2241 - top1_accuracy: 0.9210\n",
      "Epoch 40: val_top1_accuracy did not improve from 0.85420\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.2241 - top1_accuracy: 0.9210 - val_loss: 0.4804 - val_top1_accuracy: 0.8522 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.2144 - top1_accuracy: 0.9251\n",
      "Epoch 41: val_top1_accuracy improved from 0.85420 to 0.85980, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 0.2144 - top1_accuracy: 0.9251 - val_loss: 0.4584 - val_top1_accuracy: 0.8598 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.2116 - top1_accuracy: 0.9244\n",
      "Epoch 42: val_top1_accuracy did not improve from 0.85980\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.2117 - top1_accuracy: 0.9244 - val_loss: 0.4691 - val_top1_accuracy: 0.8586 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.2050 - top1_accuracy: 0.9273\n",
      "Epoch 43: val_top1_accuracy did not improve from 0.85980\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.2051 - top1_accuracy: 0.9273 - val_loss: 0.5384 - val_top1_accuracy: 0.8472 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.1975 - top1_accuracy: 0.9320\n",
      "Epoch 44: val_top1_accuracy did not improve from 0.85980\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 117ms/step - loss: 0.1975 - top1_accuracy: 0.9320 - val_loss: 0.4710 - val_top1_accuracy: 0.8564 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.1942 - top1_accuracy: 0.9313\n",
      "Epoch 45: val_top1_accuracy did not improve from 0.85980\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 117ms/step - loss: 0.1942 - top1_accuracy: 0.9313 - val_loss: 0.4760 - val_top1_accuracy: 0.8570 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.1771 - top1_accuracy: 0.9378\n",
      "Epoch 46: val_top1_accuracy did not improve from 0.85980\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 117ms/step - loss: 0.1771 - top1_accuracy: 0.9378 - val_loss: 0.5042 - val_top1_accuracy: 0.8538 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.1544 - top1_accuracy: 0.9464\n",
      "Epoch 47: val_top1_accuracy improved from 0.85980 to 0.86420, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 0.1544 - top1_accuracy: 0.9464 - val_loss: 0.4832 - val_top1_accuracy: 0.8642 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.1402 - top1_accuracy: 0.9510\n",
      "Epoch 48: val_top1_accuracy did not improve from 0.86420\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 117ms/step - loss: 0.1402 - top1_accuracy: 0.9510 - val_loss: 0.4969 - val_top1_accuracy: 0.8626 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.1242 - top1_accuracy: 0.9583\n",
      "Epoch 49: val_top1_accuracy did not improve from 0.86420\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - loss: 0.1242 - top1_accuracy: 0.9583 - val_loss: 0.5033 - val_top1_accuracy: 0.8626 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.1240 - top1_accuracy: 0.9570\n",
      "Epoch 50: val_top1_accuracy improved from 0.86420 to 0.86440, saving model to ./checkpoints/attention92_simple_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.1240 - top1_accuracy: 0.9570 - val_loss: 0.5008 - val_top1_accuracy: 0.8644 - learning_rate: 2.5000e-04\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "metrics_names: ['loss', 'compile_metrics']\n",
      "results: [0.5331857204437256, 0.8586999773979187]\n",
      "Final Test Accuracy: 85.87%\n",
      "Test Top-1 Error: 14.13000226020813\n",
      "Test Top-5 Error: 0.59999824\n",
      "============================================================\n",
      "Final Test Loss: 0.5332\n",
      "============================================================\n",
      "Training Top-1 Error: 4.371112585067749\n",
      "Validation Top-1 Error: 13.559997081756592\n",
      "\n",
      "Best Training Accuracy: 95.63%\n",
      "Best Validation Accuracy: 86.44%\n",
      "Final Test Accuracy: 85.87%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/attention92_simple_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python \"train_cifar_new 12.14.py\" \\\n",
    "  --model attention92_simple\\\n",
    "  --att_type arl \\\n",
    "  --epochs 50 \\\n",
    "  --batch-size 128 \\\n",
    "  --lr 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1765858188627,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "NTirRdLuYrFR",
    "outputId": "d51a8d90-7447-427d-f927-af03a9f0ed35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 4, done.\u001b[K\n",
      "remote: Counting objects:  25% (1/4)\u001b[K\rremote: Counting objects:  50% (2/4)\u001b[K\rremote: Counting objects:  75% (3/4)\u001b[K\rremote: Counting objects: 100% (4/4)\u001b[K\rremote: Counting objects: 100% (4/4), done.\u001b[K\n",
      "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 3 (delta 1), reused 3 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 3.15 KiB | 3.15 MiB/s, done.\n",
      "From https://github.com/ecbme4040/e4040-fall2025-project-swye\n",
      "   104402a..c669f5b  main       -> origin/main\n",
      "Updating 104402a..c669f5b\n",
      "Fast-forward\n",
      " .Rhistory                |   0\n",
      " README .md               | 134 \u001b[32m++++++++++\u001b[m\n",
      " README-proj.md           |   1 \u001b[31m-\u001b[m\n",
      " README.md                |  42 \u001b[31m---\u001b[m\n",
      " README_TF.md             | 225 \u001b[31m----------------\u001b[m\n",
      " train_cifar_100.py       | 241 \u001b[31m-----------------\u001b[m\n",
      " train_cifar_new 12.13.py | 246 \u001b[31m------------------\u001b[m\n",
      " train_cifar_new.py       | 286 \u001b[31m---------------------\u001b[m\n",
      " train_cifar_tf.py        | 653 \u001b[31m-----------------------------------------------\u001b[m\n",
      " 9 files changed, 134 insertions(+), 1694 deletions(-)\n",
      " create mode 100644 .Rhistory\n",
      " create mode 100644 README .md\n",
      " delete mode 100644 README-proj.md\n",
      " delete mode 100644 README.md\n",
      " delete mode 100644 README_TF.md\n",
      " delete mode 100644 train_cifar_100.py\n",
      " delete mode 100644 train_cifar_new 12.13.py\n",
      " delete mode 100644 train_cifar_new.py\n",
      " delete mode 100644 train_cifar_tf.py\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn-0o9CUENGz"
   },
   "source": [
    "-- due to noise are are failed to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1765858562038,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "tzlKyD4YYtdb",
    "outputId": "98d43345-518a-4932-f319-efbccb7eaa28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add/rm <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mdeleted:    __init__.py\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1765858661255,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "QuDIImpOP3xA"
   },
   "outputs": [],
   "source": [
    "!git ls-files | grep demo_code_4.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1765858681587,
     "user": {
      "displayName": "IvyYu e",
      "userId": "06001008946499759185"
     },
     "user_tz": 300
    },
    "id": "8v63HHKeYNKR",
    "outputId": "e7693df5-f0b3-4369-b878-d0b81c4f7b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add/rm <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mdeleted:    __init__.py\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status --untracked-files=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QDsFNIzYSH4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO/vAh3vYni5QMJAMjibwhs",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
