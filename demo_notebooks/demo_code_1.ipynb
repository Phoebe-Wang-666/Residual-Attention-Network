{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in this Notebook:\n",
    "- simple version Attention164 nal\n",
    "- Robustness examination on noise-label data of ResNet92 and Attention92\n",
    "- Attention92 arl & nal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run simple version attention164 with naive attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T02:22:22.943175Z",
     "iopub.status.busy": "2025-12-15T02:22:22.942565Z",
     "iopub.status.idle": "2025-12-15T03:03:23.999094Z",
     "shell.execute_reply": "2025-12-15T03:03:23.998363Z",
     "shell.execute_reply.started": "2025-12-15T02:22:22.943140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 02:22:23.307453: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765765343.326840    1290 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765765343.332709    1290 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention164\n",
      "Epochs: 50\n",
      "Batch size: 128\n",
      "Learning rate: 0.0001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 45000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "\n",
      "Building attention164 model...\n",
      "I0000 00:00:1765765352.448430    1290 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1765765352.449061    1290 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "I0000 00:00:1765765360.557090    1290 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001b[1mModel: \"residual_attention_model164\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module                │ ?                      │     \u001b[32m1,133,056\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_1              │ ?                      │     \u001b[32m4,204,544\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ attention_module_2              │ ?                      │    \u001b[32m13,371,392\u001b[0m │\n",
      "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m6,037,504\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_142         │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m28,222,666\u001b[0m (107.66 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m28,138,698\u001b[0m (107.34 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m83,968\u001b[0m (328.00 KB)\n",
      "\n",
      "Total parameters: 28,222,666\n",
      "Epoch 1/50\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765765424.173181    1317 service.cc:148] XLA service 0x7e812c002e70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765765424.173255    1317 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1765765424.173270    1317 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1765765467.269482    1317 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - loss: 2.3638 - top1_accuracy: 0.1555\n",
      "Epoch 1: val_top1_accuracy improved from -inf to 0.23260, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 283ms/step - loss: 2.3635 - top1_accuracy: 0.1556 - val_loss: 2.2185 - val_top1_accuracy: 0.2326 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 2.0651 - top1_accuracy: 0.2357\n",
      "Epoch 2: val_top1_accuracy improved from 0.23260 to 0.28320, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 132ms/step - loss: 2.0650 - top1_accuracy: 0.2357 - val_loss: 1.9434 - val_top1_accuracy: 0.2832 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.9351 - top1_accuracy: 0.2830\n",
      "Epoch 3: val_top1_accuracy improved from 0.28320 to 0.31180, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 130ms/step - loss: 1.9350 - top1_accuracy: 0.2830 - val_loss: 1.8955 - val_top1_accuracy: 0.3118 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 1.8602 - top1_accuracy: 0.3075\n",
      "Epoch 4: val_top1_accuracy improved from 0.31180 to 0.34780, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 131ms/step - loss: 1.8602 - top1_accuracy: 0.3075 - val_loss: 1.7860 - val_top1_accuracy: 0.3478 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1.8130 - top1_accuracy: 0.3304\n",
      "Epoch 5: val_top1_accuracy improved from 0.34780 to 0.36820, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 130ms/step - loss: 1.8129 - top1_accuracy: 0.3304 - val_loss: 1.7369 - val_top1_accuracy: 0.3682 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1.7617 - top1_accuracy: 0.3456\n",
      "Epoch 6: val_top1_accuracy improved from 0.36820 to 0.37620, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 131ms/step - loss: 1.7617 - top1_accuracy: 0.3456 - val_loss: 1.6989 - val_top1_accuracy: 0.3762 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 1.7242 - top1_accuracy: 0.3590\n",
      "Epoch 7: val_top1_accuracy improved from 0.37620 to 0.39020, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 131ms/step - loss: 1.7242 - top1_accuracy: 0.3590 - val_loss: 1.7101 - val_top1_accuracy: 0.3902 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1.6912 - top1_accuracy: 0.3763\n",
      "Epoch 8: val_top1_accuracy improved from 0.39020 to 0.41220, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 131ms/step - loss: 1.6912 - top1_accuracy: 0.3763 - val_loss: 1.6474 - val_top1_accuracy: 0.4122 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 1.6549 - top1_accuracy: 0.3893\n",
      "Epoch 9: val_top1_accuracy improved from 0.41220 to 0.43780, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 132ms/step - loss: 1.6549 - top1_accuracy: 0.3893 - val_loss: 1.5500 - val_top1_accuracy: 0.4378 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 1.6263 - top1_accuracy: 0.3986\n",
      "Epoch 10: val_top1_accuracy improved from 0.43780 to 0.44560, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 131ms/step - loss: 1.6263 - top1_accuracy: 0.3986 - val_loss: 1.5281 - val_top1_accuracy: 0.4456 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1.5847 - top1_accuracy: 0.4150\n",
      "Epoch 11: val_top1_accuracy improved from 0.44560 to 0.45660, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 130ms/step - loss: 1.5847 - top1_accuracy: 0.4151 - val_loss: 1.5163 - val_top1_accuracy: 0.4566 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1.5589 - top1_accuracy: 0.4288\n",
      "Epoch 12: val_top1_accuracy improved from 0.45660 to 0.46180, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 130ms/step - loss: 1.5589 - top1_accuracy: 0.4288 - val_loss: 1.5301 - val_top1_accuracy: 0.4618 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.5225 - top1_accuracy: 0.4389\n",
      "Epoch 13: val_top1_accuracy improved from 0.46180 to 0.48200, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.5225 - top1_accuracy: 0.4389 - val_loss: 1.4683 - val_top1_accuracy: 0.4820 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.4952 - top1_accuracy: 0.4536\n",
      "Epoch 14: val_top1_accuracy improved from 0.48200 to 0.48940, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.4952 - top1_accuracy: 0.4536 - val_loss: 1.4606 - val_top1_accuracy: 0.4894 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 1.4577 - top1_accuracy: 0.4676\n",
      "Epoch 15: val_top1_accuracy improved from 0.48940 to 0.49240, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.4577 - top1_accuracy: 0.4676 - val_loss: 1.4350 - val_top1_accuracy: 0.4924 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.4463 - top1_accuracy: 0.4746\n",
      "Epoch 16: val_top1_accuracy improved from 0.49240 to 0.51180, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.4463 - top1_accuracy: 0.4746 - val_loss: 1.3855 - val_top1_accuracy: 0.5118 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.4143 - top1_accuracy: 0.4840\n",
      "Epoch 17: val_top1_accuracy improved from 0.51180 to 0.51860, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.4143 - top1_accuracy: 0.4840 - val_loss: 1.3450 - val_top1_accuracy: 0.5186 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.3720 - top1_accuracy: 0.5022\n",
      "Epoch 18: val_top1_accuracy improved from 0.51860 to 0.52820, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.3720 - top1_accuracy: 0.5022 - val_loss: 1.3547 - val_top1_accuracy: 0.5282 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.3513 - top1_accuracy: 0.5085\n",
      "Epoch 19: val_top1_accuracy improved from 0.52820 to 0.54660, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.3513 - top1_accuracy: 0.5085 - val_loss: 1.2895 - val_top1_accuracy: 0.5466 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.3147 - top1_accuracy: 0.5229\n",
      "Epoch 20: val_top1_accuracy did not improve from 0.54660\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 1.3147 - top1_accuracy: 0.5229 - val_loss: 1.2955 - val_top1_accuracy: 0.5386 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.2919 - top1_accuracy: 0.5303\n",
      "Epoch 21: val_top1_accuracy improved from 0.54660 to 0.55880, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.2919 - top1_accuracy: 0.5303 - val_loss: 1.3010 - val_top1_accuracy: 0.5588 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.2661 - top1_accuracy: 0.5397\n",
      "Epoch 22: val_top1_accuracy improved from 0.55880 to 0.56800, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 126ms/step - loss: 1.2661 - top1_accuracy: 0.5397 - val_loss: 1.2009 - val_top1_accuracy: 0.5680 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.2347 - top1_accuracy: 0.5525\n",
      "Epoch 23: val_top1_accuracy improved from 0.56800 to 0.58100, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.2347 - top1_accuracy: 0.5525 - val_loss: 1.1642 - val_top1_accuracy: 0.5810 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.2065 - top1_accuracy: 0.5628\n",
      "Epoch 24: val_top1_accuracy improved from 0.58100 to 0.58800, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.2065 - top1_accuracy: 0.5628 - val_loss: 1.1701 - val_top1_accuracy: 0.5880 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.1779 - top1_accuracy: 0.5747\n",
      "Epoch 25: val_top1_accuracy improved from 0.58800 to 0.59780, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.1779 - top1_accuracy: 0.5747 - val_loss: 1.1419 - val_top1_accuracy: 0.5978 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 1.1368 - top1_accuracy: 0.5909\n",
      "Epoch 26: val_top1_accuracy did not improve from 0.59780\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 1.1369 - top1_accuracy: 0.5909 - val_loss: 1.1499 - val_top1_accuracy: 0.5950 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.1126 - top1_accuracy: 0.5994\n",
      "Epoch 27: val_top1_accuracy improved from 0.59780 to 0.61120, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.1126 - top1_accuracy: 0.5994 - val_loss: 1.0985 - val_top1_accuracy: 0.6112 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.0987 - top1_accuracy: 0.6059\n",
      "Epoch 28: val_top1_accuracy improved from 0.61120 to 0.61800, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.0987 - top1_accuracy: 0.6059 - val_loss: 1.0856 - val_top1_accuracy: 0.6180 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.0642 - top1_accuracy: 0.6197\n",
      "Epoch 29: val_top1_accuracy improved from 0.61800 to 0.62160, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 129ms/step - loss: 1.0642 - top1_accuracy: 0.6196 - val_loss: 1.0520 - val_top1_accuracy: 0.6216 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.0478 - top1_accuracy: 0.6230\n",
      "Epoch 30: val_top1_accuracy improved from 0.62160 to 0.63820, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 129ms/step - loss: 1.0478 - top1_accuracy: 0.6230 - val_loss: 1.0376 - val_top1_accuracy: 0.6382 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1.0150 - top1_accuracy: 0.6357\n",
      "Epoch 31: val_top1_accuracy improved from 0.63820 to 0.64240, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 1.0150 - top1_accuracy: 0.6357 - val_loss: 1.0603 - val_top1_accuracy: 0.6424 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.9981 - top1_accuracy: 0.6400\n",
      "Epoch 32: val_top1_accuracy improved from 0.64240 to 0.65240, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 0.9981 - top1_accuracy: 0.6400 - val_loss: 0.9838 - val_top1_accuracy: 0.6524 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.9786 - top1_accuracy: 0.6513\n",
      "Epoch 33: val_top1_accuracy improved from 0.65240 to 0.65680, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 0.9786 - top1_accuracy: 0.6513 - val_loss: 0.9783 - val_top1_accuracy: 0.6568 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.9488 - top1_accuracy: 0.6592\n",
      "Epoch 34: val_top1_accuracy did not improve from 0.65680\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - loss: 0.9488 - top1_accuracy: 0.6592 - val_loss: 0.9990 - val_top1_accuracy: 0.6526 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.9213 - top1_accuracy: 0.6685\n",
      "Epoch 35: val_top1_accuracy improved from 0.65680 to 0.66040, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 0.9213 - top1_accuracy: 0.6685 - val_loss: 0.9939 - val_top1_accuracy: 0.6604 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.8881 - top1_accuracy: 0.6779\n",
      "Epoch 36: val_top1_accuracy improved from 0.66040 to 0.67600, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 0.8881 - top1_accuracy: 0.6779 - val_loss: 0.9200 - val_top1_accuracy: 0.6760 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.8631 - top1_accuracy: 0.6923\n",
      "Epoch 37: val_top1_accuracy did not improve from 0.67600\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 0.8631 - top1_accuracy: 0.6923 - val_loss: 0.9169 - val_top1_accuracy: 0.6760 - learning_rate: 1.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.8526 - top1_accuracy: 0.6914\n",
      "Epoch 38: val_top1_accuracy did not improve from 0.67600\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 0.8526 - top1_accuracy: 0.6914 - val_loss: 1.3278 - val_top1_accuracy: 0.6684 - learning_rate: 1.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.8237 - top1_accuracy: 0.7077\n",
      "Epoch 39: val_top1_accuracy improved from 0.67600 to 0.68820, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 0.8237 - top1_accuracy: 0.7077 - val_loss: 0.8748 - val_top1_accuracy: 0.6882 - learning_rate: 1.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.8020 - top1_accuracy: 0.7128\n",
      "Epoch 40: val_top1_accuracy improved from 0.68820 to 0.69560, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 0.8021 - top1_accuracy: 0.7128 - val_loss: 0.8588 - val_top1_accuracy: 0.6956 - learning_rate: 1.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.7868 - top1_accuracy: 0.7201\n",
      "Epoch 41: val_top1_accuracy did not improve from 0.69560\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 0.7868 - top1_accuracy: 0.7201 - val_loss: 0.8881 - val_top1_accuracy: 0.6864 - learning_rate: 1.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.7615 - top1_accuracy: 0.7273\n",
      "Epoch 42: val_top1_accuracy improved from 0.69560 to 0.70080, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 0.7615 - top1_accuracy: 0.7273 - val_loss: 0.8540 - val_top1_accuracy: 0.7008 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.7428 - top1_accuracy: 0.7334\n",
      "Epoch 43: val_top1_accuracy improved from 0.70080 to 0.70260, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 129ms/step - loss: 0.7428 - top1_accuracy: 0.7334 - val_loss: 0.8550 - val_top1_accuracy: 0.7026 - learning_rate: 1.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.7194 - top1_accuracy: 0.7415\n",
      "Epoch 44: val_top1_accuracy did not improve from 0.70260\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 125ms/step - loss: 0.7194 - top1_accuracy: 0.7415 - val_loss: 0.8559 - val_top1_accuracy: 0.7004 - learning_rate: 1.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.7073 - top1_accuracy: 0.7456\n",
      "Epoch 45: val_top1_accuracy improved from 0.70260 to 0.70440, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 129ms/step - loss: 0.7073 - top1_accuracy: 0.7456 - val_loss: 0.8561 - val_top1_accuracy: 0.7044 - learning_rate: 1.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.6854 - top1_accuracy: 0.7571\n",
      "Epoch 46: val_top1_accuracy improved from 0.70440 to 0.71860, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 129ms/step - loss: 0.6854 - top1_accuracy: 0.7571 - val_loss: 0.8228 - val_top1_accuracy: 0.7186 - learning_rate: 1.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.6682 - top1_accuracy: 0.7609\n",
      "Epoch 47: val_top1_accuracy improved from 0.71860 to 0.72400, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 128ms/step - loss: 0.6682 - top1_accuracy: 0.7609 - val_loss: 0.8089 - val_top1_accuracy: 0.7240 - learning_rate: 1.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.6670 - top1_accuracy: 0.7584\n",
      "Epoch 48: val_top1_accuracy did not improve from 0.72400\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 0.6670 - top1_accuracy: 0.7584 - val_loss: 0.8631 - val_top1_accuracy: 0.7140 - learning_rate: 1.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.6247 - top1_accuracy: 0.7769\n",
      "Epoch 49: val_top1_accuracy improved from 0.72400 to 0.72620, saving model to ./checkpoints/attention164_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - loss: 0.6248 - top1_accuracy: 0.7769 - val_loss: 0.8294 - val_top1_accuracy: 0.7262 - learning_rate: 1.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.6159 - top1_accuracy: 0.7769\n",
      "Epoch 50: val_top1_accuracy did not improve from 0.72620\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 0.6159 - top1_accuracy: 0.7769 - val_loss: 0.8324 - val_top1_accuracy: 0.7200 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 49.\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "metrics_names: ['loss', 'compile_metrics']\n",
      "results: [0.8610534071922302, 0.7138000130653381]\n",
      "Final Test Accuracy: 71.38%\n",
      "Test Top-1 Error: 28.619998693466187\n",
      "Test Top-5 Error: 2.2899985313415527\n",
      "============================================================\n",
      "Final Test Loss: 0.8611\n",
      "============================================================\n",
      "Training Top-1 Error: 22.673332691192627\n",
      "Validation Top-1 Error: 27.380001544952393\n",
      "\n",
      "Best Training Accuracy: 77.33%\n",
      "Best Validation Accuracy: 72.62%\n",
      "Final Test Accuracy: 71.38%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/attention164_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python \"/kaggle/input/ecbm4040-final-project/e4040-fall2025-project-swye/train_cifar_new 12.14.py\" \\\n",
    "  --model attention164_simple \\\n",
    "  --att_type nal \\\n",
    "  --epochs 50 \\\n",
    "  --batch-size 128 \\\n",
    "  --lr 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run attention92 on noisy-label data for robustness\n",
    "(here done with three noise labels:0.1, 0.3, 0.5)\n",
    "\n",
    "**This reult do not show in report since the setup is different**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-15T16:53:46.099Z",
     "iopub.execute_input": "2025-12-15T06:01:24.024990Z",
     "iopub.status.busy": "2025-12-15T06:01:24.024043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 06:01:24.615001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765778484.637453     255 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765778484.644446     255 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      ">>> script started\n",
      "train_n = 20000 batch_size = 128 steps/epoch = 200 epochs = 20 => max_iters = 4000\n",
      "I0000 00:00:1765778493.784566     255 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1765778493.785368     255 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "I0000 00:00:1765778510.884126     255 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "[attention92] noise=0.10 step=1000/4000 lr=0.10000 train_acc(ma)=0.0976 val_acc=0.1031 best=0.1031\n",
      "[attention92] noise=0.10 step=2000/4000 lr=0.10000 train_acc(ma)=0.0947 val_acc=0.1029 best=0.1031\n",
      "[attention92] noise=0.10 step=3000/4000 lr=0.10000 train_acc(ma)=0.0997 val_acc=0.1027 best=0.1031\n",
      "[attention92] noise=0.10 step=4000/4000 lr=0.10000 train_acc(ma)=0.0977 val_acc=0.1029 best=0.1031\n",
      "[attention92] noise=0.30 step=1000/4000 lr=0.10000 train_acc(ma)=0.0952 val_acc=0.0771 best=0.0771\n",
      "[attention92] noise=0.30 step=2000/4000 lr=0.10000 train_acc(ma)=0.0969 val_acc=0.0756 best=0.0771\n",
      "[attention92] noise=0.30 step=3000/4000 lr=0.10000 train_acc(ma)=0.0933 val_acc=0.0719 best=0.0771\n",
      "[attention92] noise=0.30 step=4000/4000 lr=0.10000 train_acc(ma)=0.0975 val_acc=0.0801 best=0.0801\n",
      "[attention92] noise=0.50 step=1000/4000 lr=0.10000 train_acc(ma)=0.0960 val_acc=0.1041 best=0.1041\n",
      "[attention92] noise=0.50 step=2000/4000 lr=0.10000 train_acc(ma)=0.0992 val_acc=0.1041 best=0.1041\n",
      "[attention92] noise=0.50 step=3000/4000 lr=0.10000 train_acc(ma)=0.0985 val_acc=0.1041 best=0.1041\n",
      "[attention92] noise=0.50 step=4000/4000 lr=0.10000 train_acc(ma)=0.0996 val_acc=0.1041 best=0.1041\n",
      "[attention92] noise=0.70 step=1000/4000 lr=0.10000 train_acc(ma)=0.0958 val_acc=0.0947 best=0.0947\n",
      "[attention92] noise=0.70 step=2000/4000 lr=0.10000 train_acc(ma)=0.0960 val_acc=0.0947 best=0.0947\n"
     ]
    }
   ],
   "source": [
    "!python \"/kaggle/input/ecbm4040-v5/e4040-fall2025-project-swye/train_cifar10_robustness.py\" \\\n",
    "  --model attention92 \\\n",
    "  --att_type arl \\\n",
    "  --noise_levels 0.1 0.3 0.5 0.7 \\\n",
    "  --epochs 20 \\\n",
    "  --train_subset 20000 \\\n",
    "  --steps_per_epoch 200 \\\n",
    "  --batch_size 128 \\\n",
    "  --outdir \"/kaggle/working/robustness_runs_fast\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ResNet92 on noise-label data for robustness\n",
    "(Run all noise levels: 0.1, 0.3, 0.5, 0.7, but only use 0.1 and 0.3 in the report since the test for Attention92 only done 0.1 and 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T20:32:06.645994Z",
     "iopub.status.busy": "2025-12-15T20:32:06.645673Z",
     "iopub.status.idle": "2025-12-15T22:10:27.589494Z",
     "shell.execute_reply": "2025-12-15T22:10:27.588768Z",
     "shell.execute_reply.started": "2025-12-15T20:32:06.645967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 20:32:07.176332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765830727.196173    3209 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765830727.202242    3209 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      ">>> script started\n",
      "train_n = 20000 batch_size = 64 steps/epoch = 100 epochs = 20 => max_iters = 2000\n",
      "I0000 00:00:1765830734.991843    3209 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "I0000 00:00:1765830742.717582    3209 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "[resnet92] noise=0.10 step=1000/2000 lr=0.10000 train_acc(ma)=0.0762 val_acc=0.0635 best=0.0635\n",
      "[resnet92] noise=0.10 step=2000/2000 lr=0.10000 train_acc(ma)=0.0335 val_acc=0.0216 best=0.0635\n",
      "[resnet92] noise=0.30 step=1000/2000 lr=0.10000 train_acc(ma)=0.1034 val_acc=0.0959 best=0.0959\n",
      "[resnet92] noise=0.30 step=2000/2000 lr=0.10000 train_acc(ma)=0.0964 val_acc=0.0959 best=0.0959\n",
      "[resnet92] noise=0.50 step=1000/2000 lr=0.10000 train_acc(ma)=0.1034 val_acc=0.1125 best=0.1125\n",
      "[resnet92] noise=0.50 step=2000/2000 lr=0.10000 train_acc(ma)=0.1035 val_acc=0.1189 best=0.1189\n",
      "[resnet92] noise=0.70 step=1000/2000 lr=0.10000 train_acc(ma)=0.0960 val_acc=0.1027 best=0.1027\n",
      "[resnet92] noise=0.70 step=2000/2000 lr=0.10000 train_acc(ma)=0.0975 val_acc=0.1027 best=0.1027\n",
      "\n",
      "Noise        10%    30%    50%    70%\n",
      "Test err  93.70% 89.97% 88.53% 90.05%\n",
      "Test acc   6.30% 10.03% 11.47%  9.95%\n",
      "\n",
      "Saved results to robustness_results.json\n"
     ]
    }
   ],
   "source": [
    "!python \"/kaggle/input/ecbm4040-v8/e4040-fall2025-project-swye/train_cifar10_robustness.py\" \\\n",
    "  --model resnet92 \\\n",
    "  --noise_levels 0.1 0.3 0.5 0.7 \\\n",
    "  --epochs 20 \\\n",
    "  --train_subset 20000 \\\n",
    "  --steps_per_epoch 100 \\\n",
    "  --batch_size 64 \\\n",
    "  --outdir \"/kaggle/working/robustness_runs_fast\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Attention92 on noise-label data for robustness\n",
    "(only done 0.1 and 0.3 here, too slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T22:12:30.427364Z",
     "iopub.status.busy": "2025-12-15T22:12:30.426706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 22:12:30.944203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765836750.965390    3664 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765836750.972003    3664 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      ">>> script started\n",
      "train_n = 20000 batch_size = 64 steps/epoch = 100 epochs = 20 => max_iters = 2000\n",
      "I0000 00:00:1765836758.658001    3664 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "I0000 00:00:1765836782.314187    3664 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "[attention92] noise=0.10 step=1000/2000 lr=0.10000 train_acc(ma)=0.0994 val_acc=0.1003 best=0.1003\n",
      "[attention92] noise=0.10 step=2000/2000 lr=0.10000 train_acc(ma)=0.0934 val_acc=0.1003 best=0.1003\n",
      "[attention92] noise=0.30 step=1000/2000 lr=0.10000 train_acc(ma)=0.1011 val_acc=0.1027 best=0.1027\n",
      "[attention92] noise=0.30 step=2000/2000 lr=0.10000 train_acc(ma)=0.1001 val_acc=0.1027 best=0.1027\n"
     ]
    }
   ],
   "source": [
    "!python \"/kaggle/input/ecbm4040-v8/e4040-fall2025-project-swye/train_cifar10_robustness.py\" \\\n",
    "  --model attention92 \\\n",
    "  --noise_levels 0.1 0.3 0.5 0.7 \\\n",
    "  --epochs 20 \\\n",
    "  --train_subset 20000 \\\n",
    "  --steps_per_epoch 100 \\\n",
    "  --batch_size 64 \\\n",
    "  --outdir \"/kaggle/working/robustness_runs_fast\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Attention92 arl on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T18:11:24.141172Z",
     "iopub.status.busy": "2025-12-15T18:11:24.140241Z",
     "iopub.status.idle": "2025-12-15T19:02:55.681956Z",
     "shell.execute_reply": "2025-12-15T19:02:55.681042Z",
     "shell.execute_reply.started": "2025-12-15T18:11:24.141141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 18:11:24.527130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765822284.546718     631 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765822284.552561     631 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention92\n",
      "Epochs: 50\n",
      "Batch size: 128\n",
      "Learning rate: 0.001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 45000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "\n",
      "Building attention92 model...\n",
      "I0000 00:00:1765822293.436375     631 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "I0000 00:00:1765822308.851646     631 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001b[1mModel: \"residual_attention_model92\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ att1_x2 (\u001b[94mSequential\u001b[0m)            │ (\u001b[32m1\u001b[0m, \u001b[32m8\u001b[0m, \u001b[32m8\u001b[0m, \u001b[32m256\u001b[0m)         │     \u001b[32m2,123,776\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ att1_x2 (\u001b[94mSequential\u001b[0m)            │ (\u001b[32m1\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m512\u001b[0m)         │     \u001b[32m6,156,288\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ att1_x2 (\u001b[94mSequential\u001b[0m)            │ (\u001b[32m1\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m1024\u001b[0m)        │    \u001b[32m24,502,272\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │    \u001b[32m14,974,976\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_232         │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m51,233,482\u001b[0m (195.44 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m51,090,378\u001b[0m (194.89 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m143,104\u001b[0m (559.00 KB)\n",
      "\n",
      "Total parameters: 51,233,482\n",
      "Epoch 1/50\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765822409.787124     653 service.cc:148] XLA service 0x7823bc09efb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765822409.787177     653 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1765822476.514452     653 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - loss: 2.0446 - top1_accuracy: 0.2803\n",
      "Epoch 1: val_top1_accuracy improved from -inf to 0.41820, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 387ms/step - loss: 2.0440 - top1_accuracy: 0.2805 - val_loss: 2.0946 - val_top1_accuracy: 0.4182 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 1.5850 - top1_accuracy: 0.4486\n",
      "Epoch 2: val_top1_accuracy did not improve from 0.41820\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 157ms/step - loss: 1.5849 - top1_accuracy: 0.4486 - val_loss: 23.2303 - val_top1_accuracy: 0.3344 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.6636 - top1_accuracy: 0.4246\n",
      "Epoch 3: val_top1_accuracy did not improve from 0.41820\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - loss: 1.6638 - top1_accuracy: 0.4245 - val_loss: 401095.0938 - val_top1_accuracy: 0.1024 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 1.6552 - top1_accuracy: 0.4003\n",
      "Epoch 4: val_top1_accuracy improved from 0.41820 to 0.51500, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 161ms/step - loss: 1.6549 - top1_accuracy: 0.4004 - val_loss: 1.3261 - val_top1_accuracy: 0.5150 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 1.3490 - top1_accuracy: 0.5185\n",
      "Epoch 5: val_top1_accuracy improved from 0.51500 to 0.57260, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - loss: 1.3490 - top1_accuracy: 0.5185 - val_loss: 1.1986 - val_top1_accuracy: 0.5726 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 1.2364 - top1_accuracy: 0.5591\n",
      "Epoch 6: val_top1_accuracy did not improve from 0.57260\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 152ms/step - loss: 1.2364 - top1_accuracy: 0.5591 - val_loss: 2.4515 - val_top1_accuracy: 0.5372 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 1.3542 - top1_accuracy: 0.5305\n",
      "Epoch 7: val_top1_accuracy did not improve from 0.57260\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 153ms/step - loss: 1.3542 - top1_accuracy: 0.5305 - val_loss: 1.2821 - val_top1_accuracy: 0.5354 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.3014 - top1_accuracy: 0.5506\n",
      "Epoch 8: val_top1_accuracy did not improve from 0.57260\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 1.3016 - top1_accuracy: 0.5506 - val_loss: 2.3801 - val_top1_accuracy: 0.4862 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 1.4711 - top1_accuracy: 0.4984\n",
      "Epoch 9: val_top1_accuracy did not improve from 0.57260\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - loss: 1.4713 - top1_accuracy: 0.4983 - val_loss: 289.6141 - val_top1_accuracy: 0.2278 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.5751 - top1_accuracy: 0.4597\n",
      "Epoch 10: val_top1_accuracy did not improve from 0.57260\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 1.5747 - top1_accuracy: 0.4598 - val_loss: 1.2495 - val_top1_accuracy: 0.5528 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 1.2297 - top1_accuracy: 0.5697\n",
      "Epoch 11: val_top1_accuracy improved from 0.57260 to 0.62660, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 1.2296 - top1_accuracy: 0.5697 - val_loss: 1.0412 - val_top1_accuracy: 0.6266 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.1157 - top1_accuracy: 0.6018\n",
      "Epoch 12: val_top1_accuracy improved from 0.62660 to 0.65820, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 163ms/step - loss: 1.1157 - top1_accuracy: 0.6018 - val_loss: 1.0607 - val_top1_accuracy: 0.6582 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.0641 - top1_accuracy: 0.6202\n",
      "Epoch 13: val_top1_accuracy did not improve from 0.65820\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 1.0642 - top1_accuracy: 0.6202 - val_loss: 5.5267 - val_top1_accuracy: 0.6102 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.1235 - top1_accuracy: 0.6134\n",
      "Epoch 14: val_top1_accuracy did not improve from 0.65820\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 1.1234 - top1_accuracy: 0.6134 - val_loss: 2.4750 - val_top1_accuracy: 0.6364 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 1.0798 - top1_accuracy: 0.6207\n",
      "Epoch 15: val_top1_accuracy improved from 0.65820 to 0.69040, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - loss: 1.0797 - top1_accuracy: 0.6207 - val_loss: 0.8689 - val_top1_accuracy: 0.6904 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.9754 - top1_accuracy: 0.6567\n",
      "Epoch 16: val_top1_accuracy did not improve from 0.69040\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 154ms/step - loss: 0.9754 - top1_accuracy: 0.6568 - val_loss: 0.8826 - val_top1_accuracy: 0.6862 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.9100 - top1_accuracy: 0.6755\n",
      "Epoch 17: val_top1_accuracy improved from 0.69040 to 0.70900, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 162ms/step - loss: 0.9100 - top1_accuracy: 0.6756 - val_loss: 0.8773 - val_top1_accuracy: 0.7090 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.8544 - top1_accuracy: 0.6988\n",
      "Epoch 18: val_top1_accuracy improved from 0.70900 to 0.72020, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 162ms/step - loss: 0.8544 - top1_accuracy: 0.6988 - val_loss: 0.9802 - val_top1_accuracy: 0.7202 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.8230 - top1_accuracy: 0.7124\n",
      "Epoch 19: val_top1_accuracy did not improve from 0.72020\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 153ms/step - loss: 0.8230 - top1_accuracy: 0.7124 - val_loss: 0.8200 - val_top1_accuracy: 0.7190 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.8057 - top1_accuracy: 0.7175\n",
      "Epoch 20: val_top1_accuracy improved from 0.72020 to 0.73020, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - loss: 0.8057 - top1_accuracy: 0.7175 - val_loss: 0.7921 - val_top1_accuracy: 0.7302 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.7709 - top1_accuracy: 0.7302\n",
      "Epoch 21: val_top1_accuracy improved from 0.73020 to 0.74280, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - loss: 0.7709 - top1_accuracy: 0.7302 - val_loss: 0.7417 - val_top1_accuracy: 0.7428 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.7490 - top1_accuracy: 0.7374\n",
      "Epoch 22: val_top1_accuracy improved from 0.74280 to 0.75840, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 160ms/step - loss: 0.7490 - top1_accuracy: 0.7374 - val_loss: 0.6969 - val_top1_accuracy: 0.7584 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.6836 - top1_accuracy: 0.7557\n",
      "Epoch 23: val_top1_accuracy improved from 0.75840 to 0.76480, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - loss: 0.6836 - top1_accuracy: 0.7557 - val_loss: 0.6891 - val_top1_accuracy: 0.7648 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.6509 - top1_accuracy: 0.7718\n",
      "Epoch 24: val_top1_accuracy improved from 0.76480 to 0.77000, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - loss: 0.6509 - top1_accuracy: 0.7717 - val_loss: 0.6532 - val_top1_accuracy: 0.7700 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.6301 - top1_accuracy: 0.7786\n",
      "Epoch 25: val_top1_accuracy did not improve from 0.77000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 152ms/step - loss: 0.6302 - top1_accuracy: 0.7786 - val_loss: 0.7699 - val_top1_accuracy: 0.7330 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.6404 - top1_accuracy: 0.7735\n",
      "Epoch 26: val_top1_accuracy did not improve from 0.77000\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 151ms/step - loss: 0.6404 - top1_accuracy: 0.7735 - val_loss: 0.6777 - val_top1_accuracy: 0.7632 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.6020 - top1_accuracy: 0.7888\n",
      "Epoch 27: val_top1_accuracy improved from 0.77000 to 0.77600, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - loss: 0.6020 - top1_accuracy: 0.7888 - val_loss: 0.6877 - val_top1_accuracy: 0.7760 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.5603 - top1_accuracy: 0.8039\n",
      "Epoch 28: val_top1_accuracy improved from 0.77600 to 0.77860, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - loss: 0.5603 - top1_accuracy: 0.8038 - val_loss: 0.7075 - val_top1_accuracy: 0.7786 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.5420 - top1_accuracy: 0.8087\n",
      "Epoch 29: val_top1_accuracy improved from 0.77860 to 0.78900, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - loss: 0.5421 - top1_accuracy: 0.8087 - val_loss: 0.6214 - val_top1_accuracy: 0.7890 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.5221 - top1_accuracy: 0.8174\n",
      "Epoch 30: val_top1_accuracy did not improve from 0.78900\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 151ms/step - loss: 0.5221 - top1_accuracy: 0.8173 - val_loss: 0.6191 - val_top1_accuracy: 0.7862 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.5140 - top1_accuracy: 0.8187\n",
      "Epoch 31: val_top1_accuracy improved from 0.78900 to 0.79240, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - loss: 0.5140 - top1_accuracy: 0.8187 - val_loss: 0.5934 - val_top1_accuracy: 0.7924 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.4781 - top1_accuracy: 0.8324\n",
      "Epoch 32: val_top1_accuracy improved from 0.79240 to 0.79980, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - loss: 0.4782 - top1_accuracy: 0.8324 - val_loss: 0.5912 - val_top1_accuracy: 0.7998 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.4674 - top1_accuracy: 0.8330\n",
      "Epoch 33: val_top1_accuracy did not improve from 0.79980\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 152ms/step - loss: 0.4674 - top1_accuracy: 0.8330 - val_loss: 0.6009 - val_top1_accuracy: 0.7984 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.4359 - top1_accuracy: 0.8459\n",
      "Epoch 34: val_top1_accuracy improved from 0.79980 to 0.80360, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - loss: 0.4360 - top1_accuracy: 0.8459 - val_loss: 0.5930 - val_top1_accuracy: 0.8036 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.4165 - top1_accuracy: 0.8525\n",
      "Epoch 35: val_top1_accuracy did not improve from 0.80360\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 153ms/step - loss: 0.4165 - top1_accuracy: 0.8525 - val_loss: 0.6167 - val_top1_accuracy: 0.7976 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.4294 - top1_accuracy: 0.8483\n",
      "Epoch 36: val_top1_accuracy did not improve from 0.80360\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 152ms/step - loss: 0.4295 - top1_accuracy: 0.8482 - val_loss: 0.6215 - val_top1_accuracy: 0.8016 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.4199 - top1_accuracy: 0.8512\n",
      "Epoch 37: val_top1_accuracy improved from 0.80360 to 0.81540, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 161ms/step - loss: 0.4199 - top1_accuracy: 0.8512 - val_loss: 0.5763 - val_top1_accuracy: 0.8154 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.3878 - top1_accuracy: 0.8635\n",
      "Epoch 38: val_top1_accuracy did not improve from 0.81540\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 152ms/step - loss: 0.3878 - top1_accuracy: 0.8635 - val_loss: 0.5651 - val_top1_accuracy: 0.8146 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.3631 - top1_accuracy: 0.8717\n",
      "Epoch 39: val_top1_accuracy improved from 0.81540 to 0.81980, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 160ms/step - loss: 0.3631 - top1_accuracy: 0.8717 - val_loss: 0.5706 - val_top1_accuracy: 0.8198 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.3587 - top1_accuracy: 0.8733\n",
      "Epoch 40: val_top1_accuracy did not improve from 0.81980\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 151ms/step - loss: 0.3587 - top1_accuracy: 0.8733 - val_loss: 0.5536 - val_top1_accuracy: 0.8152 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.3467 - top1_accuracy: 0.8770\n",
      "Epoch 41: val_top1_accuracy did not improve from 0.81980\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 152ms/step - loss: 0.3467 - top1_accuracy: 0.8770 - val_loss: 0.6785 - val_top1_accuracy: 0.8012 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.4049 - top1_accuracy: 0.8590\n",
      "Epoch 42: val_top1_accuracy did not improve from 0.81980\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 153ms/step - loss: 0.4050 - top1_accuracy: 0.8589 - val_loss: 0.6319 - val_top1_accuracy: 0.8000 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.5258 - top1_accuracy: 0.8242\n",
      "Epoch 43: val_top1_accuracy did not improve from 0.81980\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 152ms/step - loss: 0.5258 - top1_accuracy: 0.8242 - val_loss: 0.6042 - val_top1_accuracy: 0.7990 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.3683 - top1_accuracy: 0.8730\n",
      "Epoch 44: val_top1_accuracy improved from 0.81980 to 0.83220, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 162ms/step - loss: 0.3683 - top1_accuracy: 0.8730 - val_loss: 0.5344 - val_top1_accuracy: 0.8322 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.3218 - top1_accuracy: 0.8867\n",
      "Epoch 45: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 153ms/step - loss: 0.3218 - top1_accuracy: 0.8867 - val_loss: 0.6067 - val_top1_accuracy: 0.8192 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.3169 - top1_accuracy: 0.8880\n",
      "Epoch 46: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 154ms/step - loss: 0.3169 - top1_accuracy: 0.8880 - val_loss: 0.5487 - val_top1_accuracy: 0.8284 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.3190 - top1_accuracy: 0.8888\n",
      "Epoch 47: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 153ms/step - loss: 0.3190 - top1_accuracy: 0.8888 - val_loss: 0.6035 - val_top1_accuracy: 0.8164 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.2790 - top1_accuracy: 0.9006\n",
      "Epoch 48: val_top1_accuracy did not improve from 0.83220\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 152ms/step - loss: 0.2790 - top1_accuracy: 0.9006 - val_loss: 0.5790 - val_top1_accuracy: 0.8266 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.2658 - top1_accuracy: 0.9063\n",
      "Epoch 49: val_top1_accuracy did not improve from 0.83220\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 153ms/step - loss: 0.2659 - top1_accuracy: 0.9063 - val_loss: 0.6222 - val_top1_accuracy: 0.8116 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 0.2286 - top1_accuracy: 0.9202\n",
      "Epoch 50: val_top1_accuracy improved from 0.83220 to 0.84240, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 161ms/step - loss: 0.2286 - top1_accuracy: 0.9202 - val_loss: 0.5429 - val_top1_accuracy: 0.8424 - learning_rate: 2.5000e-04\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "metrics_names: ['loss', 'compile_metrics']\n",
      "results: [0.5686068534851074, 0.8360999822616577]\n",
      "Final Test Accuracy: 83.61%\n",
      "Test Top-1 Error: 16.39000177383423\n",
      "Test Top-5 Error: 0.8800029754638672\n",
      "============================================================\n",
      "Final Test Loss: 0.5686\n",
      "============================================================\n",
      "Training Top-1 Error: 7.822221517562866\n",
      "Validation Top-1 Error: 15.759998559951782\n",
      "\n",
      "Best Training Accuracy: 92.18%\n",
      "Best Validation Accuracy: 84.24%\n",
      "Final Test Accuracy: 83.61%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/attention92_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python \"/kaggle/input/ecbm4040-v6/e4040-fall2025-project-swye/train_cifar_new 12.14.py\" \\\n",
    "  --model attention92 \\\n",
    "  --att_type arl \\\n",
    "  --epochs 50 \\\n",
    "  --batch-size 128 \\\n",
    "  --lr 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Attention92 nal on CIFAR-10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T19:14:31.308085Z",
     "iopub.status.busy": "2025-12-15T19:14:31.307291Z",
     "iopub.status.idle": "2025-12-15T20:07:00.228028Z",
     "shell.execute_reply": "2025-12-15T20:07:00.227101Z",
     "shell.execute_reply.started": "2025-12-15T19:14:31.308052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-15 19:14:31.707173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765826071.727903    1814 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765826071.734507    1814 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      ">>> script started\n",
      "============================================================\n",
      "CIFAR-10 Training with TensorFlow/Keras\n",
      "============================================================\n",
      "Model: attention92\n",
      "Epochs: 50\n",
      "Batch size: 128\n",
      "Learning rate: 0.001\n",
      "============================================================\n",
      "Loading CIFAR-10 dataset...\n",
      "Training samples: 45000\n",
      "Test samples: 10000\n",
      "Image shape: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "\n",
      "Building attention92 model...\n",
      "I0000 00:00:1765826080.884151    1814 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "I0000 00:00:1765826097.043893    1814 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "\u001b[1mModel: \"residual_attention_model92\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ att1_x2 (\u001b[94mSequential\u001b[0m)            │ (\u001b[32m1\u001b[0m, \u001b[32m8\u001b[0m, \u001b[32m8\u001b[0m, \u001b[32m256\u001b[0m)         │     \u001b[32m2,123,776\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ att1_x2 (\u001b[94mSequential\u001b[0m)            │ (\u001b[32m1\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m512\u001b[0m)         │     \u001b[32m6,156,288\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ att1_x2 (\u001b[94mSequential\u001b[0m)            │ (\u001b[32m1\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m1024\u001b[0m)        │    \u001b[32m24,502,272\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │    \u001b[32m14,974,976\u001b[0m │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ batch_normalization_232         │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
      "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
      "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m51,233,482\u001b[0m (195.44 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m51,090,378\u001b[0m (194.89 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m143,104\u001b[0m (559.00 KB)\n",
      "\n",
      "Total parameters: 51,233,482\n",
      "Epoch 1/50\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765826197.909578    1838 service.cc:148] XLA service 0x78ccb80e0bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765826197.909640    1838 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1765826262.175946    1838 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - loss: 2.2955 - top1_accuracy: 0.1694\n",
      "Epoch 1: val_top1_accuracy improved from -inf to 0.27160, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 384ms/step - loss: 2.2952 - top1_accuracy: 0.1695 - val_loss: 2.5637 - val_top1_accuracy: 0.2716 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 1.9976 - top1_accuracy: 0.2823\n",
      "Epoch 2: val_top1_accuracy improved from 0.27160 to 0.31560, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 161ms/step - loss: 1.9975 - top1_accuracy: 0.2823 - val_loss: 2.8769 - val_top1_accuracy: 0.3156 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 1.8758 - top1_accuracy: 0.3229\n",
      "Epoch 3: val_top1_accuracy improved from 0.31560 to 0.37640, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - loss: 1.8757 - top1_accuracy: 0.3230 - val_loss: 1.9725 - val_top1_accuracy: 0.3764 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 1.7724 - top1_accuracy: 0.3657\n",
      "Epoch 4: val_top1_accuracy improved from 0.37640 to 0.38160, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 160ms/step - loss: 1.7723 - top1_accuracy: 0.3658 - val_loss: 8.0951 - val_top1_accuracy: 0.3816 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 1.6898 - top1_accuracy: 0.4000\n",
      "Epoch 5: val_top1_accuracy improved from 0.38160 to 0.44820, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - loss: 1.6897 - top1_accuracy: 0.4000 - val_loss: 2.3307 - val_top1_accuracy: 0.4482 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 1.6445 - top1_accuracy: 0.4247\n",
      "Epoch 6: val_top1_accuracy improved from 0.44820 to 0.46040, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - loss: 1.6444 - top1_accuracy: 0.4247 - val_loss: 1.4642 - val_top1_accuracy: 0.4604 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 1.5583 - top1_accuracy: 0.4392\n",
      "Epoch 7: val_top1_accuracy improved from 0.46040 to 0.47120, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 160ms/step - loss: 1.5583 - top1_accuracy: 0.4392 - val_loss: 1.9232 - val_top1_accuracy: 0.4712 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 1.5349 - top1_accuracy: 0.4594\n",
      "Epoch 8: val_top1_accuracy improved from 0.47120 to 0.50220, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - loss: 1.5349 - top1_accuracy: 0.4594 - val_loss: 1.3721 - val_top1_accuracy: 0.5022 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 1.4930 - top1_accuracy: 0.4774\n",
      "Epoch 9: val_top1_accuracy improved from 0.50220 to 0.53080, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - loss: 1.4930 - top1_accuracy: 0.4774 - val_loss: 5.8681 - val_top1_accuracy: 0.5308 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 1.4289 - top1_accuracy: 0.4930\n",
      "Epoch 10: val_top1_accuracy did not improve from 0.53080\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 151ms/step - loss: 1.4289 - top1_accuracy: 0.4930 - val_loss: 1.5516 - val_top1_accuracy: 0.5258 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.3895 - top1_accuracy: 0.5107\n",
      "Epoch 11: val_top1_accuracy improved from 0.53080 to 0.54240, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 163ms/step - loss: 1.3896 - top1_accuracy: 0.5107 - val_loss: 1.2600 - val_top1_accuracy: 0.5424 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.4999 - top1_accuracy: 0.4800\n",
      "Epoch 12: val_top1_accuracy did not improve from 0.54240\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 1.4999 - top1_accuracy: 0.4800 - val_loss: 1.2953 - val_top1_accuracy: 0.5316 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 1.3768 - top1_accuracy: 0.5133\n",
      "Epoch 13: val_top1_accuracy improved from 0.54240 to 0.56180, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 165ms/step - loss: 1.3767 - top1_accuracy: 0.5134 - val_loss: 1.5644 - val_top1_accuracy: 0.5618 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 1.3354 - top1_accuracy: 0.5402\n",
      "Epoch 14: val_top1_accuracy did not improve from 0.56180\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - loss: 1.3354 - top1_accuracy: 0.5401 - val_loss: 1.6237 - val_top1_accuracy: 0.4918 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 1.3364 - top1_accuracy: 0.5447\n",
      "Epoch 15: val_top1_accuracy improved from 0.56180 to 0.57420, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 165ms/step - loss: 1.3363 - top1_accuracy: 0.5447 - val_loss: 2.3764 - val_top1_accuracy: 0.5742 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.4202 - top1_accuracy: 0.5191\n",
      "Epoch 16: val_top1_accuracy did not improve from 0.57420\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 1.4202 - top1_accuracy: 0.5191 - val_loss: 1.4011 - val_top1_accuracy: 0.5462 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.4708 - top1_accuracy: 0.5001\n",
      "Epoch 17: val_top1_accuracy did not improve from 0.57420\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - loss: 1.4708 - top1_accuracy: 0.5000 - val_loss: 1.9800 - val_top1_accuracy: 0.5032 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.3085 - top1_accuracy: 0.5353\n",
      "Epoch 18: val_top1_accuracy did not improve from 0.57420\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 1.3085 - top1_accuracy: 0.5353 - val_loss: 1.3334 - val_top1_accuracy: 0.5446 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.2787 - top1_accuracy: 0.5476\n",
      "Epoch 19: val_top1_accuracy did not improve from 0.57420\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 1.2789 - top1_accuracy: 0.5476 - val_loss: 1.6024 - val_top1_accuracy: 0.4614 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.3726 - top1_accuracy: 0.5144\n",
      "Epoch 20: val_top1_accuracy did not improve from 0.57420\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 1.3724 - top1_accuracy: 0.5145 - val_loss: 1.5348 - val_top1_accuracy: 0.4918 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.1912 - top1_accuracy: 0.5774\n",
      "Epoch 21: val_top1_accuracy improved from 0.57420 to 0.63080, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 163ms/step - loss: 1.1911 - top1_accuracy: 0.5774 - val_loss: 1.0318 - val_top1_accuracy: 0.6308 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.0683 - top1_accuracy: 0.6188\n",
      "Epoch 22: val_top1_accuracy improved from 0.63080 to 0.64940, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 1.0683 - top1_accuracy: 0.6188 - val_loss: 1.1077 - val_top1_accuracy: 0.6494 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1.0407 - top1_accuracy: 0.6299\n",
      "Epoch 23: val_top1_accuracy improved from 0.64940 to 0.65200, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 1.0407 - top1_accuracy: 0.6299 - val_loss: 1.7343 - val_top1_accuracy: 0.6520 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.9964 - top1_accuracy: 0.6467\n",
      "Epoch 24: val_top1_accuracy improved from 0.65200 to 0.67960, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 163ms/step - loss: 0.9963 - top1_accuracy: 0.6467 - val_loss: 0.9972 - val_top1_accuracy: 0.6796 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.9325 - top1_accuracy: 0.6695\n",
      "Epoch 25: val_top1_accuracy improved from 0.67960 to 0.68220, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 163ms/step - loss: 0.9326 - top1_accuracy: 0.6695 - val_loss: 0.9008 - val_top1_accuracy: 0.6822 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.8898 - top1_accuracy: 0.6856\n",
      "Epoch 26: val_top1_accuracy improved from 0.68220 to 0.70100, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 163ms/step - loss: 0.8898 - top1_accuracy: 0.6857 - val_loss: 0.8599 - val_top1_accuracy: 0.7010 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.8649 - top1_accuracy: 0.6940\n",
      "Epoch 27: val_top1_accuracy did not improve from 0.70100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 0.8650 - top1_accuracy: 0.6940 - val_loss: 1.4858 - val_top1_accuracy: 0.6724 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.8658 - top1_accuracy: 0.6976\n",
      "Epoch 28: val_top1_accuracy did not improve from 0.70100\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 154ms/step - loss: 0.8659 - top1_accuracy: 0.6976 - val_loss: 0.9021 - val_top1_accuracy: 0.6834 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.8736 - top1_accuracy: 0.6937\n",
      "Epoch 29: val_top1_accuracy improved from 0.70100 to 0.70560, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 163ms/step - loss: 0.8737 - top1_accuracy: 0.6936 - val_loss: 0.8954 - val_top1_accuracy: 0.7056 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.8149 - top1_accuracy: 0.7125\n",
      "Epoch 30: val_top1_accuracy improved from 0.70560 to 0.71520, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 0.8149 - top1_accuracy: 0.7125 - val_loss: 1.1702 - val_top1_accuracy: 0.7152 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.7745 - top1_accuracy: 0.7254\n",
      "Epoch 31: val_top1_accuracy did not improve from 0.71520\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 0.7745 - top1_accuracy: 0.7254 - val_loss: 0.8351 - val_top1_accuracy: 0.7056 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.7406 - top1_accuracy: 0.7431\n",
      "Epoch 32: val_top1_accuracy did not improve from 0.71520\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 0.7407 - top1_accuracy: 0.7430 - val_loss: 4.6429 - val_top1_accuracy: 0.5662 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.8228 - top1_accuracy: 0.7086\n",
      "Epoch 33: val_top1_accuracy improved from 0.71520 to 0.73240, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 0.8227 - top1_accuracy: 0.7086 - val_loss: 0.7706 - val_top1_accuracy: 0.7324 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.7051 - top1_accuracy: 0.7547\n",
      "Epoch 34: val_top1_accuracy improved from 0.73240 to 0.73480, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 0.7051 - top1_accuracy: 0.7547 - val_loss: 0.7753 - val_top1_accuracy: 0.7348 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.6944 - top1_accuracy: 0.7557\n",
      "Epoch 35: val_top1_accuracy improved from 0.73480 to 0.75200, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 163ms/step - loss: 0.6944 - top1_accuracy: 0.7557 - val_loss: 0.7297 - val_top1_accuracy: 0.7520 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.6441 - top1_accuracy: 0.7730\n",
      "Epoch 36: val_top1_accuracy did not improve from 0.75200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 0.6444 - top1_accuracy: 0.7730 - val_loss: 1.1014 - val_top1_accuracy: 0.6584 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.7505 - top1_accuracy: 0.7403\n",
      "Epoch 37: val_top1_accuracy improved from 0.75200 to 0.75840, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 0.7504 - top1_accuracy: 0.7403 - val_loss: 0.6877 - val_top1_accuracy: 0.7584 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.6115 - top1_accuracy: 0.7876\n",
      "Epoch 38: val_top1_accuracy improved from 0.75840 to 0.76560, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 0.6115 - top1_accuracy: 0.7876 - val_loss: 0.6852 - val_top1_accuracy: 0.7656 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.5858 - top1_accuracy: 0.7948\n",
      "Epoch 39: val_top1_accuracy did not improve from 0.76560\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 0.5858 - top1_accuracy: 0.7948 - val_loss: 0.6719 - val_top1_accuracy: 0.7636 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.5689 - top1_accuracy: 0.7984\n",
      "Epoch 40: val_top1_accuracy improved from 0.76560 to 0.77460, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 0.5689 - top1_accuracy: 0.7984 - val_loss: 0.6431 - val_top1_accuracy: 0.7746 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.5545 - top1_accuracy: 0.8022\n",
      "Epoch 41: val_top1_accuracy did not improve from 0.77460\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - loss: 0.5545 - top1_accuracy: 0.8023 - val_loss: 0.6883 - val_top1_accuracy: 0.7736 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.5211 - top1_accuracy: 0.8147\n",
      "Epoch 42: val_top1_accuracy improved from 0.77460 to 0.78260, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 0.5211 - top1_accuracy: 0.8147 - val_loss: 0.6529 - val_top1_accuracy: 0.7826 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.4954 - top1_accuracy: 0.8244\n",
      "Epoch 43: val_top1_accuracy did not improve from 0.78260\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 0.4955 - top1_accuracy: 0.8244 - val_loss: 0.8718 - val_top1_accuracy: 0.7586 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.5133 - top1_accuracy: 0.8203\n",
      "Epoch 44: val_top1_accuracy did not improve from 0.78260\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 0.5132 - top1_accuracy: 0.8203 - val_loss: 0.6438 - val_top1_accuracy: 0.7816 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.4692 - top1_accuracy: 0.8338\n",
      "Epoch 45: val_top1_accuracy did not improve from 0.78260\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 155ms/step - loss: 0.4692 - top1_accuracy: 0.8338 - val_loss: 0.6470 - val_top1_accuracy: 0.7770 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.4652 - top1_accuracy: 0.8362\n",
      "Epoch 46: val_top1_accuracy improved from 0.78260 to 0.78680, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 0.4652 - top1_accuracy: 0.8362 - val_loss: 0.6471 - val_top1_accuracy: 0.7868 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.4465 - top1_accuracy: 0.8437\n",
      "Epoch 47: val_top1_accuracy improved from 0.78680 to 0.79720, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 165ms/step - loss: 0.4465 - top1_accuracy: 0.8437 - val_loss: 0.6207 - val_top1_accuracy: 0.7972 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.4243 - top1_accuracy: 0.8494\n",
      "Epoch 48: val_top1_accuracy improved from 0.79720 to 0.80200, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 165ms/step - loss: 0.4243 - top1_accuracy: 0.8494 - val_loss: 0.5892 - val_top1_accuracy: 0.8020 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 0.4095 - top1_accuracy: 0.8557\n",
      "Epoch 49: val_top1_accuracy did not improve from 0.80200\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - loss: 0.4095 - top1_accuracy: 0.8557 - val_loss: 0.6131 - val_top1_accuracy: 0.7978 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.3925 - top1_accuracy: 0.8597\n",
      "Epoch 50: val_top1_accuracy improved from 0.80200 to 0.80780, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 164ms/step - loss: 0.3925 - top1_accuracy: 0.8597 - val_loss: 0.5755 - val_top1_accuracy: 0.8078 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 50.\n",
      "\n",
      "Loading best model weights...\n",
      "\n",
      "Evaluating on test set...\n",
      "metrics_names: ['loss', 'compile_metrics']\n",
      "results: [0.5889126658439636, 0.8015999794006348]\n",
      "Final Test Accuracy: 80.16%\n",
      "Test Top-1 Error: 19.840002059936523\n",
      "Test Top-5 Error: 1.0100007057189941\n",
      "============================================================\n",
      "Final Test Loss: 0.5889\n",
      "============================================================\n",
      "Training Top-1 Error: 14.428889751434326\n",
      "Validation Top-1 Error: 19.220000505447388\n",
      "\n",
      "Best Training Accuracy: 85.57%\n",
      "Best Validation Accuracy: 80.78%\n",
      "Final Test Accuracy: 80.16%\n",
      "\n",
      "Training complete!\n",
      "Best model saved to: ./checkpoints/attention92_best_model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "!python \"/kaggle/input/ecbm4040-v6/e4040-fall2025-project-swye/train_cifar_new 12.14.py\" \\\n",
    "  --model attention92 \\\n",
    "  --att_type nal \\\n",
    "  --epochs 50 \\\n",
    "  --batch-size 128 \\\n",
    "  --lr 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9024580,
     "sourceId": 14158909,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9024794,
     "sourceId": 14159261,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9025335,
     "sourceId": 14160132,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9025407,
     "sourceId": 14160231,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9025439,
     "sourceId": 14160279,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9025453,
     "sourceId": 14160297,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9025463,
     "sourceId": 14160314,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9025490,
     "sourceId": 14160371,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9030762,
     "sourceId": 14167625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9031382,
     "sourceId": 14168591,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9031408,
     "sourceId": 14168634,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
