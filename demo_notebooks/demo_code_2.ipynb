{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook runs simple version Attention92 nal"
      ],
      "metadata": {
        "id": "EuaLsPEb7pGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/e4040-fall2025-project-swye\n",
        "!python train_cifar_new\\ 12.14.py --model attention92_simple --att_type nal --epochs 50 --batch-size 128 --lr 0.001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imCIxQ3-npmH",
        "outputId": "febde38b-06ef-4618-dda5-d501b68fa069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/e4040-fall2025-project-swye\n",
            "2025-12-15 00:02:02.464849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765756922.483441   19663 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765756922.489149   19663 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765756922.503798   19663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765756922.503825   19663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765756922.503828   19663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765756922.503833   19663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-15 00:02:02.508191: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            ">>> script started\n",
            "============================================================\n",
            "CIFAR-10 Training with TensorFlow/Keras\n",
            "============================================================\n",
            "Model: attention92\n",
            "Epochs: 50\n",
            "Batch size: 128\n",
            "Learning rate: 0.001\n",
            "============================================================\n",
            "Loading CIFAR-10 dataset...\n",
            "Training samples: 45000\n",
            "Test samples: 10000\n",
            "Image shape: (32, 32, 3)\n",
            "Number of classes: 10\n",
            "\n",
            "Building attention92 model...\n",
            "2025-12-15 00:02:12.494769: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1765756932.494929   19663 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "I0000 00:00:1765756939.727254   19663 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "\u001b[1mModel: \"residual_attention_model92\"\u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
            "│ conv2d (\u001b[94mConv2D\u001b[0m)                 │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │         \u001b[32m9,408\u001b[0m │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ batch_normalization             │ (\u001b[32m1\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m16\u001b[0m, \u001b[32m64\u001b[0m)        │           \u001b[32m256\u001b[0m │\n",
            "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ max_pooling2d (\u001b[94mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[32m0\u001b[0m │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ pre_act_layer (\u001b[94mPreActLayer\u001b[0m)     │ ?                      │        \u001b[32m74,496\u001b[0m │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ attention_module                │ ?                      │     \u001b[32m1,061,888\u001b[0m │\n",
            "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ pre_act_layer_1 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │        \u001b[32m71,168\u001b[0m │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ pre_act_layer_2 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m378,880\u001b[0m │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ attention_module_1              │ ?                      │     \u001b[32m3,078,144\u001b[0m │\n",
            "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ pre_act_layer_3 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │       \u001b[32m281,600\u001b[0m │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ pre_act_layer_4 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,511,424\u001b[0m │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ attention_module_2              │ ?                      │    \u001b[32m12,251,136\u001b[0m │\n",
            "│ (\u001b[94mAttentionModule\u001b[0m)               │                        │               │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ pre_act_layer_5 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │     \u001b[32m1,120,256\u001b[0m │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ pre_act_layer_6 (\u001b[94mPreActLayer\u001b[0m)   │ ?                      │    \u001b[32m14,974,976\u001b[0m │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ batch_normalization_130         │ (\u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m2048\u001b[0m)        │         \u001b[32m8,192\u001b[0m │\n",
            "│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ global_average_pooling2d        │ ?                      │             \u001b[32m0\u001b[0m │\n",
            "│ (\u001b[94mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
            "│ dense (\u001b[94mDense\u001b[0m)                   │ (\u001b[32m1\u001b[0m, \u001b[32m10\u001b[0m)                │        \u001b[32m20,490\u001b[0m │\n",
            "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
            "\u001b[1m Total params: \u001b[0m\u001b[32m34,842,314\u001b[0m (132.91 MB)\n",
            "\u001b[1m Trainable params: \u001b[0m\u001b[32m34,756,042\u001b[0m (132.58 MB)\n",
            "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m86,272\u001b[0m (337.00 KB)\n",
            "\n",
            "Total parameters: 34,842,314\n",
            "2025-12-15 00:02:21.728478: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1105920000 exceeds 10% of free system memory.\n",
            "2025-12-15 00:02:22.814883: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1105920000 exceeds 10% of free system memory.\n",
            "Epoch 1/50\n",
            "2025-12-15 00:02:23.856568: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1105920000 exceeds 10% of free system memory.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1765756998.364767   19722 service.cc:152] XLA service 0x7ae498002c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1765756998.364805   19722 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2025-12-15 00:03:20.032973: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "I0000 00:00:1765757043.289232   19722 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - loss: 2.1345 - top1_accuracy: 0.2460\n",
            "Epoch 1: val_top1_accuracy improved from -inf to 0.35700, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 308ms/step - loss: 2.1339 - top1_accuracy: 0.2462 - val_loss: 2.6142 - val_top1_accuracy: 0.3570 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 1.6655 - top1_accuracy: 0.4030\n",
            "Epoch 2: val_top1_accuracy improved from 0.35700 to 0.44460, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 157ms/step - loss: 1.6655 - top1_accuracy: 0.4030 - val_loss: 2.1173 - val_top1_accuracy: 0.4446 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.5835 - top1_accuracy: 0.4457\n",
            "Epoch 3: val_top1_accuracy improved from 0.44460 to 0.47640, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 157ms/step - loss: 1.5835 - top1_accuracy: 0.4457 - val_loss: 1.6934 - val_top1_accuracy: 0.4764 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.4910 - top1_accuracy: 0.4810\n",
            "Epoch 4: val_top1_accuracy did not improve from 0.47640\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 123ms/step - loss: 1.4911 - top1_accuracy: 0.4810 - val_loss: 14.6903 - val_top1_accuracy: 0.3160 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.5234 - top1_accuracy: 0.4585\n",
            "Epoch 5: val_top1_accuracy improved from 0.47640 to 0.52160, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - loss: 1.5232 - top1_accuracy: 0.4586 - val_loss: 1.3402 - val_top1_accuracy: 0.5216 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.3904 - top1_accuracy: 0.5156\n",
            "Epoch 6: val_top1_accuracy did not improve from 0.52160\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 123ms/step - loss: 1.3904 - top1_accuracy: 0.5156 - val_loss: 17.5265 - val_top1_accuracy: 0.3956 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.4039 - top1_accuracy: 0.5057\n",
            "Epoch 7: val_top1_accuracy improved from 0.52160 to 0.55400, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 141ms/step - loss: 1.4037 - top1_accuracy: 0.5057 - val_loss: 1.2415 - val_top1_accuracy: 0.5540 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.2376 - top1_accuracy: 0.5617\n",
            "Epoch 8: val_top1_accuracy improved from 0.55400 to 0.60440, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 155ms/step - loss: 1.2376 - top1_accuracy: 0.5617 - val_loss: 1.1017 - val_top1_accuracy: 0.6044 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.1595 - top1_accuracy: 0.5978\n",
            "Epoch 9: val_top1_accuracy did not improve from 0.60440\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 123ms/step - loss: 1.1596 - top1_accuracy: 0.5978 - val_loss: 1.1244 - val_top1_accuracy: 0.5990 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 1.1142 - top1_accuracy: 0.6084\n",
            "Epoch 10: val_top1_accuracy improved from 0.60440 to 0.66940, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 161ms/step - loss: 1.1141 - top1_accuracy: 0.6084 - val_loss: 0.9532 - val_top1_accuracy: 0.6694 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.0384 - top1_accuracy: 0.6363\n",
            "Epoch 11: val_top1_accuracy did not improve from 0.66940\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 124ms/step - loss: 1.0384 - top1_accuracy: 0.6363 - val_loss: 1.4804 - val_top1_accuracy: 0.4954 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.1865 - top1_accuracy: 0.5839\n",
            "Epoch 12: val_top1_accuracy did not improve from 0.66940\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 1.1863 - top1_accuracy: 0.5840 - val_loss: 1.4555 - val_top1_accuracy: 0.5674 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.4227 - top1_accuracy: 0.5232\n",
            "Epoch 13: val_top1_accuracy did not improve from 0.66940\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 124ms/step - loss: 1.4230 - top1_accuracy: 0.5230 - val_loss: 2.2441 - val_top1_accuracy: 0.4902 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.3324 - top1_accuracy: 0.5306\n",
            "Epoch 14: val_top1_accuracy did not improve from 0.66940\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 124ms/step - loss: 1.3323 - top1_accuracy: 0.5306 - val_loss: 1.0691 - val_top1_accuracy: 0.6216 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 1.1598 - top1_accuracy: 0.6021\n",
            "Epoch 15: val_top1_accuracy did not improve from 0.66940\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 123ms/step - loss: 1.1599 - top1_accuracy: 0.6020 - val_loss: 1.7929 - val_top1_accuracy: 0.3884 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1.2995 - top1_accuracy: 0.5498\n",
            "Epoch 16: val_top1_accuracy improved from 0.66940 to 0.67060, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 162ms/step - loss: 1.2992 - top1_accuracy: 0.5499 - val_loss: 1.0894 - val_top1_accuracy: 0.6706 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.9972 - top1_accuracy: 0.6514\n",
            "Epoch 17: val_top1_accuracy improved from 0.67060 to 0.70040, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 166ms/step - loss: 0.9971 - top1_accuracy: 0.6514 - val_loss: 0.8695 - val_top1_accuracy: 0.7004 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.9403 - top1_accuracy: 0.6696\n",
            "Epoch 18: val_top1_accuracy did not improve from 0.70040\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 125ms/step - loss: 0.9403 - top1_accuracy: 0.6696 - val_loss: 0.8913 - val_top1_accuracy: 0.6848 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.8941 - top1_accuracy: 0.6860\n",
            "Epoch 19: val_top1_accuracy improved from 0.70040 to 0.71300, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 176ms/step - loss: 0.8941 - top1_accuracy: 0.6860 - val_loss: 0.8431 - val_top1_accuracy: 0.7130 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.8549 - top1_accuracy: 0.6986\n",
            "Epoch 20: val_top1_accuracy improved from 0.71300 to 0.71440, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 162ms/step - loss: 0.8550 - top1_accuracy: 0.6986 - val_loss: 0.8702 - val_top1_accuracy: 0.7144 - learning_rate: 5.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.8388 - top1_accuracy: 0.7065\n",
            "Epoch 21: val_top1_accuracy improved from 0.71440 to 0.72000, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 177ms/step - loss: 0.8388 - top1_accuracy: 0.7065 - val_loss: 0.8063 - val_top1_accuracy: 0.7200 - learning_rate: 5.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.8228 - top1_accuracy: 0.7150\n",
            "Epoch 22: val_top1_accuracy improved from 0.72000 to 0.73320, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 180ms/step - loss: 0.8228 - top1_accuracy: 0.7150 - val_loss: 0.7531 - val_top1_accuracy: 0.7332 - learning_rate: 5.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.7615 - top1_accuracy: 0.7329\n",
            "Epoch 23: val_top1_accuracy did not improve from 0.73320\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 124ms/step - loss: 0.7615 - top1_accuracy: 0.7329 - val_loss: 0.8307 - val_top1_accuracy: 0.7272 - learning_rate: 5.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.7365 - top1_accuracy: 0.7418\n",
            "Epoch 24: val_top1_accuracy improved from 0.73320 to 0.76000, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - loss: 0.7365 - top1_accuracy: 0.7418 - val_loss: 0.7055 - val_top1_accuracy: 0.7600 - learning_rate: 5.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.6763 - top1_accuracy: 0.7614\n",
            "Epoch 25: val_top1_accuracy did not improve from 0.76000\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 123ms/step - loss: 0.6763 - top1_accuracy: 0.7614 - val_loss: 1.3618 - val_top1_accuracy: 0.7172 - learning_rate: 5.0000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.6935 - top1_accuracy: 0.7574\n",
            "Epoch 26: val_top1_accuracy improved from 0.76000 to 0.76820, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 160ms/step - loss: 0.6935 - top1_accuracy: 0.7574 - val_loss: 0.6698 - val_top1_accuracy: 0.7682 - learning_rate: 5.0000e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.6279 - top1_accuracy: 0.7808\n",
            "Epoch 27: val_top1_accuracy did not improve from 0.76820\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 123ms/step - loss: 0.6279 - top1_accuracy: 0.7808 - val_loss: 0.9065 - val_top1_accuracy: 0.7436 - learning_rate: 5.0000e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.6457 - top1_accuracy: 0.7725\n",
            "Epoch 28: val_top1_accuracy did not improve from 0.76820\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - loss: 0.6457 - top1_accuracy: 0.7725 - val_loss: 0.7128 - val_top1_accuracy: 0.7568 - learning_rate: 5.0000e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.6197 - top1_accuracy: 0.7823\n",
            "Epoch 29: val_top1_accuracy improved from 0.76820 to 0.77880, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 161ms/step - loss: 0.6197 - top1_accuracy: 0.7823 - val_loss: 0.6407 - val_top1_accuracy: 0.7788 - learning_rate: 5.0000e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.5867 - top1_accuracy: 0.7960\n",
            "Epoch 30: val_top1_accuracy improved from 0.77880 to 0.78600, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 163ms/step - loss: 0.5867 - top1_accuracy: 0.7960 - val_loss: 0.6220 - val_top1_accuracy: 0.7860 - learning_rate: 5.0000e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.5865 - top1_accuracy: 0.7940\n",
            "Epoch 31: val_top1_accuracy did not improve from 0.78600\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 124ms/step - loss: 0.5865 - top1_accuracy: 0.7940 - val_loss: 0.6702 - val_top1_accuracy: 0.7798 - learning_rate: 5.0000e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.5630 - top1_accuracy: 0.8021\n",
            "Epoch 32: val_top1_accuracy improved from 0.78600 to 0.79360, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 176ms/step - loss: 0.5630 - top1_accuracy: 0.8021 - val_loss: 0.6119 - val_top1_accuracy: 0.7936 - learning_rate: 5.0000e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.5309 - top1_accuracy: 0.8155\n",
            "Epoch 33: val_top1_accuracy improved from 0.79360 to 0.79920, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 159ms/step - loss: 0.5309 - top1_accuracy: 0.8155 - val_loss: 0.5912 - val_top1_accuracy: 0.7992 - learning_rate: 5.0000e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.5074 - top1_accuracy: 0.8248\n",
            "Epoch 34: val_top1_accuracy did not improve from 0.79920\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 125ms/step - loss: 0.5074 - top1_accuracy: 0.8248 - val_loss: 0.5932 - val_top1_accuracy: 0.7934 - learning_rate: 5.0000e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.4866 - top1_accuracy: 0.8266\n",
            "Epoch 35: val_top1_accuracy did not improve from 0.79920\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 123ms/step - loss: 0.4867 - top1_accuracy: 0.8266 - val_loss: 0.6060 - val_top1_accuracy: 0.7952 - learning_rate: 5.0000e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 0.8572 - top1_accuracy: 0.7204\n",
            "Epoch 36: val_top1_accuracy did not improve from 0.79920\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 124ms/step - loss: 0.8568 - top1_accuracy: 0.7205 - val_loss: 0.6486 - val_top1_accuracy: 0.7758 - learning_rate: 5.0000e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.5154 - top1_accuracy: 0.8197\n",
            "Epoch 37: val_top1_accuracy improved from 0.79920 to 0.80320, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 156ms/step - loss: 0.5154 - top1_accuracy: 0.8197 - val_loss: 0.5785 - val_top1_accuracy: 0.8032 - learning_rate: 5.0000e-04\n",
            "Epoch 38/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.4748 - top1_accuracy: 0.8339\n",
            "Epoch 38: val_top1_accuracy did not improve from 0.80320\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 124ms/step - loss: 0.4749 - top1_accuracy: 0.8339 - val_loss: 0.5987 - val_top1_accuracy: 0.8014 - learning_rate: 5.0000e-04\n",
            "Epoch 39/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.4801 - top1_accuracy: 0.8308\n",
            "Epoch 39: val_top1_accuracy improved from 0.80320 to 0.80560, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 154ms/step - loss: 0.4801 - top1_accuracy: 0.8308 - val_loss: 0.5746 - val_top1_accuracy: 0.8056 - learning_rate: 5.0000e-04\n",
            "Epoch 40/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.4420 - top1_accuracy: 0.8433\n",
            "Epoch 40: val_top1_accuracy improved from 0.80560 to 0.80580, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 157ms/step - loss: 0.4420 - top1_accuracy: 0.8433 - val_loss: 0.5904 - val_top1_accuracy: 0.8058 - learning_rate: 5.0000e-04\n",
            "Epoch 41/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.4265 - top1_accuracy: 0.8476\n",
            "Epoch 41: val_top1_accuracy did not improve from 0.80580\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 123ms/step - loss: 0.4266 - top1_accuracy: 0.8476 - val_loss: 0.5890 - val_top1_accuracy: 0.8020 - learning_rate: 5.0000e-04\n",
            "Epoch 42/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.4182 - top1_accuracy: 0.8515\n",
            "Epoch 42: val_top1_accuracy did not improve from 0.80580\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 124ms/step - loss: 0.4182 - top1_accuracy: 0.8515 - val_loss: 0.6152 - val_top1_accuracy: 0.7974 - learning_rate: 5.0000e-04\n",
            "Epoch 43/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.4093 - top1_accuracy: 0.8561\n",
            "Epoch 43: val_top1_accuracy improved from 0.80580 to 0.81020, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 157ms/step - loss: 0.4093 - top1_accuracy: 0.8561 - val_loss: 0.5675 - val_top1_accuracy: 0.8102 - learning_rate: 5.0000e-04\n",
            "Epoch 44/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.3971 - top1_accuracy: 0.8596\n",
            "Epoch 44: val_top1_accuracy did not improve from 0.81020\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 123ms/step - loss: 0.3971 - top1_accuracy: 0.8596 - val_loss: 0.6211 - val_top1_accuracy: 0.7920 - learning_rate: 5.0000e-04\n",
            "Epoch 45/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.4131 - top1_accuracy: 0.8558\n",
            "Epoch 45: val_top1_accuracy improved from 0.81020 to 0.82080, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 150ms/step - loss: 0.4130 - top1_accuracy: 0.8558 - val_loss: 0.5471 - val_top1_accuracy: 0.8208 - learning_rate: 5.0000e-04\n",
            "Epoch 46/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.3859 - top1_accuracy: 0.8648\n",
            "Epoch 46: val_top1_accuracy did not improve from 0.82080\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 124ms/step - loss: 0.3859 - top1_accuracy: 0.8648 - val_loss: 0.5697 - val_top1_accuracy: 0.8132 - learning_rate: 5.0000e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.3710 - top1_accuracy: 0.8675\n",
            "Epoch 47: val_top1_accuracy did not improve from 0.82080\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 123ms/step - loss: 0.3710 - top1_accuracy: 0.8675 - val_loss: 0.5550 - val_top1_accuracy: 0.8166 - learning_rate: 5.0000e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.3749 - top1_accuracy: 0.8658\n",
            "Epoch 48: val_top1_accuracy improved from 0.82080 to 0.82400, saving model to ./checkpoints/attention92_best_model.weights.h5\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 155ms/step - loss: 0.3749 - top1_accuracy: 0.8658 - val_loss: 0.5377 - val_top1_accuracy: 0.8240 - learning_rate: 5.0000e-04\n",
            "Epoch 49/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.3466 - top1_accuracy: 0.8771\n",
            "Epoch 49: val_top1_accuracy did not improve from 0.82400\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 123ms/step - loss: 0.3467 - top1_accuracy: 0.8771 - val_loss: 0.5656 - val_top1_accuracy: 0.8152 - learning_rate: 5.0000e-04\n",
            "Epoch 50/50\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.3981 - top1_accuracy: 0.8607\n",
            "Epoch 50: val_top1_accuracy did not improve from 0.82400\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 124ms/step - loss: 0.3981 - top1_accuracy: 0.8607 - val_loss: 0.5741 - val_top1_accuracy: 0.8196 - learning_rate: 5.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 48.\n",
            "\n",
            "Loading best model weights...\n",
            "\n",
            "Evaluating on test set...\n",
            "metrics_names: ['loss', 'compile_metrics']\n",
            "results: [0.5632759928703308, 0.8209999799728394]\n",
            "Final Test Accuracy: 82.10%\n",
            "Test Top-1 Error: 17.900002002716064\n",
            "Test Top-5 Error: 1.0200024\n",
            "============================================================\n",
            "Final Test Loss: 0.5633\n",
            "============================================================\n",
            "Training Top-1 Error: 12.835556268692017\n",
            "Validation Top-1 Error: 17.59999990463257\n",
            "\n",
            "Best Training Accuracy: 87.16%\n",
            "Best Validation Accuracy: 82.40%\n",
            "Final Test Accuracy: 82.10%\n",
            "\n",
            "Training complete!\n",
            "Best model saved to: ./checkpoints/attention92_best_model.weights.h5\n"
          ]
        }
      ]
    }
  ]
}